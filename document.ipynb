{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eb21b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document data structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f6a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Pradnya Karwade', 'date_created': '2025-01-01'}, page_content='This is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document (\n",
    "    page_content =\"This is the main text content I am using to create RAG\",\n",
    "    metadata= {\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Pradnya Karwade\" ,\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de67dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f38d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!! Sample text files are created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\": \"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability. \n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular \n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility}\n",
    "-Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificail intelligence, and automation.\"\"\",\n",
    "\n",
    "\"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve \n",
    "from experience without being explicitly programmed. It focuses on developing computer programs \n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "     with open (filepath, 'w', encoding = \"utf-8\") as f:\n",
    "           f.write (content)\n",
    "\n",
    "print (\"Done!! Sample text files are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d7fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability. \\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular \\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility}\\n-Strong community support\\n\\nPython is widely used in web development, data science, artificail intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "###Text Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader (\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ab1abb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve \\nfrom experience without being explicitly programmed. It focuses on developing computer programs \\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability. \\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular \\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility}\\n-Strong community support\\n\\nPython is widely used in web development, data science, artificail intelligence, and automation.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "## load all the text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader (\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ##Pattern to match the files\n",
    "    loader_cls = TextLoader, ##loader class to use\n",
    "    loader_kwargs ={'encoding' : 'utf-8'},\n",
    "    \n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0cb2155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 0}, page_content='Attention capabilities for AI systems\\n \\nHelgi Páll Helgason1, Kristinn R. Thórisson1,2 \\n1Center for Analysis & Design of Intelligent Agents / School of Computer Science, Venus 2nd floor, Reykjavik \\nUniversity,Menntavegur 1, 101 Reykjavik, Iceland \\n2Icelandic Institute for Intelligent Machines, 2. h. Uranus, Menntavegur 1, 101 Reykjavik, Iceland \\nhelgih09@ru.is, thorisson@ru.is \\nKeywords: \\nArtificial intelligence, attention, resource management \\nAbstract: \\nMuch of present AI research is based on the assumption of computational systems with infinite resources, \\nan assumption that is either explicitly stated or implicit in the work as researchers ignore the fact that most \\nreal-world tasks must be finished within certain time limits, and it is the role of intelligence to effectively \\ndeal with such limitations. Expecting AI systems to give equal treatment to every piece of data they \\nencounter is not appropriate in most real-world cases; available resources are likely to be insufficient for \\nkeeping up with available data in even moderately complex environments. Even if sufficient resources are \\navailable, they might possibly be put to better use than blindly applying them to every possible piece of \\ndata. Finding inspiration for more intelligent resource management schemes is not hard, we need to look no \\nfurther than ourselves. This paper explores what human attention has to offer in terms of ideas and concepts \\nfor implementing intelligent resource management and how the resulting principles can be extended to \\nlevels beyond human attention. We also discuss some ideas for the principles behind attention mechanisms \\nfor artificial (general) intelligences. \\n1 INTRODUCTION \\nThe field of AI has a long history of targeting \\nisolated, well-defined problems to demonstrate \\nintelligent capabilities. While useful, many of these \\nproblems (and especially their task environments, as \\nperceived by the system) are extremely simple \\ncompared to the problem of learning how to solve \\nnovel tasks and adapting to changes in real-world \\nenvironments - a problem which must be addressed \\nand solved in order for AI systems to approach \\nhuman-level intelligence. Given the nature of this \\nprior work, it is not surprising that limited focus has \\nbeen given to real-time processing and resource \\nmanagement. However, the design of any AI system \\nexpected to learn and perform a range of tasks in \\neveryday environments needs to face these realities: \\n \\n\\uf001 \\nThe real world is highly dynamic and complex \\nand can provide an abundance of information \\nat any given moment. \\n\\uf001 \\nResources of any intelligent system are not \\nonly limited, but insufficient in light of the \\nmassive amount of information available from \\nthe environment. \\n \\n\\uf001 \\nA range of time constraints, many of which \\nare dictated by the environment, must be \\nsatisfied in order to ensure safe and successful \\noperation of the system. \\n \\nMuch of existing work in the field of AI is also \\nbased on greatly simplified operating assumptions - \\na case in point being the practically impossible (but \\nsurprisingly \\ncommon) \\nassumption \\nof \\ninfinite \\nresources, often in terms of storage but particularly \\nin terms of processing: A system based on this \\nassumption will fail to perform and potentially crash \\nin real world operation when fed with information at \\na greater rate than it is capable of processing. To \\nfind \\ninspiration \\nfor \\nimplementing \\nintelligent \\nresource management we need not look far, nature \\nhas provided us with a prime example in human \\nattention; a cognitive function that enables us to \\nfocus \\nour \\nlimited \\nresources \\nselectively \\non \\ninformation that is most important to us at any given \\nmoment as we perform various tasks while'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 1}, page_content='remaining reactive to unexpected but important \\nevents in the environment. Consider that while \\nreading this chapter you have effectively ignored \\nmore than 99.9% of the numerous things that your \\nmind could have spent time and resources on doing. \\nPerhaps not surprisingly, it turns out that this is \\nexactly the kind of resource management that is \\nrequired to enable AI systems to approach human-\\nlevel intelligence in real-world environments. Thus, \\nit makes perfect sense to investigate how AI systems \\ncan be endowed with this cognitive function for the \\npurpose of improving their operation and making \\nthem applicable to more open-ended and complex \\ntasks and environments. The goal need not be to \\nreplicate any biological function in detail, but rather \\nto extract useful concepts and methods from the \\nbiological side while leaving undesirable limitations \\nbehind in order to facilitate the creation of AI \\nsystems that can successfully operate in real-world \\nenvironments in realtime using limited resources.  \\n \\nWhile attention has been largely ignored in \\nthe field to-date, there are notable exceptions. These \\ninclude cognitive architectures such as NARS \\n(Wang, 1995), LIDA (Baars, 2009) and Clarion \\n(Sun, 2006). However, the attentional functionality \\nimplemented in these systems is incomplete in \\nvarious ways, such as focusing solely on data-\\nfiltering \\n(ignoring \\ncontrol \\nissues, \\ne.g. \\nhow \\nprioritization affects processing of selected data) and \\nexternal \\nenvironmental \\ninformation \\n(ignoring \\ninternal system states). The ASMO framework \\n(Novianto, 2009) is somewhat unique as it assumes a \\ntight coupling between attention and self-awareness \\nand includes focus on internal states. However, none \\nof this work addresses realtime processing, which is \\none of the major reasons we desire attentional \\nfunctionality, in a vigorous fashion. Attention has \\nalso been studied in relation to AI within the limited \\nscope of working memory (c.f. Phillips 2005 and \\nSkubic 2004). While attention and working memory \\nare closely related, this is a restrictive context to \\nstudy attention within as working memory can in \\nmost cases be modelled as a cognitive function \\nrather than an architectural component. \\nThis paper starts with a brief overview of \\nhuman attention and subsequently attempts to \\nextract principles that may be useful for AI systems. \\nThis is followed by a discussion of how these \\nprinciples might be extended to levels beyond \\nhuman \\nattention \\nfor \\nmeta-reasoning \\nand \\nintrospection. We then present a high-level design of \\nan \\nattention \\nmechanism \\nintended \\nfor \\nAI \\narchitectures.  \\n2 \\nHUMAN ATTENTION \\nResearch of human attention has a long history \\ndating back to the beginnings of psychology. Back \\nin 1890, the American psychologist William James \\nwrote the following (James 1890): \\n \\n“Everyone knows what attention is. It is the taking \\npossession by the mind, in clear and vivid form, of \\none out of what seem several simultaneously \\npossible objects or trains of thought. Focalization, \\nconcentration, of consciousness are of its essence. It \\nimplies withdrawal from some things in order to \\ndeal effectively with others, and is a condition which \\nhas a real opposite in the confused, dazed, \\nscatterbrained state which in French is called \\ndistraction, and Zerstreutheit in German.” \\n \\n- \\nWilliam James \\n \\nThis elegant description indicates that the \\nimportance of attention for the human mind was \\nidentified as early as the 18th century. The beginning \\nof modern attention research is commonly tied to \\nColin Cherry’s work on what has been called the \\n“cocktail party effect” (Cherry 1953), which \\naddresses how we are able to focus on particular \\nsensory data in the presence of distracting \\ninformation and noise, such as following and \\nparticipating in a conversation at a cocktail party in \\nthe presence of many other conversations and \\nbackground noise, and still be able to catch when \\nsomeone calls our name in the background. The \\nability to be in a focused state of attention while \\nremaining reactive to unexpected events, seems to \\ncall for a selective filtering mechanism of some sort \\nwhile at the same time requiring deliberate steering \\nof cognitive resources. The cocktail party scenario is \\na good illustration of the dual nature of attention: \\nWe will refer to the deliberate, goal-driven side as \\ntop-down attention and the reactive, stimulus-driven \\nside as bottom-up attention. \\nA number of models for attention were \\nsubsequently proposed, some of which were \\nconsidered early selection models as selection of \\nsensory information is assumed to occur early in the \\nsensory pipeline based on primitive physical features \\nof \\nthe \\ninformation. \\nThis \\nimplies \\nthat \\nthe \\ndetermination of what is important and should be \\nselected is based on shallow, primitive processing \\nwith very limited or non-existent analysis of \\nmeaning. The Broadbent filter model (Broadbent \\n1958) is the most prominent of these. A number of \\nlate selection models have also been proposed, that'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 2}, page_content='assume further analysis of incoming sensory \\ninformation must be performed in order to determine \\nits relevance and carry out efficient selection. The \\nDeutsch-Norman (Norman 1969) model is based on \\nthe assumption that sensory information is not \\nactually filtered, but processed to the point of \\nactivating \\nrepresentations \\nstored \\nin \\nmemory. \\nSelection then occurs at the level of representations, \\nwhere the most active ones are selected for further \\nprocessing. The model also assumes an attentional \\nbottleneck \\nat \\nthis \\npoint, \\nwhere \\nonly \\none \\nrepresentation can be selected for processing at a \\ntime. These two classes of attention models are \\nreferred to as the early vs. late selection models, and \\nhave resulted in some debate. Shortcomings of many \\nearly selection models are obvious, as they fail to \\naccount for parts of the cocktail party effect, \\nespecially phenomena such as noticing your own \\nname being called from across the room while \\nengaged in conversation. This contradicts the model, \\nas the physical characteristics of the data (our name \\nbeing called) would not be sufficient to attract our \\nattention and pass through the filter; some analysis \\nof meaning must be involved. \\nSome more recent theories and models of \\nattention focus on the interaction between top-down \\nand bottom-up attention. In (Knudsen 2007), an \\nattention framework is presented based on four \\nfundamental processes: working memory, top-down \\nsensitivity \\ncontrol, \\ncompetitive \\nselection \\nand \\nbottom-up filtering for salient stimuli. The first three \\nprocesses work in a recurrent loop to implement top-\\ndown control attention. Working memory is \\nintimately linked to attention as its contents are \\ndetermined by attention. This framework seems to \\ncapture most of the essential components of \\nattention and is a promising candidate for inspiration \\nwith regards to attention for AI. \\n \\n3 ATTENTION AND AI \\nLet us now consider how the previous chapter can \\ninspire implementation of attentional capabilities for \\nAI systems. As suggested in the introduction, we \\nspecifically target general AI systems designed to \\noperate in complex environments under real-time \\nconstraints with limited resources. These systems are \\nexpected to perform various tasks while being \\nreactive to events in the environment, a requirement \\nthat  maps  neatly to the  top-down  and  bottom-up a \\nworkings  of  attention  mentioned  earlier. Both of \\n \\nFigure 1: The Knudsen attention framework (from \\nKnudsen 2007). Data flows up from the environment, \\npasses through salience filters (which detect infrequent or \\nimportant stimuli) and activates neural representations, \\nwhich encode various types of knowledge. The activation \\nof neural representations is also influenced by working \\nmemory via the sensitivity control of top-down attention \\nthat \\nadjusts \\nactivation \\nthresholds \\nof \\nindividual \\nrepresentations. Representations compete for access to \\nworking memory with only the most active ones being \\nadmitted. Gaze is controlled by working memory and the \\nselection process. \\nthese are necessary for a complete system; those that \\nimplement only top-down down attention will \\ncontinue to work on tasks without being able to react \\nto unexpected or novel events in the environment – \\nevents that may be relevant to the current task or \\nnecessary triggers for generation of new ones. \\nConversely, systems implementing only bottom-up \\nattention cannot perform tasks beyond those that are \\nsimple and reactive; tasks consisting of multiple \\nsteps are not possible. However, when these two \\ntypes of attention are properly combined, the result \\nis a flexible system capable of performing complex \\ntasks while being faced with interruptions and \\nunexpected events. Part of the role of attention \\ntherefore, is to manage the balance between these \\ntwo at every point in time. \\nThe early vs. late selection debate mentioned in \\nthe previous chapter is also relevant here. It is \\npossible to implement attention mechanisms for AI \\nsystems that perform selection early in the sensory'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 3}, page_content='pipeline based on primitive features of the data. This \\napproach is adopted in some of the best known \\nexisting cognitive architectures, such as SOAR \\n(Laird 2008), where attention is viewed as a \\nperceptual process rather than a cognitive one. Early \\nselection unavoidably means that some data is \\n(partly or fully) ignored without being processed for \\nmeaning; ignoring data that is not understood by the \\nsystem introduces considerable risk as its relevance \\nfor the system is not known. This may be acceptable \\nfor narrow AI systems designed for specific tasks in \\nspecific environments as it may be possible to create \\nshortcuts to understand the nature of incoming \\ninformation in such cases. However, for general AI \\n(AGI) systems designed for tasks and environments \\nnot specified at implementation time, this is highly \\nproblematic. Early stages of the sensory pipeline can \\ncontribute to attention in useful ways, such as \\nperforming biasing as opposed to absolute selection. \\nFor example, such biasing might be based on \\nnovelty or unexpectedness of the data as these \\nproperties may give rough clues to the importance of \\nthe information without requiring the information to \\nbe processed for meaning. Furthermore, this is a \\nreasonable way to implement bottom-up attention, as \\nsuggested by the Knudsen model in Figure 1. As \\nshallow processing at early stages of the sensory \\npipeline seems unlikely to provide a reliable \\nmeasure of the importance of information, the late \\nselection paradigm seems more promising than early \\nselection in terms of AI and attention. \\nTop-down attention may be viewed as a goal-\\ndriven process as it is intimately related to current \\ngoals of the system. For goals to direct top-down \\nattention, their level of specification is critical. In a \\nsystem where goals are fully specified in terms of \\noperation, the goal definition will be extremely \\nuseful in adjusting attention to elements that are \\nrelevant to the goal. A top-down attention \\nmechanism based on pattern matching could \\ngenerate partially specified patterns from goal \\nspecifications and attempt to find matches in sensory \\ninformation. Predictions and expectations may also \\nbe expected to be necessary control input for top-\\ndown attention in systems that explicitly implement \\npredictive capabilities – and there is good reason to \\nbelieve that this is necessary in order to approach \\nhuman-level intelligence. In terms of top-down \\nattention, predictions may be treated in virtually the \\nsame fashion as goals (with level of specification \\nbeing equally important as for goals). \\n \\n4 AI ATTENTION: BEYOND THE \\nHUMAN LEVEL \\nAI systems have an interesting advantage over \\nhuman minds; they are based on software rather than \\nhardware (“wetware”). While neurons of our brains \\ncan adaptively wire up to encode skills, knowledge \\nand experiences the core mechanisms of these \\nprocesses are fixed. For example, humans cannot \\neasily acquire dramatically better ways of learning \\nor remembering. This limitation does not apply to \\nsoftware AI systems; their potential for flexibility \\nand reconfiguration are only limited by their \\narchitectural design. The same can be said for their \\nlevel of introspection; our introspective capabilities \\nare greatly limited - we only have a very vague \\nsense of what is going on in our minds. On the other \\nhand, there are much weaker limitations on self-\\nobservation in software AI systems, which again are \\nlimited only by architectural design. \\nA case for flexible architectures capable of \\nautonomous \\nself-reconfiguration \\nis \\nmade \\nin \\n(Thórisson 2009). There are limitations on the \\ncomplexity of manually built software systems and it \\nis not unreasonable to assume that more complex \\nsoftware systems than exist today are needed in \\norder to approach human-like AI. If our chances of \\nmanually building such systems are low, having the \\nsystems build themselves (in a sense) from \\nexperience is not an unreasonable line of research. \\nIn order to perform deep levels of introspection \\nin complex AI systems, attention is equally useful as \\nfor information originating outside the system; the \\nsum of activity within such a system can be \\nconsidered to be a vast stream of information and \\nsystem resources remain limited. Determining which \\nparts of this stream are worth processing in order to \\nachieve meta-cognitive goals may be considered as \\nthe role of attention, in much the same way as \\nattention operates on environmental information. \\nThe main purpose of introspection is to provide \\ninformation to direct self-reconfiguration of the \\nsystem. For example, an observation that system \\nprocess P fails repeatedly in certain contexts can be \\nused by the system to shut down process P and \\nactivate a different process (which may exist or need \\nto be created/learned, generating a new meta-\\ncognitive goal) when such contexts occur in the \\nfuture.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 4}, page_content='5 AN ATTENTION MECHANISM \\nFOR AI SYSTEMS \\nThis section presents a design of one possible \\nattention mechanism for AI systems which addresses \\nthe \\nconcepts \\nrelated \\nto \\nattention \\ndiscussed \\npreviously. The implementation and evaluation of \\nthis mechanism is upcoming future work. The \\napproach \\ntaken \\nadopts \\nthe \\ntheoretical \\nand \\nmethodological framework presented in Thórisson \\n(2009). \\nAs attention is a ubiquitous cognitive process \\nthat cannot be easily separated from the rest of the \\ncognitive \\narchitecture, \\nsome \\narchitectural \\nrequirements are unavoidable when tackling the \\ndesign of an AI attention mechanism. The attention \\nmechanism proposed here rests on the requirements \\nthat the underlying cognitive architecture has the \\nfollowing properties: \\n \\n\\uf001 \\nData-driven. \\nAll \\nprocessing \\noccurs \\nin \\nreaction to data. Processes are activated only \\nwhen paired with compatible input data \\n(fitting the input data specification of the \\nprocess). Absence of fixed control loops allow \\nfor greater flexibility and operation on \\nmultiple time scales. \\n\\uf001 \\nFine-grained. Processing and data elements \\nof the architecture are numerous and small. \\nComplex tasks require collaboration of many \\nsuch elements. Reasoning about small, simple \\ncomponents and their effects on the system is \\nmore practical than attempting to do so for \\nlarger components. \\n\\uf001 \\nPredictive capabilities. Generate predictions \\nwith regards to expected events. Expectations \\nare part of the control data of the attention \\nmechanism. \\n\\uf001 \\nUnified sensory pipeline. Data from the \\nenvironment and from within the system are \\ntreated equally. Enables systems to sense their \\nown \\noperation \\nand \\npotentially \\nallows \\ncognitive functions to be applied equally to \\ntask performance in the environment as well \\nas meta-cognitive processing (e.g. self-\\nreconfiguration). \\n \\nThe proposed attention mechanism implements \\nboth top-down and bottom-up attention. Top-down \\nattention is based on goals and predictions, which \\nserve as the basis for generation of so called \\nattentional templates (AT), which are patterns that \\ntarget data to various levels of specification. An AT \\ncan target general data (such as all data from a single \\nmodality, e.g. auditory) or more specific data such as \\nanything directly related to an object or location in \\nthe environment and everything in between. As the \\narchitecture implements a unified sensory pipeline, \\nsensory data and internal data are targeted in an \\nidentical fashion by attention. When a data object \\nmatches an active AT, it becomes a candidate to \\nserve as input to a process for which it is compatible \\nas input. Data objects that do not match any active \\nAT are not caught by top-down attention and cannot \\ntrigger processing (unless caught by bottom-up \\nattention). Each AT is created with an associated \\npriority value, which is used when a match occurs \\nwith data, where the matching data item is assigned \\nthe same value. This value initially comes from the \\ngoal or prediction used to generate the AT. The \\nassignment of priority values to data upon a match \\nwith an AT is called biasing. Available resources of \\nthe system are allocated to data items in order of \\ntheir priority; data items with high priority values \\n(greatest bias) will have better chances of receiving \\nprocessing than those with lower values. \\nBottom-up attention is implemented by primitive \\ndata selection principles that attempt to quantify the \\nnovelty and unexpectedness of input data based on \\ncontent, temporal factors and operational experience. \\nThe novelty of data is based on how similar it is to \\ndata the system has previously seen, with higher \\nnovelty values being assigned to data that is \\ndifferent from previously seen data. Time also plays \\na role as data that has not been seen recently (but is \\nnot completely new to the system) will receive \\nhigher novelty values than those that have occurred \\nrecently. For example, if the environment has been \\nsilent for a while and sound is suddenly heard, \\nauditory data is considered novel and would be \\ncaught by bottom-up attention. If the sound persists \\nfor some period of time, auditory data will cease to \\nbe novel and require top-down attention in order to \\nbe processed. In this way, the bottom-up part of the \\nattention mechanism implements habituation. \\nFinally, a special mapping process is responsible \\nfor ensuring processes capable of consuming data \\ncaught by attention will be in active states. As the \\nsystem is expected to contain numerous processes \\nand data objects at any given time, attempting to \\nmatch every data object to every process to \\ndetermine if an operational match exists is not \\npractically feasible. The data-to-process mapping \\ncomponent can be viewed as an optimization that \\nreduces the number of data/process matched \\nrequired.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'format': 'PDF 1.3', 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'subject': '', 'keywords': '', 'moddate': \"D:20130115124904Z00'00'\", 'trapped': '', 'modDate': \"D:20130115124904Z00'00'\", 'creationDate': \"D:20130115124904Z00'00'\", 'page': 5}, page_content='Figure 2: Overview of the proposed attention mechanism. \\n6 CONCLUSIONS \\nAs has been shown, mapping models and \\nconcepts from attention in cognitive psychology to \\nAI systems can be useful and straightforward. \\nSurprisingly limited work has been performed on \\nattention in the field of AI given that it is a field with \\nthe ultimate goal of creating human-like intelligence \\nand that attention is clearly a critical cognitive \\nprocess for humans. The fact that the human mind \\nimplements this kind of sophisticated resource \\nmanagement while being orders of magnitude more \\ncomputationally powerful than existing computer \\nhardware today also hints at the importance of \\nattention for AI. \\nFurthermore, attention is likely to be equally \\ncritical for introspective systems such as those that \\ncan manage their own growth and adapt to \\nexperience at the architecture level. The internals of \\nthe system can be viewed dynamic and complex \\nenvironment in the same way as the task \\nenvironment. With a general and flexible attention \\nmechanism, it may be possible to apply the same \\nattention \\nmechanism \\nfor \\nboth \\nenvironments \\nsimultaneously; giving rise to AI systems that \\nperform tasks and improve their own performance \\nwhile being subject to real-time constraints and \\nresource limitations.   \\nACKNOWLEDGEMENTS \\nThis work was supported by the European Project \\nHUMANOBS – Humanoids that Learn Socio-\\nCommunicative Skills Through Observation (grant \\nnumber 231453). \\nREFERENCES \\nBaars, B. J., Franklin, S. 2009. Consciousness is \\ncomputational: The LIDA model of Global Workspace \\nTheory. \\nInternational \\nJournal \\nof \\nMachine \\nConsciousness, 2009, 1(1): p. 23-32. \\n \\nBroadbent, D. E. 1958. Perception and Communication. \\nLondon: Pergamon. \\n \\nCherry, E. C. 1953. Some experiments on the recognition \\nof speech, with one and two ears. Journal of the \\nAcoustical Society of America. Pages 975-979. \\n \\nKnudsen, E. I. 2007. Fundamental components of \\nattention. Annu Rev Neurosci, volume 30. Pages 57-78. \\n \\nJames, W. 1890. The Principles of Psychology. New York: \\nHenry Holt, Vol.1, pages 403-404. \\n \\nNorman, D. A. 1969. Memory while shadowing. \\nQuarterly Journal of Experimental Psychology, vol. \\n21, pages 85-93. \\n \\nNovianto, R., Williams, M.-A. 2009. The Role of \\nAttention \\nin \\nRobot \\nSelf-Awareness, \\nThe \\n18th \\nInternational Symposium on Robot and Human \\nInteractive Communication. Pages 1047-1053. \\n \\nLaird, J. E. 2008. Extending the SOAR cognitive \\narchitecture. In Proceedings of the artificial general \\nintelligence conference. Memphis. TN: IOS Press. \\n \\nPhillips, J. L. 2005. A biologically inspired working \\nmemory framework for robots. Proc. 27th Ann. Conf. \\nCongitive Science Society. Pages 1750-1755. \\n \\nSkubic, M., Noelle, D., Wilkes, M., Kawamura, K., \\nKeller, J.M. 2004. A biologically inspired adaptive \\nworking memory for robots. AAAI Fall Symp., \\nWorkshop on the Intersection of Cognitive Science and \\nRobotics. Washington D.C. 2004. \\n \\nSun, R. 2006. The CLARION cognitive architecture: \\nExtending cognitive modelling to social simulation. \\nIn: Ron Sun (ed.), Cognition and Multi-Agent \\nInteraction. Cambridge University Press, New York. \\n \\nThórisson, K. R. 2009. From Constructionist to \\nConstructivist A.I. Keynote, Technical Report, FS-90-\\n01, AAAI press, Menlo Park, California. \\n \\nWang, P. 1995. Non-Axiomatic Reasoning System: \\nExploring the Essence of Intelligence. Ph.D. diss., Dept. \\nof Computer Science, Indiana Univ., CITY, Indiana.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'trapped': '', 'modDate': 'D:20250409011006Z', 'creationDate': 'D:20250409011006Z', 'page': 0}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/390598346\\nVector Embeddings Unveiled: A Comprehensive Exploration of Their Creation,\\nTypes, Applications, Challenges, and Future Directions in Machine Learning\\nResearch · April 2025\\nDOI: 10.13140/RG.2.2.15544.05129\\nCITATIONS\\n0\\nREADS\\n381\\n1 author:\\nPaul Pajo\\nDe La Salle-College of Saint Benilde\\n109 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Paul Pajo on 09 April 2025.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'trapped': '', 'modDate': 'D:20250409011006Z', 'creationDate': 'D:20250409011006Z', 'page': 1}, page_content='Vector Embeddings Unveiled: A Comprehensive Exploration of\\nTheir Creation, Types, Applications, Challenges, and Future\\nDirections in Machine Learning\\nby Paul Pajo∗\\nApril 9, 2025\\nAbstract\\nVector embeddings, numerical representations of complex data such as text, images,\\nand audio, have become foundational in machine learning by encoding semantic relation-\\nships in high-dimensional spaces.\\nThis paper provides a thorough examination of their\\ncreation via neural networks (e.g., Word2Vec, BERT, CLIP), categorization into word, sen-\\ntence, document, image, audio, and multimodal types, and diverse applications including\\nsemantic search, recommendation systems, and generative AI. We analyze persistent chal-\\nlenges—high dimensionality, interpretability, and scalability—and recent advancements like\\ncontextual embeddings, vector databases, and multimodal integration, supported by empiri-\\ncal evidence and theoretical insights. Our findings highlight embeddings’ transformative role\\nin AI, with static models like Word2Vec offering efficiency and contextual models like BERT\\nenhancing semantic precision, though at increased computational cost. We conclude that\\nvector embeddings bridge human-like understanding and machine processing, with future\\nresearch poised to address efficiency, bias mitigation, and cross-modal generalization. This\\nwork synthesizes current knowledge and charts a path for advancing embedding technologies.\\n1\\nIntroduction\\nVector embeddings represent a cornerstone of modern artificial intelligence (AI), enabling ma-\\nchines to process and understand unstructured data—such as text, images, or sounds—by con-\\nverting them into numerical vectors. These vectors, situated in a high-dimensional space, posi-\\ntion similar items closer together, reflecting their semantic or relational proximity. For example,\\nin natural language processing (NLP), the words ”king” and ”queen” might occupy nearby coor-\\ndinates, capturing their shared royal context [13]. This intuitive yet powerful concept underpins\\napplications ranging from search engines to generative models like DALL-E.\\nFor those new to the field, consider embeddings as a translation mechanism: just as a\\ndictionary translates words between languages, embeddings translate diverse data into a format\\nmachines can analyze. This translation preserves meaning, allowing AI to perform tasks like\\nfinding similar documents or recommending movies.\\nHowever, creating and utilizing these\\nembeddings involves complex processes, diverse methodologies, and significant challenges.\\nThis paper aims to demystify vector embeddings by exploring their creation, types, ap-\\nplications, challenges, and advancements. We provide a structured analysis for both novices\\nand experts, bolstered by citations to seminal works and recent studies. We also propose fu-\\nture research directions to address unresolved issues, ensuring a comprehensive resource for\\nunderstanding this pivotal technology.\\n∗thanks Grok(xAI) from paulamerigo.pajojr@benilde.edu.ph\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'trapped': '', 'modDate': 'D:20250409011006Z', 'creationDate': 'D:20250409011006Z', 'page': 2}, page_content='2\\nCreation of Vector Embeddings\\nVector embeddings are generated through a multi-step process involving neural networks trained\\non large datasets:\\n1. Data Collection: A representative dataset is amassed, such as a text corpus (e.g.,\\nWikipedia) for NLP or an image set (e.g., ImageNet) for vision tasks [3].\\n2. Preprocessing: Data is cleaned—text tokenized into words, images resized—to ensure\\nuniformity [9].\\n3. Model Training: Neural networks map data to vectors. Word2Vec predicts word con-\\ntexts [13], BERT uses transformers for contextual understanding [4], and CNNs extract\\nimage features [10].\\n4. Embedding Generation: Trained models produce vectors for new inputs, preserving\\nlearned relationships.\\nModels like Word2Vec (trained on 100 billion words [14]) and CLIP (400 million image-\\ncaption pairs [18]) exemplify this process, balancing efficiency and semantic depth.\\n3\\nTypes of Vector Embeddings\\nEmbeddings vary by data type and granularity: - Word Embeddings: Represent words (e.g.,\\nWord2Vec, 300 dimensions [13]; GloVe [15]). - Sentence Embeddings: Capture sentence\\nmeaning (e.g., Sentence-BERT [20]). - Document Embeddings: Encode entire texts (e.g.,\\nDoc2Vec [11]).\\n- Image Embeddings: Represent visual content (e.g., CNNs [6]).\\n- Au-\\ndio Embeddings: Model sound features (e.g., VGGish [7]). - Multimodal Embeddings:\\nCombine data types (e.g., CLIP [18]).\\nEach type addresses specific needs, from fine-grained word analysis to cross-modal tasks.\\n4\\nApplications\\nEmbeddings enable diverse applications: - Semantic Search: Match queries to content by\\nmeaning [17]. - Recommendation Systems: Suggest items via vector proximity [21]. - NLP\\nTasks: Enhance translation, sentiment analysis [9]. - Generative AI: Power text-to-image\\nmodels [19]. - Multimodal Tasks: Enable image captioning [18].\\nThese applications demonstrate embeddings’ versatility across industries.\\n5\\nChallenges\\nKey challenges include: - High Dimensionality: Increases computational cost [1]. - Inter-\\npretability: Dimensions lack clear meaning [12]. - Contextual Limitations: Static embed-\\ndings miss nuance [16]. - Scalability: Large datasets strain storage [22].\\n6\\nAdvancements\\nRecent innovations address these issues: - Contextual Embeddings: BERT improves pol-\\nysemy handling [4]. - Vector Databases: Pinecone, Weaviate enhance scalability [17, 22].\\n- Multimodal Models: CLIP integrates data types [18].\\n- Compression: Quantization\\nreduces resource use [5].\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'trapped': '', 'modDate': 'D:20250409011006Z', 'creationDate': 'D:20250409011006Z', 'page': 3}, page_content='7\\nAnalysis\\nStatic embeddings (e.g., Word2Vec) offer efficiency for resource-constrained systems, with train-\\ning times in hours, while contextual models (e.g., BERT) demand days but excel in accuracy\\n[4, 13]. Multimodal embeddings like CLIP suggest a convergence of data modalities, though\\ntheir 512-dimensional vectors require optimization [18]. Vector databases mitigate scalability,\\nsupporting real-time applications [8].\\n8\\nConclusion\\nVector embeddings are a linchpin of AI, translating complex data into actionable represen-\\ntations. Their evolution from static to contextual and multimodal forms reflects a trade-off\\nbetween efficiency and richness, with infrastructure like vector databases ensuring practical\\ndeployment. Future research should focus on: - Efficiency: Optimizing contextual models\\nfor edge devices. - Bias Mitigation: Addressing training data biases [2]. - Cross-Modal\\nGeneralization: Enhancing multimodal robustness.\\nEmbeddings will continue shaping AI’s ability to mirror human understanding.\\nReferences\\n[1] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural prob-\\nabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.\\n[2] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.\\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings.\\nAdvances in Neural Information Processing Systems, 29, 2016.\\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\\nscale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 248–255, 2009.\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\nBert:\\nPre-\\ntraining of deep bidirectional transformers for language understanding.\\narXiv preprint\\narXiv:1810.04805, 2018.\\n[5] Robert M Gray. Quantization and Data Compression. Springer, 2011.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[7] Shawn Hershey, Sourish Chaudhuri, Daniel P W Ellis, Jort F Gemmeke, Aren Jansen,\\nR Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, et al. Cnn architectures for\\nlarge-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pages 131–135, 2017.\\n[8] Jeff Johnson, Matthijs Douze, and Herv´e J´egou. Billion-scale similarity search with gpus.\\nIEEE Transactions on Big Data, 7(3):535–547, 2019.\\n[9] Daniel Jurafsky and James H Martin. Speech and Language Processing. Pearson, 2009.\\n[10] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\\nconvolutional neural networks. Advances in Neural Information Processing Systems, 25,\\n2012.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'trapped': '', 'modDate': 'D:20250409011006Z', 'creationDate': 'D:20250409011006Z', 'page': 4}, page_content='[11] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents.\\nInternational Conference on Machine Learning, pages 1188–1196, 2014.\\n[12] Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018.\\n[13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\\nrepresentations of words and phrases and their compositionality. Advances in Neural In-\\nformation Processing Systems, 26, 2013.\\n[15] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\\nword representation. Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 1532–1543, 2014.\\n[16] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\\nLee, and Luke Zettlemoyer.\\nDeep contextualized word representations.\\narXiv preprint\\narXiv:1802.05365, 2018.\\n[17] Pinecone.\\nWhat\\nare\\nvector\\nembeddings.\\nhttps://www.pinecone.io/learn/\\nvector-embeddings/, 2023.\\n[18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning trans-\\nferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020,\\n2021.\\n[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\\nMark Chen, and Ilya Sutskever.\\nZero-shot text-to-image generation.\\narXiv preprint\\narXiv:2102.12092, 2021.\\n[20] Nils Reimers and Iryna Gurevych.\\nSentence-bert: Sentence embeddings using siamese\\nbert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[21] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B Kantor, editors. Recommender\\nSystems Handbook. Springer, 2011.\\n[22] Weaviate.\\nVector\\nembeddings\\nexplained.\\nhttps://weaviate.io/blog/\\nvector-embeddings-explained, 2023.\\n4\\nView publication stats'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 0}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilities.\\nBuilt upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁed multi-task frame-\\nwork comprising specialized data transformation and training strategies.\\nThe\\ndata transformation scheme enables the incorporation of more diverse textual\\ntraining datasets, while the task-speciﬁc training strategies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline leveraging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentation, and Hard negative\\nexample generation to improve the semantic richness and sample diﬃculty of\\nthe training set. Additionally, we employ a two-stage training strategy, compris-\\ning initial retrieval-focused pretraining followed by full-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robust retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more diverse data is crucial for\\nadvancing retrieval model performance, and that leveraging LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructions on GitHub2.\\n1\\nIntroduction\\nText embedding models, which transform natural language text into mathematical vec-\\ntor representations, play an indispensable role in text mining, question-answering sys-\\ntems, recommendation systems, and retrieval-augmented generation. Recently, LLM-\\nbased agent technology has experienced rapid development and widespread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhanced agent systems\\n∗https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 1}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nin terms of real-time performance, long-term memory, data privacy preservation, and\\nknowledge integration capabilities. With the continuous advancement of neural net-\\nworks and deep learning, text embeddings have evolved from early sparse representa-\\ntions (e.g., BM25[1]) to dense representations based on ﬁne-tuned deep networks such\\nas BERT[2] and T5[3], leading to signiﬁcant performance improvements[4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatGPT[9], ushered in\\na new era of text embeddings based on LLM representations, including models like text-\\nembedding-3-large and RepLLaMA[10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For instance, to address\\nthe limitation of decoder-only architectures—where causal attention mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—several approaches have\\nbeen proposed: Echo Embedding[11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semantics. LLM2Vec[12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dependency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction.\\nAnother widely adopted approach is knowledge distillation,\\nwhere text embeddings are treated as the ”signal states” representing textual seman-\\ntics. By distilling knowledge from high-performing teacher models to student models,\\nthe objective is to optimize the embedding performance. For instance, Jasper[14] em-\\nploys a multi-stage knowledge distillation framework, combining with multiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debater[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, iteratively optimizing doc-\\nument representations through continuous COT. Distillation is applied to constrain\\nthe ﬁnal token representation to learn the optimal semantic states from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for model optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to diminishing gra-\\ndient norms during optimization. Thus they developed an asynchronous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refreshes the negative\\nsample pool using the current model parameters, thereby ensuring the maintenance\\nof up-to-date and optimally challenging negative samples. Both Conan-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering mechanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerful Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed multi-task learn-\\ning framework that not only accommodates more diverse training data but also bring\\neﬃcient learning across three key tasks: retrieval, natural language inference (NLI),\\nand classiﬁcation. Our framework comprises two core components: 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc requirements of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extraction from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s characteristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and generalization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 2}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ntor representation, we propose a data synthesis method by employing three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building upon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling batch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a two-phase train-\\ning approach, through the ﬁrst-stage retrieval training and second-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilities, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state-of-the-art av-\\nerage scores on CMTEB[22] and MTEB[23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness of our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematically coordi-\\nnates both data processing and training pipelines, enhancing diversity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, including Para-\\nphrasing, Data augmentation, and Hard negative generation.\\nThese methods\\nsigniﬁcantly enhance the quality of training corpora, thereby improving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval performance; and\\nstage 2 implements balanced training with controled retrieval/non-retrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pair classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintaining re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTEB and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed methods.\\n2\\nRelated Works\\n2.1\\nText Embedding Models\\nText vector representation is a fundamental research area in natural language processing\\n(NLP) and serves as the cornerstone for language understanding. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[25], BM25[26], and LSA[27]. With\\nthe advent of pretrained language models, dense contextualized representations based\\non architectures like BERT[2] and T5[3] became widely studied and applied[4][5][6]. In\\nthe era of large language models (LLMs), major advancements have led to the devel-\\nopment of LLM-based embedding models, such as text-embedding-3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—such\\nas RoPE positional encoding[35], RMSNorm[36], and GeGLU activation[37]—combined\\nwith their strong semantic contextualization capabilities acquired through large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 3}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior performance in re-\\ntrieval and related tasks.\\n2.2\\nEmbedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrastive learning training on\\nhigh-quality labeled positive and negative samples.\\nIn unsupervised learning, early\\nwork like SimCSE[7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance the model’s dis-\\ncriminative representation capability. For weakly supervised learning, gte[33] utilized\\nlarge-scale structured data (web search data, title-article pairs, etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval training data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fewer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to optimize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tuning, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀerent tasks. Piccolo2[39]\\nintroduced multi-task hybrid loss functions for diverse downstream tasks, an approach\\nwe also incorporate.\\nSFR-Embedding[30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimination. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categories from the per-\\nspective of circle loss[40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3\\nData Synthesis\\nData quantity and quality are the most critical factors in model optimization, data\\nsynthesis methods have become a critical research direction due to the high cost of\\nmanual annotation.\\nDoc2Query[41] and Query2Doc[42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents respectively, enhancing data\\nfor improved RAG performance.\\nPromptagator[43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot demonstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varying intents or distri-\\nbutions.\\nGPL[44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieval models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unnatural Instructions[45]\\nleverages prompt and in-context learning (ICL) techniques to generate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experimental results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-speciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 4}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n2.4\\nHard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive learning for retrieval model\\ntraining. Early work like ANCE[46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint states to maintain\\noptimally challenging samples. Conan-Embedding[24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refreshing samples when\\ntheir scores fall below a threshold. NV-Retriever[47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering criteria to minimize\\nfalse negatives. LGAI-Embedding[17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁltering to eliminate\\nfalse negatives.\\n3\\nUniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including retrieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them collectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-wise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[39], SFR-Embedding[30], NV-Embed[47], Conan-Embedding[24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning using diverse train-\\ning data with varying label processing methods, some employing task-speciﬁc losses\\n(InfoNCE[48], Cosent[49], etc.).\\nOur design principle aims to accommodate more tasks and data types, enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capabilities. We propose\\na uniﬁed multi-task learning framework that categorizes training data into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into embedding training data\\nthrough this framework. The following sections detail the framework’s components and\\nimplementation methods.\\n3.1\\nModel Architecture\\nEmbedding models based on BERT or T5 [39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirectional attention mech-\\nanisms. However, recent large language models predominantly adopt decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constraining tokens’ ability to\\ncapture contextual information. Several studies have addressed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoint due to its exceptional\\nChinese language contextual capabilities. Consequently, we implemented the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-directional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 5}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The model architecture is\\nshown in Figure 1\\n3.2\\nData Transformation\\n3.2.1\\nRetrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper and QA datasets.\\nGiven the heterogeneous nature of these datasets across domains and purposes, we\\ndesign a retrieval-oriented data transformation methodology to convert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their processing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transformation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is applied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commonly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets generally contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 6}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted into a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits remarkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question paired with one\\nanswer) represents the most suitable format for retrieval training. For transfor-\\nmation, the ”Question/Query/User” portion is converted into queries, while the\\n”Answer/Response/Assistant” portion is processed as documents.\\n3.2.2\\nNLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment, and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) and textual entailment\\ntasks as illustrative examples. Our approach distinctively reformulates NLI tasks into\\ntext pair-score formats compatible with Cosent loss[49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationships. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric se-\\nmantic matching to determine whether two sentences share equivalent meaning.\\nSTS datasets typically consist of sentence pairs with associated labels, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical scores (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical value of 1, while\\n”no”/”false” are converted to 0. The data is then structured into (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each single original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neutral, contradic-\\ntion.\\nOur processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contradiction respec-\\ntively. We construct (query, document, score) triplets accordingly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3\\nCLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the same category\\nexhibit semantic proximity while distinct boundaries separate diﬀerent classes. NV-\\nEmbed[47] compared label-based and example-based data construction methods, with\\nexperimental results demonstrating the superiority of the latter. Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by using the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 7}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, and selecting texts\\nfrom diﬀerent labels as negative examples.\\nFigure 2 provides a detailed schematic\\nillustration of this process.\\n3.3\\nTraining Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized training objectives to\\nto enhance model training eﬃciency.\\nThis section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1\\nRetrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[48], but incorporate an\\nimprovement inspired by gte[33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a batch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is explicitly described\\nin Equation (1).\\nLRetrieval = −1\\nn\\nX\\ni\\nlog\\nesim(qi,d+\\ni )/τ\\nesim(qi,d+\\ni )/τ + P\\nj esim(qi,d−\\nj )/τ + P\\nj̸=i esim(qi,qj)/τ\\n(1)\\n3.3.2\\nNLI\\nFor NLI tasks, the transformed labels are numerically comparable and exhibit ordinal\\nrelationships.\\nWe employ Cosent loss[49] to optimize such data, which is designed\\nbased on the principles of Circle loss[40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demonstrating faster\\nconvergence. Its mathematical formulation is presented in Equation (2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 8}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nLNLI = log(1 +\\nX\\nsim(i,j)>sim(k,l)\\nexp(sim(xk, xl) −sim(xi, xj)\\nτ\\n))\\n(2)\\n3.3.3\\nCLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However, since CLS data is\\nprocessed in an example-based manner, directly applying in-batch negative sampling\\non classiﬁcation datasets with limited categories may lead to false negatives from items\\nof diﬀerent classes.\\nNumerous studies have proposed diverse approaches to address\\nthis issue[51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling, for each negative\\nsample from other data instances, we check whether its label matches the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remains InfoNCE, with\\nthe CLS loss formulation shown in Equation (3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = −1\\nn\\nX\\ni\\nlog esim(ti,t+\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t+\\ni )/τ +\\nX\\nn\\nMASK(ti, t−\\ni,n) · esim(ti,t−\\ni,n)/τ+\\nX\\nj̸=i\\nMASK(ti, tj) · esim(ti,tj)/τ+\\nX\\nj̸=i\\nX\\nn\\nMASK(ti, t−\\nj,n) · esim(ti,t−\\nj,n)/τ\\nand Cti = Ct+\\ni\\nand MASK(ti, tj) =\\n(\\n0\\nif Cti = Ctj,\\n1\\notherwise\\n4\\nData Synthesis\\nThe production of higher-quality data through data production has gained critical im-\\nportance in embedding training.\\nManual annotation incurs higher costs and lower\\nproduction eﬃciency, thus developing eﬀective automated data synthesis methods has\\nemerged as a key research focus. Recent advancements in large language models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accurate interpretation\\nof human instructions and generation of high-quality outputs. Multiple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 9}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nleverages LLM capabilities for data production across three dimensions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃculty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The constraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1\\nStructural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and grammatical features,\\nwhich represent relatively surface-level characteristics reﬂecting word arrangements,\\ncombinations, tenses, voices, and other formal attributes.\\nEmbedding models must\\naccurately capture underlying semantics despite variations in surface form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantically equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to structural variations\\nwhile accurately capturing semantic information, we propose a Paraphrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented instances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure 3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2\\nSemantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcations yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure uniform vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphrasing, we propose an\\naugmentation method using LLM to diversify semantics. The core concept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 10}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the domain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, and viewpoints while\\nremaining contextually anchored. This process is governed via prompt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3\\nMore challenging embeddings\\nHard negative examples are crucial for enhancing the performance of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, the framework is\\nillustrated in Figure 5.\\nDuring Data paraphrasing and Augmentation, we implement task-speciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs and add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by randomly duplicating\\nexisting entries containing the original sentences and replacing them with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 11}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nFigure 6: Training pipeline\\n5\\nTraining Optimization\\n5.1\\nData Grouping Strategy\\nPrior works like Linq-Embedding[52] and SFR-Embedding-Mistral[30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixing them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointers recorded to\\nenable sequential reading in subsequent iterations. For sampling weights, we adopt\\nthe data sampling strategy from gte[33] and mgte[50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation (4)\\npi =\\nlα\\ni\\nPm\\nj=1 lα\\nj\\n(4)\\n5.2\\nTwo-Stage Training\\nInspired by NV-Embed’s[47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusively uses retrieval-\\noriented training data, while the second stage integrates both retrieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Data Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating additional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 12}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nfunction to control the proportion of retrieval training, ensuring that throughout the\\nsecond training stage, the computational contribution of retrieval data accounts for η,\\nwhile non-retrieval data constitutes 1−η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling ratio determination. Let\\nthe training data D = [d1, d2, ..., dN] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [l1, l2, ..., lN]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scaling factor α, a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets for summation.\\nThe equations are as follows:\\nSret =\\nX\\ni\\nMi · lα\\ni\\nSnon ret =\\nX\\ni\\n(1 −Mi) · lα\\ni\\nwhere Mi =\\n(\\n0\\nif di ∈RET,\\n1\\nelse\\nwhere RET denotes the set of retrieval training datasets. The retrieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [lsamp\\n1\\n, lsamp\\n2\\n, ...lsamp\\nN\\n]\\nwhere lsamp\\ni\\n=\\n(ηRET ·lα\\ni\\nSret\\nif di ∈RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6\\nExperiments\\n6.1\\nTraining Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-multilingual-gemma2-\\ndata 3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[28],\\nEcho Embedding[11], and LLM2Vec[12], is also incorporated.\\nThe aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[64], Natural Questions (NQ)[65], ELI5[66], HotpotQA[67],\\nMIRACL[68], SQuAD[69], FEVER[70], Quora Question Pairs(QQP), and DuReader[71],\\netc.\\nPrevious researchers have already systematically collected and organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such as Huatuo medical QA6,\\nall above data has been incorporated. Additional data from huggingface’s sentence-\\ntransformers7 repository includes reddit, hover[72], mr-tydi[73], law-gpt, and s2orc[74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 13}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nOther sources encompass web questions, BioASQ[54], cmrc[55], CSL8, nli for simcse\\n(used in SimCSE[7] and GTE[33]), MLDR9, GLUE Benchmark[56], Yelp Reviews[57]\\nand Weibo Sentiment10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb-Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test sets.\\nFor data requiring format conversion, we apply the methodologies described in Sen-\\ntion 3.2.\\nDatasets with limited samples (e.g., subsets of bge and e5 series, Imdb-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultimately obtained ap-\\nproximately 5M high-quality training samples through API interfaces. We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores using GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic hard negative\\ngeneration. Due to API cost constraints, only 30% of hard negatives are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3-1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal training dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2\\nTrainset Instructions\\nFor most training data containing instruction formats, we retain their original con-\\ntents. For the MTEB training set, we adopt instructions corresponding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external data lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-speciﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix A.2.\\n6.3\\nTraining Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm-up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the second stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k steps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considering data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the absence of forward\\ncomputation for negative samples, the batch size is set to 768. Across all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine temperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Grouping Strategy\\nremains unchanged between the two stages, except that the second stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 14}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all stages to ensure\\nmaximum performance improvement. The query and passage lengths are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoPE[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations for all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem\\nStage1\\nStage2\\nWarm-up\\n300\\nSteps\\n3e-5\\n2e-5\\nLR\\n32k\\n8k\\nBatch Size InfoNCE\\n256\\nBatch Size Cosent\\n-\\n768\\nPrecision\\nbﬂoat16\\nTemperature\\n0.02\\nOptimizer\\nAdam\\nQuery Length\\n256\\nPassage Length\\n1536\\n6.4\\nCompared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MTEB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For MTEB, the compar-\\native models include LGAI-Embedding-Preview[17], the Seed series (v1.5[75] , v1.6[38]),\\nQwen series (8B, 4B)[34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[30],\\nand NV-Embed-v2[47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[39].\\n6.5\\nMain Results\\nThis section presents the evaluation results of Qzhou-embedding on MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranked models. As detailed\\nin Table 2, Table 3, Qzhou-embedding achieves state-of-the-art performance across\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach.\\nFurthermore, under MTEB’s oﬃcial ranking protocol, Qzhou-embedding\\nsecured the top position on both leaderboards. (Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the listed models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/classiﬁcation\\nbenchmark where the top score does not appear in the top 10 models.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 15}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nSumm.\\nMean(Task)\\nMean(TaskType)\\nLGAI-Embedding-Preview\\n89.97\\n59.25\\n88.67\\n49.13\\n66.18\\n86.69\\n38.93\\n74.12\\n68.4\\nSeed1.5-Embedding\\n89.88\\n60.83\\n87.39\\n50.67\\n67.45\\n87.23\\n36.44\\n74.76\\n68.56\\nQwen3-Embedding-8B\\n90.43\\n58.57\\n87.52\\n51.56\\n69.44\\n88.58\\n34.83\\n75.22\\n68.71\\nQwen3-Embedding-4B\\n89.84\\n57.51\\n87.01\\n50.76\\n68.46\\n88.72\\n34.39\\n74.6\\n68.1\\nSeed1.6-embedding\\n92.42\\n59.22\\n85.07\\n50.28\\n64.9\\n86.87\\n37.1\\n74.07\\n67.98\\ngemini-embedding-001\\n90.05\\n59.39\\n87.7\\n48.59\\n64.35\\n85.29\\n38.28\\n73.3\\n67.67\\njasper en vision language v1\\n90.27\\n60.52\\n88.14\\n50\\n56.05\\n84.37\\n37.19\\n71.41\\n66.65\\nLinq-Embed-Mistral\\n83\\n54.07\\n88.44\\n49.44\\n60.14\\n84.69\\n37.26\\n69.8\\n65.29\\nSFR-Embedding-Mistral\\n80.47\\n54.93\\n88.59\\n50.15\\n59.33\\n84.77\\n36.32\\n69.31\\n64.94\\nNV-Embed-v2\\n87.19\\n47.66\\n88.69\\n49.61\\n62.84\\n83.82\\n35.21\\n69.81\\n65\\nQZhou-Embedding(Ours)\\n88.97\\n61.65\\n92.43\\n51.77\\n67.12\\n91.65\\n33.05\\n75.97\\n69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel\\nClass.\\nClust.\\nPair Class.\\nRerank.\\nSTS\\nRetr.\\nMean(Task)\\nMean(TaskType)\\nSeed1.6-embedding\\n77.98\\n73.11\\n88.71\\n71.65\\n79.69\\n68.94\\n75.63\\n76.68\\nSeed1.5-Embedding\\n79.37\\n71.11\\n89.57\\n70.14\\n79.33\\n66.56\\n74.87\\n76.01\\nritrieve zh v1\\n76.88\\n66.5\\n85.98\\n72.86\\n76.97\\n63.92\\n72.71\\n73.85\\nConan-embedding-v2\\n76.47\\n68.84\\n92.44\\n74.41\\n78.31\\n65.48\\n74.24\\n75.99\\nxiaobu-embedding-v2\\n76.53\\n65.17\\n85.94\\n72.58\\n76.49\\n64.18\\n72.36\\n73.48\\nQwen3-Embedding-8B\\n76.97\\n80.08\\n84.23\\n66.99\\n78.21\\n63.53\\n73.84\\n75\\nConan-embedding-v1\\n76.77\\n66.33\\n85.68\\n72.76\\n76.67\\n63.67\\n72.5\\n73.65\\nzpoint large embedding zh\\n76.4\\n62.23\\n85.75\\n72.33\\n76.36\\n63.86\\n71.81\\n72.82\\npiccolo-large-zh-v2\\n76.42\\n62.16\\n85.22\\n70\\n74.36\\n63.46\\n70.86\\n71.94\\nQwen3-Embedding-4B\\n75.46\\n77.89\\n83.34\\n66.05\\n77.03\\n61.26\\n72.27\\n73.51\\nQZhou-Embedding(Ours)\\n79.99\\n70.91\\n95.07\\n74.85\\n78.80\\n71.89\\n76.99\\n78.58\\n7\\nConclusion\\nIn this technical report, we present QZhou-Embedding, a general-purpose contextual\\ntext embedding model with exceptional text representation capabilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transformation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we developed a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques such as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a two-stage training\\nstrategy comprising initial retrieval-focused training followed by full-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robust retrieval per-\\nformance.\\nThe model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings establish that data qual-\\nity and diversity are pivotal for improving embedding model capabilities. In the future,\\nwe will focus on developing multimodal and multilingual embedding models, as well\\nas exploring eﬀective applications of embedding models in agent systems, aiming to\\nintegrate cutting-edge technologies to optimize this classical module.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 16}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conference on Research and\\nDevelopment in Information Retrieval, organised by Dublin City University, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine learning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-supervised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert:\\nSentence embeddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conference on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics.\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-shot learners.”\\nAdvances in neural information processing systems 33 (2020): 1877-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”Fine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” arXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large language models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.05961 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 17}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jasper and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv:2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective representa-\\ntions for dense retrieval through deliberate thinking before search.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical report[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved techniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405.17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:2407.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu Lian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embeddings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embedding:\\nGen-\\neral text embedding with more and better negative samples.” arXiv preprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–idf measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGIR’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Conference on Research\\nand Development in Information Retrieval, organised by Dublin City University,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 18}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and\\nFuru Wei. Improving text embeddings with large language models. arXiv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with transfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Representations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learners. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage contrastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embedding.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer normalization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer,\\nNoam.\\n”Glu\\nvariants\\nimprove\\ntransformer.”\\narXiv\\npreprint\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” arXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarity op-\\ntimization.” In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 19}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with\\nlarge language models. In Proceedings of the 2023 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapore. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense\\nretrieval from 8 examples. In The Eleventh International Conference on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of dense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, pages\\n2345–2360, Seattle, United States. Association for Computational Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unnatural in-\\nstructions: Tuning language models with (almost) no human labor.” arXiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbor negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:2007.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text embedding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large language\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevating text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 20}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competition[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chinese machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis\\nplatform for natural language understanding[J]. arXiv preprint arXiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sentiment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association for computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural language understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Proceedings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computational linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAWS-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation.” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine read-\\ning comprehension dataset. In Proceedings of the Workshop on Cognitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-located with the\\n30th Annual Conference on Neural Information Processing Systems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 21}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\\net al. Natural questions: a benchmark for question answering research. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019.\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\\nMichael Auli. 2019. ELI5:\\nLong Form Question Answering. In Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages\\n3558–3567, Florence, Italy. Association for Computational Linguistics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2023.\\n[69] Pranav\\nRajpurkar,\\nJian\\nZhang,\\nKonstantin\\nLopyrev,\\nand\\nPercy\\nLiang.\\nSquad:\\n100,000+ questions for machine comprehension of text. arXiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang.\\n2018. DuReader: a Chinese Machine Reading Comprehension Dataset from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Reading for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association for Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extraction And Claim\\nVeriﬁcation. In Findings of the Association for Computational Linguistics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Linguistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark for dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 22}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar\\nNaim, Gustavo Hernandez ´ Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA\\nAppendix\\nA.1\\nFramework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem\\nExplanation\\nKeep core semantics\\nPreserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within±15%\\nThe length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language\\nThe language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld\\nThe content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 23}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nA.2\\nInstruction Examples\\nTable 5: Instruction for partial training data\\nDataset\\nInstruction\\nHuatuo\\nGiven a medical question, retrieve user replies that\\nbest answer the question\\nReddit\\nRetrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT\\nRetrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI\\nRetrieve semantically similar text\\nYelp\\nClassify the customer review of businesses\\nWeibo\\nClassify the sentiment of Weibo comments\\nA.3\\nData Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, followed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery\\npos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 24}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery\\npos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury.\\nBy the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 25}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 8: Augmentation Example\\nquery\\npos\\nneg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire\\na\\nrefundable\\nde-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards\\nmay\\naccept\\nlower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks\\noﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By\\nthe\\ntime\\nof\\nEliz-\\nabethan literature a vig-\\norous\\nliterary\\nculture\\nin\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The\\nFaerie\\nQueene’,\\nan\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater\\neclipsed\\nby\\nShake-\\nspeare’s popularity.\\nChristopher\\nMarlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork\\nit\\nwasn’t\\nan\\nepic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical\\nthemes\\nthrough\\nblank\\nverse\\nand\\nbecame\\na\\ncornerstone\\nof\\nEnglish\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso\\ndealt\\nwith\\nreligious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'file_path': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'format': 'PDF 1.4', 'title': 'QZhou-Embedding Technical Report', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'subject': '', 'keywords': '', 'moddate': '2025-09-01T00:50:53+00:00', 'trapped': '', 'modDate': \"D:20250901005053+00'00'\", 'creationDate': \"D:20250901005053+00'00'\", 'page': 26}, page_content='QZhou-Embedding Technical Report\\nKingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery\\npos\\nneg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli\\nexpansion\\nduring\\nthe\\nArab-Israeli\\nconﬂicts,\\nthough\\nhis\\nwarnings\\nto\\nNasser\\nwere delayed\\nand\\ninitially\\ndismissed,\\nwhile\\nother Arab leaders focused\\nmore\\non\\ndirect\\nmilitary\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary\\nthat\\nan\\nEgyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing\\nHussein\\nexpressed\\nconcerns\\nabout\\npotential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand\\nmixed\\nwith\\nbroader\\nregional\\ntensions,\\nwhile\\nEgyptian\\nmilitary\\nmove-\\nments in Sinai were already\\nunderway\\nunder\\nAmer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 0}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/376885449\\nArtiﬁcial Intelligence for Object Detection and its Metadata\\nArticle · December 2023\\nDOI: 10.17605/OSF.IO/FG3SQ\\nCITATIONS\\n5\\nREADS\\n2,055\\n1 author:\\nNarayana Challa\\nJawaharlal Nehru Technological University, Hyderabad\\n26 PUBLICATIONS\\xa0\\xa0\\xa049 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Narayana Challa on 04 February 2024.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 1}, page_content=\"https://iaeme.com/Home/journal/IJAIML \\n121 \\neditor@iaeme.com \\nInternational Journal of Artificial Intelligence & Machine Learning (IJAIML)  \\nVolume 2, Issue 01, Jan-Dec 2023, pp. 121-133. Article ID: IJAIML_02_01_012 \\nAvailable online at https://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1 \\nJournal ID: 9339-1263, https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\n© IAEME Publication \\nARTIFICIAL INTELLIGENCE FOR OBJECT \\nDETECTION AND ITS METADATA \\nNarayana Challa \\nDirector of ERP Strategy at Cabinetworks Group, Texas, USA \\nABSTRACT \\nIn the ever-evolving field of computer vision, the infusion of artificial intelligence \\n(AI) has inaugurated a revolutionary era, providing unparalleled precision and \\nefficiency in identifying objects within images and videos. This exploration delves into \\nthe domain of AI-driven object detection, emphasizing metadata's pivotal role in \\nenhancing the understanding and utility of recognized entities. The collaboration \\nbetween AI and metadata enhances the precision of object detection and opens up novel \\navenues for extracting and analyzing information. \\nThe significance of metadata is underscored as it contributes context and \\ncategorization to identified objects. This metadata encompasses crucial details such as \\nobject class, detection location, time of occurrence, and inter-object relationships, \\nfurnishing invaluable insights for downstream applications like autonomous vehicles, \\nsurveillance, and augmented reality. The research paper showcases the seamless \\nintegration of metadata extraction and management with AI-powered object detection \\nsystems, thereby boosting the accuracy of object identification and tracking. \\nThis study sheds light on the intricate interplay between artificial intelligence and \\ncomputer vision, molding a landscape where precision and adaptability redefine the \\nboundaries of object detection capabilities. \\nKeywords: Artificial Intelligence (AI), Machine Learning (ML), Internet of Things \\n(IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data \\nScience careers \\n \\nCite this Article: Narayana Challa, Artificial Intelligence for Object Detection and Its \\nMetadata, International Journal of Artificial Intelligence & Machine Learning \\n(IJAIML), 2(1), 2023, pp. 121-133. \\nhttps://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 2}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n122 \\neditor@iaeme.com \\nINTRODUCTION \\nArtificial Intelligence (AI) has significantly advanced in computer vision, particularly object \\ndetection. This paper explores how AI has transformed object detection, emphasizing the \\ncrucial role of metadata in enhancing its capabilities. \\nIn the digital era, the abundance of visual data, from surveillance to medical imaging and \\nself-driving cars, necessitates accurate object detection. AI, especially with deep learning \\nmodels like convolutional neural networks (CNNs), has become a robust solution, overcoming \\nchallenges such as occlusion and scale variations with increasing accuracy. \\nAI, especially utilizing CNNs, has revolutionized object detection in various sectors dealing \\nwith vast visual data. These models excel in acquiring hierarchical feature representations, \\nadapting to the dynamic nature of visual input, and continuously improving comprehension. \\nObject detection faces challenges like occlusion and background clutter. Through \\ncontinuous learning and sophisticated algorithms, AI techniques adeptly address these \\nchallenges. The adaptive nature of AI allows the development of nuanced models for accurate \\ndetection in complex environments. \\nWhile AI enhances object detection, metadata is crucial, providing additional information \\nsuch as object class, position, detection time, and associations. Integrating metadata with AI-\\ndriven systems transforms the comprehension and utility of identified objects. \\nMetadata goes beyond visual identification, adding context to detected objects. \\nUnderstanding object class, location, and temporal occurrence enhances visual data \\ninterpretation, contributing to richer information. In downstream applications like augmented \\nreality and surveillance, metadata becomes invaluable, aiding in informed decision-making. \\nThe study demonstrates the seamless integration of AI object identification systems with \\nmetadata extraction. This integration enhances object tracking and recognition accuracy, paving \\nthe way for sophisticated decision-making across various applications. \\n \\nConsequences of AI-Powered Object Identification and Integration with \\nMetadata \\nIntegrating AI with metadata for object detection has far-reaching consequences, extending \\nbeyond a single domain. This synergy can revolutionize various industries, enhancing security \\nmeasures and industrial automation capabilities. \\nThe central focus of this paper is the crucial integration of metadata with AI object detection \\nsystems, representing a defining aspect of the exploration. Metadata, containing contextual \\ninformation about detected objects, becomes pivotal for unlocking the full potential of object \\ndetection. It includes critical details like object class, detection location, time, and inter-object \\nrelationships. This symbiosis between AI and metadata refines object identification and \\ntracking and finds applications in sectors like autonomous vehicles, surveillance, and \\naugmented reality.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 3}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n123 \\neditor@iaeme.com \\nThe implications of the collaboration between AI and metadata extend into heightened \\nsecurity protocols, increased efficiency in industrial automation, and a deeper understanding of \\nthe visual environment. Ethical and privacy concerns related to AI-driven object detection and \\nmetadata are also addressed in this paper. This introduction sets the stage for a comprehensive \\nexploration of AI in object detection and its synergistic interplay with metadata, offering \\ninsights into the transformative potential of these technologies. It provides a glimpse into a \\nfuture where intelligent object detection systems redefine our perception and interaction with \\nthe visual world. \\nObject detection in computer vision involves locating and identifying items in images or \\nvideo frames, going beyond mere identification to accurately define their locations using \\nbounding boxes. This technical advancement is fundamental to applications like robots, \\naugmented reality, driverless cars, and surveillance systems, shaping how machines perceive \\nand interact with their visual surroundings. \\nObject detection commonly involves the following fundamental elements: \\nIdentifying the position and dimensions of objects within an image. This is often visualized by \\noutlining objects with bounding boxes, specifying their spatial coordinates (x, y) and \\ndimensions (width and height). \\nAssigning a label or category to each detected object, indicating its type. For instance, in a \\nscene with diverse objects, object detection can discern whether there are cars, pedestrians, \\nanimals, or other entities and assign corresponding labels. \\nDeep learning models, such as convolutional neural networks (CNNs), are frequently \\nemployed for object detection. These models are designed to handle both localization and \\nclassification tasks concurrently. Through training on extensive datasets with labeled images, \\nthey can recognize objects and accurately determine their positions. \\nEffectively training object detection models necessitates large datasets with annotated \\nimages. These datasets typically include images where objects of interest are labeled with \\nbounding boxes and associated class labels. During training, models learn from these examples \\nto precisely predict object positions and classes in new, unseen images. \\nWhat makes object detection crucial? \\nObject detection holds significant importance in computer vision for various reasons. The key \\nrationales for its significance include: \\n1. Visual Scene Content Interpretation: \\nObject detection aids in interpreting the content of visual scenes. It enables computers to \\nidentify and locate objects, providing context for more sophisticated processing. Object \\ndetection is crucial in industrial settings for monitoring product movement, automating \\nprocesses, and ensuring quality control. Additionally, it promotes greater independence for \\nindividuals with visual impairments by recognizing and characterizing objects and their \\nlocations. \\n2. Obstacle Avoidance and Navigation: \\nObject detection is essential for identifying and avoiding obstacles, navigating surroundings, \\nand making decisions based on the presence of objects. This is particularly crucial in \\nautonomous vehicles, drones, and robotics applications. For security and surveillance systems, \\nobject detection enables real-time identification and monitoring of items and people, enhancing \\nthreat detection and response. In retail, it automates checkout procedures, tracks products on \\nshelves, and manages inventory for improved productivity and consumer satisfaction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 4}, page_content=\"Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n124 \\neditor@iaeme.com \\n3. Medical Imaging and Diagnosis: \\nIn medical imaging, object detection aids in locating and identifying specific structures, \\ncontributing to diagnosis and therapy. It ensures precision in identifying objects within images, \\nfacilitating better healthcare outcomes. \\n4. Interactive Experiences and Entertainment: \\nObject detection follows user movements and gestures in interactive applications and games, \\ncreating immersive experiences. It automatically tags and categorizes films and images, \\nsimplifying the search and organization of extensive media collections. \\n5. Automation and Speeding Up Processes: \\nObject detection automates tasks across various industries, reducing the need for manual \\ninvolvement and accelerating procedures. Augmented reality (AR) applications recognize real-\\nworld objects through object detection, enhancing user experiences by overlaying digital data \\nor graphics. Combining object detection with natural language processing (NLP) bridges \\ninformation gaps between text and images, facilitating content comprehension. \\n6. Environmental Monitoring and Wildlife Research: \\nResearchers and conservationists benefit from object detection by obtaining crucial data for \\ntracking environmental changes, monitoring wildlife, and studying animal behavior. It plays a \\npivotal role in understanding and preserving ecosystems. \\nObject detection is a versatile and indispensable technology with applications ranging from \\nindustrial automation to healthcare, security, entertainment, and environmental conservation. \\nObject Detection and the Impact of Deep Learning \\nThe realms of object detection and deep learning are intricately linked, with deep learning \\ntechniques profoundly shaping the landscape of object detection. The advent of deep learning \\nhas brought about a paradigm shift, enhancing the accuracy and efficiency of object detection \\nsystems to handle intricate tasks with remarkable precision. Let's delve into the intricate \\nconnection between object detection and deep learning: \\nCNNs, in particular, are deep learning models that include specialized convolutional layers \\nthat are skilled at extracting spatial information from images. These layers are excellent at \\nrecognizing objects because they can recognize linkages and local patterns among pixels. \\nModels pre-trained on large datasets (like ImageNet) can be refined for particular item \\nrecognition tasks using relatively small, labeled datasets thanks to deep learning's support for \\ntransfer learning. As a result, less training data is needed for functions involving bespoke object \\ndetection. \\nConvolutional neural networks (CNNs) are deep learning models that show proficiency in \\nautomatically extracting hierarchical features from images. Acquired traits are essential for \\ndifferentiating objects with different sizes, shapes, and orientations. Deep neural networks can \\ncapture high-level features that represent the components and structures of an item and low-\\nlevel features like edges and textures. Learning The benefit of end-to-end training for object \\nlocalization and classification is provided by deep learning models. This suggests that an \\nindividual model can recognize things and anticipate where they will be found in an image \\ninstead of conventional computer vision techniques that call for distinct, labor-intensive phases \\nfor object recognition and feature extraction. \\nA wide range of object sizes, types, and degrees of complexity may be handled by deep \\nlearning models with versatility. They may adapt to real-world events and are not limited to \\nsimple items or shapes.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 5}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n125 \\neditor@iaeme.com \\nDeep learning models are particularly good at handling partially veiled or occluded objects. \\nTheir capacity to pick up on intricate object appearances makes it easier to identify objects even \\nwhen they are partially obscured. Ongoing research and advancements in deep learning \\ncontribute to the continual enhancement of the accuracy and speed of object detection systems. \\nThis ongoing progress renders them suitable for an expanding range of applications. \\nLarge, labeled datasets are essential for the effectiveness of deep learning-based item \\ndetection. Deep learning models may learn from various examples, increasing their accuracy \\nand robustness. This is made possible by the availability of large image datasets that have been \\ntagged with object locations and class labels. Researchers have introduced deep learning \\narchitectures specifically designed for object detection. Examples include SSD (Single Shot \\nMultiBox Detector), YOLO (You Only Look Once), and Faster R-CNN. These designs \\neffectively combine localization and classification to optimize tasks related to object \\nidentification. \\nDeep learning has made real-time object detection possible, allowing for high-speed object \\ntracking and recognition in applications like augmented reality, video surveillance, and \\nautonomous cars. \\nChatGPT \\nThe realms of object detection and deep learning are intricately connected, with deep learning \\ntechniques significantly influencing and enhancing the landscape of object detection. The \\nintroduction of deep learning has led to a paradigm shift, considerably improving the precision \\nand efficiency of object detection systems for handling complex tasks. \\n1. Role of Convolutional Neural Networks (CNNs): \\nCNNs, a deep learning model, feature specialized convolutional layers adept at extracting \\nspatial information from images. These layers excel at recognizing objects by identifying \\nlinkages and local patterns among pixels. Pretrained models, such as those on ImageNet, can \\nbe fine-tuned for specific object recognition tasks using smaller labeled datasets, showcasing \\nthe transfer learning capabilities of deep learning and reducing the need for extensive training \\ndata. \\n2. Hierarchical Feature Extraction: \\nCNNs automatically extract hierarchical features from images, which is crucial for \\ndistinguishing objects with varying sizes, shapes, and orientations. These models capture high-\\nlevel features representing components and low-level features like edges and textures. End-to-\\nend training facilitates object localization and classification within a single model, eliminating \\nthe need for separate, labor-intensive phases in conventional computer vision techniques. \\n3. Versatility and Adaptability: \\nDeep learning models exhibit versatility in handling various object sizes, types, and \\ncomplexities. They can adapt to real-world scenarios and are not limited to simple objects or \\nshapes. Notably, deep learning models excel at handling partially veiled or occluded objects, \\nthanks to their ability to recognize intricate object appearances even when partially obscured. \\nOngoing research contributes to continuous improvements in the accuracy and speed of object \\ndetection systems, expanding their applicability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 6}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n126 \\neditor@iaeme.com \\n4. Importance of Large, Labeled Datasets: \\nLarge, labeled datasets are essential for the effectiveness of deep learning-based object \\ndetection. These datasets enable models to learn from diverse examples, enhancing their \\naccuracy and robustness. Researchers have introduced specialized deep learning architectures \\nfor object detection, such as SSD, YOLO, and Faster R-CNN, which efficiently combine \\nlocalization and classification to optimize object identification tasks. \\n5. Real-Time Object Detection: \\nDeep learning has enabled real-time object detection, facilitating high-speed object tracking \\nand recognition in applications like augmented reality, video surveillance, and autonomous \\nvehicles. \\nThe marriage of object detection and deep learning, mainly through CNNs and other \\nspecialized architectures, has ushered in a transformative era, offering enhanced accuracy, \\nadaptability, and real-time capabilities across diverse applications. Ongoing advancements in \\ndeep learning continue to push the boundaries of what is achievable in object detection. \\n \\nHow Object Detection Operates \\nObject detection, a critical computer vision task, involves using algorithms, particularly deep \\nlearning models, to identify and locate objects within images or video frames. The operational \\nprocess of object detection can be broken down as follows: \\n1. Data Gathering and Preparation: \\nCollect a dataset comprising images or video frames displaying objects of interest. \\nLabel each image with bounding boxes indicating the object\\'s location and class labels \\nspecifying the object type (e.g., \"car,\" \"person,\" \"cat\").'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 7}, page_content=\"Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n127 \\neditor@iaeme.com \\n2. Selection of Deep Learning Model: \\nFor object detection, utilize deep learning models, especially convolutional neural networks \\n(CNNs). \\nVarious models like SSD, RetinaNet, YOLO, and Faster R-CNN address specific \\napplication, speed, and accuracy concerns. \\n3. Training the Model: \\nTrain the selected deep learning model using the prepared dataset. \\nAdjust internal parameters to minimize the difference between model predictions and \\nground truth (labeled bounding boxes and class labels). \\nOptimize a loss function measuring the discrepancy between predicted and actual object \\npositions and classifications through an iterative training process. \\n4. Inference: \\nAfter training, use the model to identify objects in new, unlabeled images or video frames. \\nDuring inference, analyze the image either as a whole or by segmenting it into smaller \\nregions based on the model architecture. \\n5. Prediction Outputs: \\nGenerate predictions for each region, including: \\nBounding boxes (coordinates) around local objects. \\nClass designations corresponding to the identified objects. \\nConfidence scores indicate the model's confidence in each prediction. \\n6. Post-Processing: \\nAddress repetitive boxes or predictions with low confidence scores using post-processing \\ntechniques. \\nNon-maximum suppression (NMS) is one such method to eliminate redundant boxes, \\nretaining the most accurate and confident predictions. \\n7. Final Results: \\nDraw bounding boxes around objects in the image to represent the final object detection results, \\nincluding position and class information. \\n8. Applications: \\nApply object detection findings in various domains, such as augmented reality, safety systems, \\nautonomous vehicle decision-making, and object tracking.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 8}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n128 \\neditor@iaeme.com \\nMost Popular Object Detection Algorithms \\n \\nConvolutional neural networks, such as Mask R-CNN, R-CNN (Region-Based \\nConvolutional Neural Networks), YOLO (You Only Look Once), MobileNet, and SqueezeDet, \\nare frequently used methods for object detection. \\nR-CNN: \\nThe Region-based Convolutional Neural Network (R-CNN) is a computer vision model \\ndesigned for object recognition and image detection. Here\\'s a breakdown of its functionality: \\n1. Region Proposal: \\nR-CNN begins by identifying potential objects in an image using a \" selective search method,\" \\nsuggesting areas likely to contain objects. \\n2. Convolutional Neural Network (CNN) Analysis: \\nR-CNN utilizes a CNN for each suggested region to analyze and understand the content, \\ncreating a descriptive map of the area. \\n3. Support Vector Machine (SVM): \\nR-CNN applies a support vector machine to identify the objects present in the regions. It \\ncategorizes and recognizes objects using labels. \\n4. Bounding Box Refinement: \\nThe model refines the bounding boxes surrounding the recognized objects, adjusting them to \\nmatch the shapes of the objects better. \\n5. Result Selection: \\nR-CNN selects the most accurate and confident outcomes, eliminating less trustworthy ones to \\nprevent displaying excessively similar findings. \\n6. Drawbacks and Enhancements: \\nR-CNN, while a breakthrough, was slow and complex for object identification. Later models \\nlike Fast R-CNN and Faster R-CNN addressed these issues, introducing improvements for \\nincreased accuracy and speed in object detection.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 9}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n129 \\neditor@iaeme.com \\nMask R-CNN: \\n• An extension of Faster R-CNN, Mask R-CNN, introduced in 2017, handles instance \\nsegmentation, providing pixel-level masks for each unique instance of an object in \\naddition to object detection. \\n• Mask R-CNN utilizes a CNN to extract features, refines bounding boxes for precise \\nobject fitting, and creates binary masks to identify pixel-level object instances. \\n• The model groups identified items into various types or categories and employed non-\\nmaximum suppression to remove duplicate or overlapping results after object detection, \\nmask creation, and classification. \\n• Applications for Mask R-CNN are found in domains requiring instance-level \\nsegmentation and pixel-level precision, such as robotics, computer vision tasks, and \\nmedical imaging. \\n \\nDeep learning-based object identification models are crucial in recognizing items in images \\nor video frames and delineating them with bounding boxes. After object detection, metadata \\nmodels come into play, focusing on obtaining additional data or characteristics associated with \\nthe detected objects. The extracted metadata is linked to each bounding box, providing precise \\nattribute descriptions for each recognized object. Here are some common metadata attributes: \\n• \\nObject Class: Identifying the type of object (e.g., \"car,\" \"person,\" \"dog\"). \\n• \\nColor: Specifying the object\\'s color or color patterns. \\n• \\nSize: Determining the object\\'s actual dimensions. \\n• \\nOrientation: Identifying the object\\'s orientation or position. \\n• \\nVelocity: Determining the motion or speed of the object. \\n• \\nTimestamp: Documenting the moment the object was detected. \\n• \\nContext: Recognizing how an object fits into its surroundings or connects to other \\nobjects.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 10}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n130 \\neditor@iaeme.com \\nCombining spatial information from the bounding box with associated metadata enables \\npinpointing specific characteristics of individual objects. For instance, metadata might describe \\na detected car as \"red,\" \"sedan,\" and \"parked.\" \\nApplications for combined object detection and metadata information span various fields: \\n• \\nTraffic Monitoring: Identifying vehicle types, colors, and speeds for traffic studies. \\n• \\nSecurity and Surveillance: Sorting individuals and items based on potential threats. \\n• \\nAutonomous Vehicles: Recognizing characteristics and motion of nearby automobiles \\nand people. \\n• \\nRetail Inventory: Monitoring goods on store shelves, including locations and \\nattributes. \\n• \\nAugmented Reality: Overlaying digital data based on identified items and their \\ncharacteristics. \\n• \\nMedical Imaging: Analyzing and recording characteristics of anatomical structures or \\nanomalies. \\nCombining object detection and metadata models provides a comprehensive and contextual \\nunderstanding of objects within an image or video frame. This information enhances the utility \\nand functionality of computer vision systems, enabling improved decision-making, analysis, \\nand automation across diverse fields. \\nYOLO (You Only Look Once): \\nYOLO (You Only Look Once) is a real-time object detection system widely popular in \\ncomputer vision and deep learning. Introduced by Joseph Redmon and Santosh Divvala in a \\nseries of papers, the original paper titled \"You Only Look Once: Unified, Real-Time Object \\nDetection\" was published in 2016. YOLO is designed for instantaneous object detection in \\nvideo and image streams, quickly locating and identifying multiple objects in a single neural \\nnetwork pass. \\nKey features and aspects of YOLO include: \\n1. Unified Approach: \\n• YOLO takes a unique approach by splitting the image into a grid and predicting \\nbounding boxes and class probabilities for every grid cell. Unlike region-based object \\nidentification techniques like R-CNN, YOLO analyzes the entire image in a single pass. \\n2. Single Pass Analysis: \\n• YOLO effectively analyzes the complete picture in one pass, providing predictions for \\nevery object simultaneously. This architecture significantly improves speed compared \\nto traditional techniques that require multiple network passes. \\n3. Bounding Box Prediction: \\n• YOLO predicts bounding boxes that define the locations of objects for each grid cell. \\nConfidence scores indicate the accuracy of predictions. Class probability predictions are \\nalso used to identify the object type in each bounding box.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 11}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n131 \\neditor@iaeme.com \\n4. Non-Maximum Suppression: \\n• YOLO uses non-maximum suppression to eliminate redundant bounding boxes, \\nretaining the most reliable and accurate ones. This post-prediction step is performed for \\nevery grid cell. \\n5. Iterative Improvements: \\n• YOLO has undergone several revisions, including YOLOv1, YOLOv2 (YOLO9000), \\nYOLOv3, and YOLOv4. Each iteration has surpassed the previous ones in accuracy and \\nspeed, solidifying YOLO\\'s reputation as a flexible option for various object detection \\napplications. \\n6. Applications: \\n• YOLO\\'s strength lies in its high-precision real-time object identification capability, \\nmaking it ideal for applications such as robots, autonomous cars, surveillance, and \\nmore. \\n7. Ongoing Relevance: \\n• YOLO is still frequently used in computer vision and has inspired the development of \\nother real-time object detection methods. \\n• YOLO\\'s innovative approach to real-time object detection, combined with its iterative \\nimprovements, has made it a widely adopted and versatile solution for various \\napplications in the field of computer vision. \\nMobileNet \\nMobileNet is a family of simplified convolutional neural network architectures designed for \\nefficient and low-latency deep learning inference on mobile and embedded devices. Developed \\nby Google\\'s Research Brain Team, MobileNet prioritizes high performance within a small \\nmemory and computational framework. Notable features include using depthwise separable \\nconvolutions and dividing the convolution process into depthwise and pointwise stages to \\nreduce parameters and calculations without compromising precision. MobileNet is widely \\nutilized in settings with limited computational resources, such as edge computing, IoT devices, \\nand mobile phones. It has undergone iterative improvements with versions like MobileNetV1, \\nV2, and V3, each introducing enhancements to the architecture, accuracy, and speed. \\nIn the realm of real-time object detection for autonomous driving, SqueezeDet is a \\nspecialized model designed for identifying objects on roads. Introduced in a 2017 paper titled \\n\"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time \\nObject Detection for Autonomous Driving,\" SqueezeDet is built on the lightweight SqueezeNet \\narchitecture, well-suited for embedded and real-time applications. It processes whole images in \\na single pass using a fully convolutional neural network (FCN) architecture, generating \\nbounding boxes and class predictions for recognized objects. SqueezeDet incorporates \"squeeze \\nand excitation\" blocks to dynamically vary channel-wise scaling factors, improving object \\nfeature representation. The model predicts bounding boxes and assigns class labels to each box, \\nachieving a balance between processing efficiency and accuracy for real-time object detection \\nin autonomous vehicles. \\nMobileNet and SqueezeDet showcase the importance of efficient and lightweight neural \\nnetwork architectures in enabling deep learning applications on devices with limited \\ncomputational capabilities.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 12}, page_content=\"Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML \\n132 \\neditor@iaeme.com \\nMobileNet's versatility makes it a popular choice for various applications, while \\nSqueezeDet excels in the specific context of real-time object detection for autonomous driving. \\nCONCLUSION \\nWhen integrated with analytics and metadata, object detection proves invaluable across diverse \\nindustries, especially in manufacturing and quality control. This powerful combination excels \\nin identifying flaws on assembly lines, optimizing processes, and enabling preventive \\nmaintenance. For inventory tracking, metadata enhances supply chain efficiency, while in \\nproduct monitoring, it predicts quality and facilitates customization. Integration is equally \\nbeneficial in tracking machinery functionality, enabling predictive maintenance, and ensuring \\ncompliance in sectors with stringent regulations. This amalgamation revolutionizes \\nmanufacturing and quality control, fostering data-driven decision-making, improving product \\nquality, reducing waste, and enhancing overall productivity. \\nREFERENCES \\n[1] \\nDeci, “Deci Introduces YOLO-NAS - A Next-Generation, Object Detection Foundation Model \\nGenerated by Deci’s Neural Architecture Search Technology,” Deci, May 03, 2023. \\nhttps://deci.ai/blog/yolo-nas-foundation-model-object-detection \\n[2] \\nS. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, and B. Lee, “A survey of modern \\ndeep learning based object detection models,” Digital Signal Processing, vol. 126, p. 103514, \\nJun. 2022, doi: https://doi.org/10.1016/j.dsp.2022.103514. \\n[3] \\n“SqueezeDet: \\nDeep \\nLearning \\nfor \\nObject \\nDetection,” Mez \\nGebre. \\nhttps://mez.sh/2017/04/21/squeezedet-deep-learning-for-object-detection/ (accessed Dec. 22, \\n2023). \\n[4] \\nR. Alake, “How Does AI Detect Objects? (Technical),” Medium, Jan. 14, 2020. \\nhttps://towardsdatascience.com/how-does-ai-detect-objects-technical-d8d63fc12881 \\n[5] \\nD. S. Shenwai, “Top Object Detection Algorithms and Libraries in Artificial Intelligence \\n(AI),” MarkTechPost, Jul. 18, 2023. https://www.marktechpost.com/2023/07/18/top-object-\\ndetection-algorithms-and-libraries-in-artificial-intelligence-ai/ (accessed Dec. 22, 2023). \\n[6] \\nRajath Karangara, “Unique Methods for Highly Populous Countries to Leverage Post-Pandemic \\nEconomy to Ramp Up Digital Payments,” SSRG international journal of computer science and \\nengineering, vol. 10, no. 7, pp. 21–26, Jul. 2023, doi: https://doi.org/10.14445/23488387/ijcse-\\nv10i7p103.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'format': 'PDF 1.7', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'trapped': '', 'modDate': \"D:20240109103651+05'30'\", 'creationDate': \"D:20240109103651+05'30'\", 'page': 13}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML \\n133 \\neditor@iaeme.com \\nAuthor Details \\nNarayana Challa, Director of ERP Strategy, IEEE Senior Member, Texas, USA  \\nNarayana Challa serves as the Director of ERP Strategy in the manufacturing industry.  \\nExpert in Digital Transformation leveraging Enterprise Resource Planning to unlock \\noperational efficiencies in supply chain elements such as manufacturing and inventory \\nmanagement. He has led multiple teams across various technologies throughout his career, \\ndemonstrating enthusiasm for researching new technologies and processes within the \\ninformation technology department. With a diverse skill set, he has executed numerous projects \\nin roles such as architect, data engineering, data ingestion, ETL developer, administrator, and \\nenterprise architect. His expertise extends to cloud platforms, notably Amazon Web Services \\nand Azure. \\n \\n \\n \\n \\nCitation: Narayana Challa, Artificial Intelligence for Object Detection and Its Metadata, International Journal of \\nArtificial Intelligence & Machine Learning (IJAIML), 2(1), 2023, pp. 121-133 \\n \\nDOI: https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\nArticle Link:  \\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJAIML/VOLUME_2_ISSUE_1/IJAIML_02_01_012.pdf \\n \\nAbstract:  \\nhttps://iaeme.com/Home/article_id/IJAIML_02_01_012 \\n \\nCopyright: © 2023 Authors. This is an open-access article distributed under the terms of the Creative Commons \\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the \\noriginal author and source are credited. \\n \\nThis work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). \\n \\n \\n✉ editor@iaeme.com \\nView publication stats')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "## load all the text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader (\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ##Pattern to match the files\n",
    "    loader_cls = PyMuPDFLoader, ##loader class to use\n",
    "        \n",
    ")\n",
    "pdf_documents= dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "231214c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: Attention.pdf\n",
      "  ✓ Loaded 6 pages\n",
      "\n",
      "Processing: Embeddings.pdf\n",
      "  ✓ Loaded 5 pages\n",
      "\n",
      "Processing: Embeddings2.pdf\n",
      "  ✓ Loaded 27 pages\n",
      "\n",
      "Processing: ObjectDetection.pdf\n",
      "  ✓ Loaded 14 pages\n",
      "\n",
      "Total documents loaded: 52\n",
      "Split 52 documents into 327 chunks\n",
      "\n",
      "Example chunk\n",
      "Content : Attention capabilities for AI systems \n",
      " \n",
      "Helgi Páll Helgason1, Kristinn R. Thórisson1,2 1Center for Analysis & Design of Intelligent Agents / School of Computer Science, Venus 2nd floor, Reykjavik \n",
      "Un...\n",
      "Metadata : {'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}\n",
      "Total chunks created: 327\n"
     ]
    }
   ],
   "source": [
    "#CHUNKING\n",
    "from pathlib import Path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Split documents into smaller chunks\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects or raw strings.\n",
    "        chunk_size: Max characters per chunk.\n",
    "        chunk_overlap: Overlap between chunks.\n",
    "    \n",
    "    Returns:\n",
    "        List of Document chunks\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are Document objects\n",
    "    if isinstance(documents[0], str):\n",
    "        documents = [Document(page_content=doc, metadata={}) for doc in documents]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk\")\n",
    "        print(f\"Content : {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")\n",
    "\n",
    "# Split them into chunks\n",
    "chunks = split_documents(all_pdf_documents)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da98789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding And VectorDB\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any, Tuple \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5835453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1da00795930>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "      \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "      try:\n",
    "        print(f\"Loading embedding model: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "      except Exception as e:\n",
    "        print(f\"Error loading model {self.model_name}: {e}\")\n",
    "        raise\n",
    "      \n",
    "\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "           raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "##Initialize the Embedding Manager\n",
    "\n",
    "embedding_manager= EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96e0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1da190e4bb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "\n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                 name=self.collection_name,\n",
    "                 metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "           raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "             \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a236dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Attention capabilities for AI systems \\n \\nHelgi Páll Helgason1, Kristinn R. Thórisson1,2 1Center for Analysis & Design of Intelligent Agents / School of Computer Science, Venus 2nd floor, Reykjavik \\nUniversity,Menntavegur 1, 101 Reykjavik, Iceland \\n2Icelandic Institute for Intelligent Machines, 2. h. Uranus, Menntavegur 1, 101 Reykjavik, Iceland \\nhelgih09@ru.is, thorisson@ru.is  \\nKeywords: Artificial intelligence, attention, resou rce management'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Abstract: Much of present AI research is based on t he assumption of computational systems with infinit e resources, \\nan assumption that is either explicitly stated or i mplicit in the work as researchers ignore the fact that most \\nreal-world tasks must be finished within certain ti me limits, and it is the role of intelligence to ef fectively \\ndeal with such limitations. Expecting AI systems to  g i v e  e q u a l  t r e a t m e n t  t o  e v e r y  p i e c e  o f  d a t a  t h e y'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='encounter is not appropriate in most real-world cas es; available resources are likely to be insufficie nt for \\nkeeping up with available data in even moderately c omplex environments. Even if sufficient resources a re \\navailable, they might possibly be put to better use  t ha n bl i n dl y  a ppl y i ng  t he m  t o ev e ry  pos s i bl e  pi e c e of  \\ndata. Finding inspiration for more intelligent reso urce management schemes is not hard, we need to loo k no'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='further than ourselves. This paper explores what hu man attention has to offer in terms of ideas and co ncepts \\nfor implementing intelligent resource management an d how the resulting principles can be extended to \\nlevels beyond human attention. We also discuss some  ideas for the principles behind attention mechanis ms \\nfor artificial (general) intelligences. \\n1 INTRODUCTION \\nThe field of AI has a long history of targeting \\nisolated, well-defined problems to demonstrate'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='isolated, well-defined problems to demonstrate \\nintelligent capabilities. While useful, many of the se \\nproblems (and especially their task environments, a s \\nperceived by the system) are extremely simple \\ncompared to the problem of learning how to solve \\nnovel tasks and adapting to changes in real-world \\nenvironments - a problem which must be addressed \\nand solved in order for AI systems to approach \\nhuman-level intelligence. Given the nature of this'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='prior work, it is not surprising that limited focus  has \\nbeen given to real-time processing and resource \\nmanagement. However, the design of any AI system \\nexpected to learn and perform a range of tasks in \\neveryday environments needs to face these realities: \\n \\n\\uf001 The real world is highly dynamic and complex \\nand can provide an abundance of information \\nat any given moment. \\n\\uf001 Resources of any intelligent system are not \\nonly limited, but insufficient in light of the'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='only limited, but insufficient in light of the \\nmassive amount of information available from \\nthe environment. \\n \\n\\uf001 A range of time constraints, many of which \\nare dictated by the environment, must be \\nsatisfied in order to ensure safe and successful \\noperation of the system. \\n \\nMuch of existing work in the field of AI is also \\nbased on greatly simplified operating assumptions -  \\na case in point being the practically impossible (b ut \\nsurprisingly common) assumption of infinite'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='surprisingly common) assumption of infinite \\nresources, often in terms of storage but particular ly \\nin terms of processing: A system based on this \\nassumption will fail to perform and potentially cra sh \\nin real world operation when fed with information a t \\na greater rate than it is capable of processing. To  \\nfind inspiration for implementing intelligent \\nresource management we need not look far, nature \\nhas provided us with a prime example in human'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='has provided us with a prime example in human \\nattention; a cognitive function that enables us to \\nfocus our limited resources selectively on \\ninformation that is most important to us at any giv en \\nmoment as we perform various tasks while'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='remaining reactive to unexpected but important \\nevents in the environment. Consider that while \\nreading this chapter you have effectively ignored \\nmore than 99.9% of the numerous things that your \\nmind could have spent time and resources on doing. \\nPerhaps not surprisingly, it turns out that this is  \\nexactly the kind of resource management that is \\nrequired to enable AI systems to approach human-\\nlevel intelligence in real-world environments. Thus ,'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='it makes perfect sense to investigate how AI system s \\ncan be endowed with this cognitive function for the  \\npurpose of improving their operation and making \\nthem applicable to more open-ended and complex \\ntasks and environments. The goal need not be to \\nreplicate any biological function in detail, but ra ther \\nto extract useful concepts and methods from the \\nbiological side while leaving undesirable limitatio ns \\nbehind in order to facilitate the creation of AI'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='behind in order to facilitate the creation of AI \\nsystems that can successfully operate in real-world  \\nenvironments in realtime using limited resources.  \\n W h i l e  a t t e n t i o n  h a s  b e e n  l a r g e l y  i g n o r e d  i n  \\nthe field to-date, there are notable exceptions. Th ese \\ninclude cognitive architectures such as NARS \\n(Wang, 1995), LIDA (Baars, 2009) and Clarion \\n(Sun, 2006). However, the attentional functionality  \\nimplemented in these systems is incomplete in'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='implemented in these systems is incomplete in \\nvarious ways, such as focusing solely on data-\\nfiltering (ignoring control issues, e.g. how \\nprioritization affects processing of selected data)  and \\nexternal environmental information (ignoring \\ninternal system states). The ASMO framework \\n(Novianto, 2009) is somewhat unique as it assumes a \\ntight coupling between attention and self-awareness  \\nand includes focus on internal states. However, non e'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='of this work addresses realtime processing, which i s \\none of the major reasons we desire attentional \\nfunctionality, in a vigorous fashion. Attention has  \\nalso been studied in relation to AI within the limi ted \\nscope of working memory (c.f. Phillips 2005 and \\nSkubic 2004). While attention and working memory \\nare closely related, this is a restrictive context to \\nstudy attention within as working memory can in \\nmost cases be modelled as a cognitive function'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='most cases be modelled as a cognitive function \\nrather than an architectural component. \\nThis paper starts with a brief overview of \\nhuman attention and subsequently attempts to \\nextract principles that may be useful for AI system s. \\nThis is followed by a discussion of how these \\nprinciples might be extended to levels beyond \\nhuman attention for meta-reasoning and \\nintrospection. We then present a high-level design of \\nan attention mechanism intended for AI \\narchitectures.  \\n2 HUMAN ATTENTION'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='architectures.  \\n2 HUMAN ATTENTION \\nResearch of human attention has a long history \\ndating back to the beginnings of psychology. Back \\nin 1890, the American psychologist William James \\nwrote the following (James 1890): \\n \\n“Everyone knows what attention is. It is the taking \\npossession by the mind, in clear and vivid form, of  \\none out of what seem several simultaneously \\npossible objects or trains of thought. Focalization , \\nconcentration, of consciousness are of its essence.  It'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='implies withdrawal from some things in order to \\ndeal effectively with others, and is a condition wh ich \\nhas a real opposite in the confused, dazed, \\nscatterbrained state which in French is called \\ndistraction, and Zerstreutheit in German.” \\n \\n- William James \\n \\nThis elegant description indicates that the \\nimportance of attention for the human mind was \\nidentified as early as the 18 th century. The beginning \\nof modern attention research is commonly tied to'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='of modern attention research is commonly tied to \\nColin Cherry’s work on what has been called the \\n“cocktail party effect” (Cherry 1953), which \\naddresses how we are able to focus on particular \\nsensory data in the presence of distracting \\ninformation and noise, such as following and \\nparticipating in a conversation at a cocktail party  in \\nthe presence of many other conversations and \\nbackground noise, and still be able to catch when \\nsomeone calls our name in the background. The'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='someone calls our name in the background. The \\nability to be in a focused state of attention while  \\nremaining reactive to unexpected events, seems to \\ncall for a selective filtering mechanism of some so rt \\nwhile at the same time requiring deliberate steerin g \\nof cognitive resources. The cocktail party scenario  is \\na good illustration of the dual nature of attention : \\nWe will refer to the deliberate, goal-driven side a s \\ntop-down attention and the reactive, stimulus-driven'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='side as bottom-up attention. \\nA number of models for attention were \\nsubsequently proposed, some of which were \\nconsidered early selection models  a s  s e l e c t i o n  o f  \\nsensory information is assumed to occur early in th e \\nsensory pipeline based on primitive physical featur es \\nof the information. This implies that the \\ndetermination of what is important and should be \\nselected is based on shallow, primitive processing \\nwith very limited or non-existent analysis of'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='with very limited or non-existent analysis of \\nmeaning. The Broadbent filter model (Broadbent \\n1958) is the most prominent of these. A number of \\nlate selection models  h a v e  a l s o be e n  pr o pos e d,  t h a t'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='assume further analysis of incoming sensory \\ninformation must be performed in order to determine  \\nits relevance and carry out efficient selection. Th e \\nDeutsch-Norman (Norman 1969) model is based on \\nthe assumption that sensory information is not \\nactually filtered, but processed to the point of \\nactivating representations stored in memory. \\nSelection then occurs at the level of representatio ns, \\nwhere the most active ones are selected for further'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='processing. The model also assumes an attentional \\nbottleneck at this point, where only one \\nrepresentation can be selected for processing at a \\ntime. These two classes of attention models are \\nreferred to as the early vs. late selection  models, and \\nhave resulted in some debate. Shortcomings of many \\nearly selection models are obvious, as they fail to  \\naccount for parts of the cocktail party effect, \\nespecially phenomena such as noticing your own'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='especially phenomena such as noticing your own \\nname being called from across the room while \\nengaged in conversation. This contradicts the model , \\nas the physical characteristics of the data (our na me \\nbeing called) would not be sufficient to attract ou r \\nattention and pass through the filter; some analysi s \\nof meaning must be involved. \\nSome more recent theories and models of \\nattention focus on the interaction between top-down  \\nand bottom-up attention. In (Knudsen 2007), an'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='and bottom-up attention. In (Knudsen 2007), an \\nattention framework is presented based on four \\nfundamental processes: working memory, top-down \\nsensitivity control, competitive selection and \\nbottom-up filtering for salient stimuli. The first three \\nprocesses work in a recurrent loop to implement top -\\ndown control attention. Working memory is \\nintimately linked to attention as its contents are \\ndetermined by attention. This framework seems to \\ncapture most of the essential components of'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='capture most of the essential components of \\nattention and is a promising candidate for inspirat ion \\nwith regards to attention for AI. \\n \\n3 ATTENTION AND AI \\nLet us now consider how the previous chapter can \\ninspire implementation of attentional capabilities for \\nAI systems. As suggested in the introduction, we \\nspecifically target general AI systems designed to \\noperate in complex environments under real-time \\nconstraints with limited resources. These systems are'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='expected to perform various tasks while being \\nreactive to events in the environment, a requiremen t \\nthat  maps  neatly to the  top-down  and  bottom-up  a \\nworkings  of  attention  mentioned  earlier. Both of \\n \\nFigure 1: The Knudsen attention framework (from \\nKnudsen 2007). Data flows up from the environment, \\npasses through salience filters (which detect infre quent or \\nimportant stimuli) and activates neural representat ions, \\nwhich encode various types of knowledge. The activa tion'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='of neural representations is also influenced by wor king \\nmemory via the sensitivity control of top-down atte ntion \\nthat adjusts activation thresholds of individual \\nrepresentations. Representations compete for access to \\nworking memory with only the most active ones being  \\nadmitted. Gaze is controlled by working memory and the \\nselection process. \\nthese are necessary for a complete system; those th at \\nimplement only top-down down attention will'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='implement only top-down down attention will \\ncontinue to work on tasks without being able to rea ct \\nto unexpected or novel events in the environment – \\nevents that may be relevant to the current task or \\nnecessary triggers for generation of new ones. \\nConversely, systems implementing only bottom-up \\nattention cannot perform tasks beyond those that ar e \\nsimple and reactive; tasks consisting of multiple \\nsteps are not possible. However, when these two'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='steps are not possible. However, when these two \\ntypes of attention are properly combined, the resul t \\nis a flexible system capable of performing complex \\ntasks while being faced with interruptions and \\nunexpected events. Part of the role of attention \\ntherefore, is to manage the balance between these \\ntwo at every point in time. \\nThe early vs. late selection debate mentioned in \\nthe previous chapter is also relevant here. It is \\npossible to implement attention mechanisms for AI'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='systems that perform selection early in the sensory'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='pipeline based on primitive features of the data. T his \\napproach is adopted in some of the best known \\nexisting cognitive architectures, such as SOAR \\n(Laird 2008), where attention is viewed as a \\nperceptual process rather than a cognitive one. Ear ly \\nselection unavoidably means that some data is \\n(partly or fully) ignored without being processed f or \\nmeaning; ignoring data that is not understood by th e \\nsystem introduces considerable risk as its relevanc e'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='for the system is not known. This may be acceptable  \\nfor narrow AI systems designed for specific tasks i n \\nspecific environments as it may be possible to crea te \\nshortcuts to understand the nature of incoming \\ninformation in such cases. However, for general AI \\n(AGI) systems designed for tasks and environments \\nnot specified at implementation time, this is highl y \\nproblematic. Early stages of the sensory pipeline c an \\ncontribute to attention in useful ways, such as'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='contribute to attention in useful ways, such as \\nperforming biasing as opposed to absolute selection . \\nFor example, such biasing might be based on \\nnovelty or unexpectedness of the data as these \\nproperties may give rough clues to the importance o f \\nthe information without requiring the information t o \\nbe processed for meaning. Furthermore, this is a \\nreasonable way to implement bottom-up attention, as \\nsuggested by the Knudsen model in Figure 1. As'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='suggested by the Knudsen model in Figure 1. As \\nshallow processing at early stages of the sensory \\npipeline seems unlikely to provide a reliable \\nmeasure of the importance of information, the late \\nselection paradigm seems more promising than early \\nselection in terms of AI and attention. \\nTop-down attention may be viewed as a goal-\\ndriven process as it is intimately related to curre nt \\ngoals of the system. For goals to direct top-down'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='attention, their level of specification is critical . In a \\nsystem where goals are fully specified in terms of \\noperation, the goal definition will be extremely \\nuseful in adjusting attention to elements that are \\nrelevant to the goal. A top-down attention \\nmechanism based on pattern matching could \\ngenerate partially specified patterns from goal \\nspecifications and attempt to find matches in senso ry \\ninformation. Predictions and expectations may also'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='be expected to be necessary control input for top-\\ndown attention in systems that explicitly implement  \\npredictive capabilities – and there is good reason to \\nbelieve that this is necessary in order to approach  \\nhuman-level intelligence. In terms of top-down \\nattention, predictions may be treated in virtually the \\nsame fashion as goals (with level of specification \\nbeing equally important as for goals). \\n \\n4 AI ATTENTION: BEYOND THE \\nHUMAN LEVEL \\nAI systems have an interesting advantage over'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='AI systems have an interesting advantage over \\nhuman minds; they are based on software rather than  \\nhardware (“wetware”). While neurons of our brains \\ncan adaptively wire up to encode skills, knowledge \\nand experiences the core mechanisms of these \\nprocesses are fixed. For example, humans cannot \\neasily acquire dramatically better ways of learning  \\nor remembering. This limitation does not apply to \\nsoftware AI systems; their potential for flexibilit y'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='and reconfiguration are only limited by their \\narchitectural design. The same can be said for thei r \\nlevel of introspection; our introspective capabilit ies \\nare greatly limited - we only have a very vague \\nsense of what is going on in our minds. On the othe r \\nhand, there are much weaker limitations on self-\\nobservation in software AI systems, which again are  \\nlimited only by architectural design. \\nA case for flexible architectures capable of \\nautonomous self-reconfiguration is made in'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='autonomous self-reconfiguration is made in \\n(Thórisson 2009). There are limitations on the \\ncomplexity of manually built software systems and it \\nis not unreasonable to assume that more complex \\nsoftware systems than exist today are needed in \\norder to approach human-like AI. If our chances of \\nmanually building such systems are low, having the \\nsystems build themselves (in a sense) from \\nexperience is not an unreasonable line of research. \\nIn order to perform deep levels of introspection'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='In order to perform deep levels of introspection \\nin complex AI systems, attention is equally useful as \\nfor information originating outside the system; the  \\nsum of activity within such a system can be \\nconsidered to be a vast stream of information and \\nsystem resources remain limited. Determining which \\nparts of this stream are worth processing in order to \\nachieve meta-cognitive goals may be considered as \\nthe role of attention, in much the same way as'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='the role of attention, in much the same way as \\nattention operates on environmental information. \\nThe main purpose of introspection is to provide \\ninformation to direct self-reconfiguration of the \\nsystem. For example, an observation that system \\nprocess P fails repeatedly in certain contexts can be \\nused by the system to shut down process P a n d  \\nactivate a different process (which may exist or ne ed \\nto be created/learned, generating a new meta-'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='to be created/learned, generating a new meta-\\ncognitive goal) when such contexts occur in the \\nfuture.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='5 AN ATTENTION MECHANISM \\nFOR AI SYSTEMS \\nThis section presents a design of one possible \\nattention mechanism for AI systems which addresses \\nthe concepts related to attention discussed \\npreviously. The implementation and evaluation of \\nthis mechanism is upcoming future work. The \\napproach taken adopts the theoretical and \\nmethodological framework presented in Thórisson \\n(2009). \\nAs attention is a ubiquitous cognitive process \\nthat cannot be easily separated from the rest of th e'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='cognitive architecture, some architectural \\nrequirements are unavoidable when tackling the \\ndesign of an AI attention mechanism. The attention \\nmechanism proposed here rests on the requirements \\nthat the underlying cognitive architecture has the \\nfollowing properties: \\n \\n\\uf001 Data-driven. All processing occurs in \\nreaction to data. Processes are activated only \\nwhen paired with compatible input data \\n(fitting the input data specification of the \\nprocess). Absence of fixed control loops allow'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='process). Absence of fixed control loops allow \\nfor greater flexibility and operation on \\nmultiple time scales. \\n\\uf001 Fine-grained. Processing and data elements \\nof the architecture are numerous and small. \\nComplex tasks require collaboration of many \\nsuch elements. Reasoning about small, simple \\ncomponents and their effects on the system is \\nmore practical than attempting to do so for \\nlarger components. \\n\\uf001 Predictive capabilities.  G e n e r a t e  p r e d i c t i o n s'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='with regards to expected events. Expectations \\nare part of the control data of the attention \\nmechanism. \\n\\uf001 Unified sensory pipeline. Data from the \\nenvironment and from within the system are \\ntreated equally. Enables systems to sense their \\nown operation and potentially allows \\ncognitive functions to be applied equally to \\ntask performance in the environment as well \\nas meta-cognitive processing (e.g. self-\\nreconfiguration). \\n \\nThe proposed attention mechanism implements'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='The proposed attention mechanism implements \\nboth top-down and bottom-up attention. Top-down \\nattention is based on goals and predictions, which \\nserve as the basis for generation of so called \\nattentional templates (AT), which are patterns that \\ntarget data to various levels of specification. An AT \\ncan target general data (such as all data from a si ngle \\nmodality, e.g. auditory) or more specific data such as \\nanything directly related to an object or location in'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='the environment and everything in between. As the \\narchitecture implements a unified sensory pipeline , \\nsensory data and internal data are targeted in an \\nidentical fashion by attention. When a data object \\nmatches an active AT, it becomes a candidate to \\nserve as input to a process for which it is compati ble \\nas input. Data objects that do not match any active  \\nAT are not caught by top-down attention and cannot \\ntrigger processing (unless caught by bottom-up'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='trigger processing (unless caught by bottom-up \\nattention). Each AT is created with an associated \\npriority value, which is used when a match occurs \\nwith data, where the matching data item is assigned  \\nthe same value. This value initially comes from the  \\ngoal or prediction used to generate the AT. The \\nassignment of priority values to data upon a match \\nwith an AT is called biasing. Available resources o f \\nthe system are allocated to data items in order of'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='their priority; data items with high priority value s \\n(greatest bias) will have better chances of receivi ng \\nprocessing than those with lower values. \\nBottom-up attention is implemented by primitive \\ndata selection principles that attempt to quantify the \\nnovelty and unexpectedness of input data based on \\ncontent, temporal factors and operational experience. \\nThe novelty of data is based on how similar it is t o \\ndata the system has previously seen, with higher'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='data the system has previously seen, with higher \\nnovelty values being assigned to data that is \\ndifferent from previously seen data. Time also play s \\na role as data that has not been seen recently (but  is \\nnot completely new to the system) will receive \\nhigher novelty values than those that have occurred  \\nrecently. For example, if the environment has been \\nsilent for a while and sound is suddenly heard, \\nauditory data is considered novel and would be'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='auditory data is considered novel and would be \\ncaught by bottom-up attention. If the sound persist s \\nfor some period of time, auditory data will cease t o \\nbe novel and require top-down attention in order to  \\nbe processed. In this way, the bottom-up part of th e \\nattention mechanism implements habituation. \\nFinally, a special mapping process is responsible \\nfor ensuring processes capable of consuming data \\ncaught by attention will be in active states. As th e'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='system is expected to contain numerous processes \\nand data objects at any given time, attempting to \\nmatch every data object to every process to \\ndetermine if an operational match exists is not \\npractically feasible. The data-to-process mapping \\ncomponent can be viewed as an optimization that \\nreduces the number of data/process matched \\nrequired.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Figure 2: Overview of the proposed attention mechanism. \\n6 CONCLUSIONS  \\nAs has been shown, mapping models and \\nconcepts from attention in cognitive psychology to \\nAI systems can be useful and straightforward. \\nSurprisingly limited work has been performed on \\nattention in the field of AI given that it is a field with \\nthe ultimate goal of creating human-like intelligen ce \\nand that attention is clearly a critical cognitive \\nprocess for humans. The fact that the human mind'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='process for humans. The fact that the human mind \\nimplements this kind of sophisticated resource \\nmanagement while being orders of magnitude more \\ncomputationally powerful than existing computer \\nhardware today also hints at the importance of \\nattention for AI. \\nFurthermore, attention is likely to be equally \\ncritical for introspective systems such as those th at \\ncan manage their own growth and adapt to \\nexperience at the architecture level. The internals  of'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='the system can be viewed dynamic and complex \\nenvironment in the same way as the task \\nenvironment. With a general and flexible attention \\nmechanism, it may be possible to apply the same \\nattention mechanism for both environments \\nsimultaneously; giving rise to AI systems that \\nperform tasks and i m p r o v e  t h e i r  o w n  p e r f o r m a n c e  \\nwhile being subject to real-time constraints and \\nresource limitations.   \\nACKNOWLEDGEMENTS \\nThis work was supported by the European Project'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='This work was supported by the European Project \\nHUMANOBS – Humanoids that Learn Socio-\\nCommunicative Skills Through Observation (grant \\nnumber 231453). \\nREFERENCES \\nBaars, B. J., Franklin, S. 2009. Consciousness is \\ncomputational: The LIDA model of Global Workspace \\nTheory. International Journal of Machine \\nConsciousness, 2009, 1(1): p. 23-32. \\n \\nBroadbent, D. E. 1958. Perception and Communication.  \\nLondon: Pergamon. \\n \\nCherry, E. C. 1953. Some experiments on the recogniti on'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='of speech, with one and two ears. Journal of the \\nAcoustical Society of America. Pages 975-979. \\n \\nKnudsen, E. I. 2007. Fundamental components of \\nattention. Annu Rev Neurosci, volume 30. Pages 57-78. \\n \\nJames, W. 1890. The Principles of Psychology. New York: \\nHenry Holt, Vol.1, pages 403-404. \\n \\nNorman, D. A. 1969. Memory while shadowing. \\nQuarterly Journal of Experimental Psychology,  v o l .  \\n21, pages 85-93. \\n \\nNovianto, R., Williams, M.-A. 2009. The Role of'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Novianto, R., Williams, M.-A. 2009. The Role of \\nAttention in Robot Self-Awareness, The 18 th \\nInternational Symposium on Robot and Human \\nInteractive Communication. Pages 1047-1053. \\n \\nLaird, J. E. 2008. Extending the SOAR cognitive \\narchitecture. In Proceedings of the artificial general \\nintelligence conference. Memphis. TN: IOS Press. \\n \\nPhillips, J. L. 2005. A biologically inspired worki ng \\nmemory framework for robots. Proc. 27 th A nn.  C onf .'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Congitive Science Society. Pages 1750-1755. \\n \\nSkubic, M., Noelle, D., Wilkes, M., Kawamura, K., \\nKeller, J.M. 2004. A biologically inspired adaptive  \\nworking memory for robots. AAAI Fall Symp., \\nWorkshop on the Intersection of Cognitive Science and \\nRobotics. Washington D.C. 2004. \\n \\nSun, R. 2006. The CLARION cognitive architecture: \\nExtending cognitive modelling to social simulation.  \\nIn: Ron Sun (ed.), Cognition and Multi-Agent \\nInteraction. Cambridge University Press, New York.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.8.2 Quartz PDFContext', 'creator': 'PDFCreator Version 1.2.3', 'creationdate': \"D:20130115124904Z00'00'\", 'title': 'Helgason-2012-CR', 'author': 'Helgi', 'moddate': \"D:20130115124904Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\Attention.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6', 'source_file': 'Attention.pdf', 'file_type': 'pdf'}, page_content='Thórisson, K. R. 2009. From Constructionist to \\nConstructivist A.I. Keynote, Technical Report, FS-90-\\n01, AAAI press, Menlo Park, California. \\n \\nWang, P. 1995. Non-Axiomatic Reasoning System: \\nExploring the Essence of Intelligence. Ph.D. diss., Dept. \\nof Computer Science, Indiana Univ., CITY, Indiana.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/390598346\\nVector Embeddings Unveiled: A Comprehensive Exploration of Their Creation,\\nTypes, Applications, Challenges, and Future Directions in Machine Learning\\nResearch · April 2025\\nDOI: 10.13140/RG.2.2.15544.05129\\nCITATIONS\\n0\\nREADS\\n381\\n1 author:\\nPaul Pajo\\nDe La Salle-College of Saint Benilde\\n109 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='109 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Paul Pajo on 09 April 2025.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Vector Embeddings Unveiled: A Comprehensive Exploration of\\nTheir Creation, Types, Applications, Challenges, and Future\\nDirections in Machine Learning\\nby Paul Pajo∗\\nApril 9, 2025\\nAbstract\\nVector embeddings, numerical representations of complex data such as text, images,\\nand audio, have become foundational in machine learning by encoding semantic relation-\\nships in high-dimensional spaces. This paper provides a thorough examination of their'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='creation via neural networks (e.g., Word2Vec, BERT, CLIP), categorization into word, sen-\\ntence, document, image, audio, and multimodal types, and diverse applications including\\nsemantic search, recommendation systems, and generative AI. We analyze persistent chal-\\nlenges—high dimensionality, interpretability, and scalability—and recent advancements like\\ncontextual embeddings, vector databases, and multimodal integration, supported by empiri-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='cal evidence and theoretical insights. Our findings highlight embeddings’ transformative role\\nin AI, with static models like Word2Vec offering efficiency and contextual models like BERT\\nenhancing semantic precision, though at increased computational cost. We conclude that\\nvector embeddings bridge human-like understanding and machine processing, with future\\nresearch poised to address efficiency, bias mitigation, and cross-modal generalization. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='work synthesizes current knowledge and charts a path for advancing embedding technologies.\\n1 Introduction\\nVector embeddings represent a cornerstone of modern artificial intelligence (AI), enabling ma-\\nchines to process and understand unstructured data—such as text, images, or sounds—by con-\\nverting them into numerical vectors. These vectors, situated in a high-dimensional space, posi-\\ntion similar items closer together, reflecting their semantic or relational proximity. For example,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='in natural language processing (NLP), the words ”king” and ”queen” might occupy nearby coor-\\ndinates, capturing their shared royal context [13]. This intuitive yet powerful concept underpins\\napplications ranging from search engines to generative models like DALL-E.\\nFor those new to the field, consider embeddings as a translation mechanism: just as a\\ndictionary translates words between languages, embeddings translate diverse data into a format'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='machines can analyze. This translation preserves meaning, allowing AI to perform tasks like\\nfinding similar documents or recommending movies. However, creating and utilizing these\\nembeddings involves complex processes, diverse methodologies, and significant challenges.\\nThis paper aims to demystify vector embeddings by exploring their creation, types, ap-\\nplications, challenges, and advancements. We provide a structured analysis for both novices'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='and experts, bolstered by citations to seminal works and recent studies. We also propose fu-\\nture research directions to address unresolved issues, ensuring a comprehensive resource for\\nunderstanding this pivotal technology.\\n∗thanks Grok(xAI) frompaulamerigo.pajojr@benilde.edu.ph\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='2 Creation of Vector Embeddings\\nVector embeddings are generated through a multi-step process involving neural networks trained\\non large datasets:\\n1. Data Collection : A representative dataset is amassed, such as a text corpus (e.g.,\\nWikipedia) for NLP or an image set (e.g., ImageNet) for vision tasks [3].\\n2. Preprocessing: Data is cleaned—text tokenized into words, images resized—to ensure\\nuniformity [9].\\n3. Model Training: Neural networks map data to vectors. Word2Vec predicts word con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='texts [13], BERT uses transformers for contextual understanding [4], and CNNs extract\\nimage features [10].\\n4. Embedding Generation: Trained models produce vectors for new inputs, preserving\\nlearned relationships.\\nModels like Word2Vec (trained on 100 billion words [14]) and CLIP (400 million image-\\ncaption pairs [18]) exemplify this process, balancing efficiency and semantic depth.\\n3 Types of Vector Embeddings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='3 Types of Vector Embeddings\\nEmbeddings vary by data type and granularity: - Word Embeddings: Represent words (e.g.,\\nWord2Vec, 300 dimensions [13]; GloVe [15]). - Sentence Embeddings : Capture sentence\\nmeaning (e.g., Sentence-BERT [20]). - Document Embeddings: Encode entire texts (e.g.,\\nDoc2Vec [11]). - Image Embeddings : Represent visual content (e.g., CNNs [6]). - Au-\\ndio Embeddings: Model sound features (e.g., VGGish [7]). - Multimodal Embeddings :\\nCombine data types (e.g., CLIP [18]).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Combine data types (e.g., CLIP [18]).\\nEach type addresses specific needs, from fine-grained word analysis to cross-modal tasks.\\n4 Applications\\nEmbeddings enable diverse applications: - Semantic Search: Match queries to content by\\nmeaning [17]. - Recommendation Systems: Suggest items via vector proximity [21]. - NLP\\nTasks: Enhance translation, sentiment analysis [9]. - Generative AI: Power text-to-image\\nmodels [19]. - Multimodal Tasks: Enable image captioning [18].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='These applications demonstrate embeddings’ versatility across industries.\\n5 Challenges\\nKey challenges include: - High Dimensionality: Increases computational cost [1]. - Inter-\\npretability: Dimensions lack clear meaning [12]. - Contextual Limitations: Static embed-\\ndings miss nuance [16]. - Scalability: Large datasets strain storage [22].\\n6 Advancements\\nRecent innovations address these issues: - Contextual Embeddings: BERT improves pol-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='ysemy handling [4]. - Vector Databases: Pinecone, Weaviate enhance scalability [17, 22].\\n- Multimodal Models : CLIP integrates data types [18]. - Compression: Quantization\\nreduces resource use [5].\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='7 Analysis\\nStatic embeddings (e.g., Word2Vec) offer efficiency for resource-constrained systems, with train-\\ning times in hours, while contextual models (e.g., BERT) demand days but excel in accuracy\\n[4, 13]. Multimodal embeddings like CLIP suggest a convergence of data modalities, though\\ntheir 512-dimensional vectors require optimization [18]. Vector databases mitigate scalability,\\nsupporting real-time applications [8].\\n8 Conclusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='8 Conclusion\\nVector embeddings are a linchpin of AI, translating complex data into actionable represen-\\ntations. Their evolution from static to contextual and multimodal forms reflects a trade-off\\nbetween efficiency and richness, with infrastructure like vector databases ensuring practical\\ndeployment. Future research should focus on: - Efficiency: Optimizing contextual models\\nfor edge devices. - Bias Mitigation : Addressing training data biases [2]. - Cross-Modal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Generalization: Enhancing multimodal robustness.\\nEmbeddings will continue shaping AI’s ability to mirror human understanding.\\nReferences\\n[1] Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural prob-\\nabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.\\n[2] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.\\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Advances in Neural Information Processing Systems, 29, 2016.\\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\\nscale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 248–255, 2009.\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='arXiv:1810.04805, 2018.\\n[5] Robert M Gray. Quantization and Data Compression. Springer, 2011.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[7] Shawn Hershey, Sourish Chaudhuri, Daniel P W Ellis, Jort F Gemmeke, Aren Jansen,\\nR Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, et al. Cnn architectures for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pages 131–135, 2017.\\n[8] Jeff Johnson, Matthijs Douze, and Herv´ e J´ egou. Billion-scale similarity search with gpus.\\nIEEE Transactions on Big Data, 7(3):535–547, 2019.\\n[9] Daniel Jurafsky and James H Martin. Speech and Language Processing. Pearson, 2009.\\n[10] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='convolutional neural networks. Advances in Neural Information Processing Systems, 25,\\n2012.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='[11] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents.\\nInternational Conference on Machine Learning, pages 1188–1196, 2014.\\n[12] Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018.\\n[13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='representations of words and phrases and their compositionality. Advances in Neural In-\\nformation Processing Systems, 26, 2013.\\n[15] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\\nword representation. Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 1532–1543, 2014.\\n[16] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\\narXiv:1802.05365, 2018.\\n[17] Pinecone. What are vector embeddings. https://www.pinecone.io/learn/\\nvector-embeddings/, 2023.\\n[18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning trans-\\nferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='2021.\\n[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint\\narXiv:2102.12092, 2021.\\n[20] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\\nbert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[21] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B Kantor, editors. Recommender\\nSystems Handbook. Springer, 2011.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-09T01:10:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-09T01:10:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'rgid': 'PB:390598346_AS:11431281363152983@1744161100917', 'source': '..\\\\data\\\\pdf\\\\Embeddings.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings.pdf', 'file_type': 'pdf'}, page_content='Systems Handbook. Springer, 2011.\\n[22] Weaviate. Vector embeddings explained. https://weaviate.io/blog/\\nvector-embeddings-explained, 2023.\\n4\\nView publication stats'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='data transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='ing initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='advancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='tor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='strict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='namic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='designed loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='tion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='of up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Instruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='formation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='sampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='CMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='signiﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='lied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='nAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='work like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='performance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='modal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='embedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='retrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='a dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='cessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='ing data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='for each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='chitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='modiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='with data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='title-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='by extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='and diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='encompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='pairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='in reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='ios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='to enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='negative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='loss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='this issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='penalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='j,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='have signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='examples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='accurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='For each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='improvements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='illustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='guity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='level partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='the data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='oriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='cantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='MSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='scaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='datasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='harder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='PAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='proximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='to top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='tions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='all training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='employ bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='are provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='ative models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='and piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='secured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Seed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='xiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='synthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='benchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='trastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='are generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='SIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='secretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Junaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='information retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='arXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Amanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='han Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='colo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='In Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='arXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='with eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Zhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='trieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='platform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Michael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='on Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='of the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='arXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Linguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Item Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='original sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='containing exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='Yelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='can apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='poetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='land boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='William Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='century. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='cessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='help build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='the lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='King Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'Embeddings2.pdf', 'file_type': 'pdf'}, page_content='King Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/376885449\\nArtiﬁcial Intelligence for Object Detection and its Metadata\\nArticle · December 2023\\nDOI: 10.17605/OSF.IO/FG3SQ\\nCITATIONS\\n5\\nREADS\\n2,055\\n1 author:\\nNarayana Challa\\nJawaharlal Nehru Technological University, Hyderabad\\n26 PUBLICATIONS\\xa0\\xa0\\xa049 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Narayana Challa on 04 February 2024.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='The user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='https://iaeme.com/Home/journal/IJAIML 121 editor@iaeme.com \\nInternational Journal of Artificial Intelligence & Machine Learning (IJAIML)  \\nVolume 2, Issue 01, Jan-Dec 2023, pp. 121-133. Article ID: IJAIML_02_01_012 \\nAvailable online at https://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1 \\nJournal ID: 9339-1263, https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\n© IAEME Publication \\nARTIFICIAL INTELLIGENCE FOR OBJECT \\nDETECTION AND ITS METADATA \\nNarayana Challa'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"DETECTION AND ITS METADATA \\nNarayana Challa \\nDirector of ERP Strategy at Cabinetworks Group, Texas, USA \\nABSTRACT \\nIn the ever-evolving field of computer vision, the infusion of artificial intelligence \\n(AI) has inaugurated a revolutionary era, providing unparalleled precision and \\nefficiency in identifying objects within images and videos. This exploration delves into \\nthe domain of AI -driven object detection, emphasizing metadata's pivotal role in\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='enhancing the understanding and utility of recognized entities. The collaboration \\nbetween AI and metadata enhances the precision of object detection and opens up novel \\navenues for extracting and analyzing information. \\nThe significance of metadata is underscored as it contributes context and \\ncategorization to identified objects. This metadata encompasses crucial details such as \\nobject class, detection location, time of occurrence, and inter -object relationships,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='furnishing invaluable insights for downstream applications like autonomous vehicles, \\nsurveillance, and augmented reality. The research paper showcases the seamless \\nintegration of metadata extraction and management with AI -powered object detection \\nsystems, thereby boosting the accuracy of object identification and tracking. \\nThis study sheds light on the intricate interplay between artificial intelligence and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='computer vision, molding a landscape where precision and adaptability redefine the \\nboundaries of object detection capabilities. \\nKeywords: Artificial Intelligence (AI), Machine Learning (ML), Internet of Things \\n(IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data \\nScience careers \\n \\nCite this Article: Narayana Challa, Artificial Intelligence for Object Detection and Its \\nMetadata, International Journal of Artificial Intelligence & Machine Learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='(IJAIML), 2(1), 2023, pp. 121-133. \\nhttps://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 122 editor@iaeme.com \\nINTRODUCTION \\nArtificial Intelligence (AI) has significantly advanced in computer vision, particularly object \\ndetection. This paper explores how AI has transformed object detection, emphasizing the \\ncrucial role of metadata in enhancing its capabilities. \\nIn the digital era, the abundance of visual data, from surveillance to medical imaging and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='self-driving cars, necessitates accurate object detection. AI, especially with deep learning \\nmodels like convolutional neural networks (CNNs), has become a robust solution, overcoming \\nchallenges such as occlusion and scale variations with increasing accuracy. \\nAI, especially utilizing CNNs, has revolutionized object detection in various sectors dealing \\nwith vast visual data. These models excel in acquiring hierarchical feature representations,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='adapting to the dynamic nature of visual input, and continuously improving comprehension. \\nObject detection faces challenges like occlusion and background clutter. Through \\ncontinuous learning and sophisticated algorithms, AI techniques adeptly address these \\nchallenges. The adaptive nature of AI allows the development of nuanced models for accura te \\ndetection in complex environments. \\nWhile AI enhances object detection, metadata is crucial, providing additional information'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='such as object class, position, detection time, and associations. Integrating metadata with AI -\\ndriven systems transforms the comprehension and utility of identified objects. \\nMetadata goes beyond visual identification, adding context to detected objects. \\nUnderstanding object class, location, and temporal occurrence enhances visual data \\ninterpretation, contributing to richer information. In downstream applications like augmented'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='reality and surveillance, metadata becomes invaluable, aiding in informed decision-making. \\nThe study demonstrates the seamless integration of AI object identification systems with \\nmetadata extraction. This integration enhances object tracking and recognition accuracy, paving \\nthe way for sophisticated decision-making across various applications. \\n \\nConsequences of AI-Powered Object Identification and Integration with \\nMetadata'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Metadata \\nIntegrating AI with metadata for object detection has far -reaching consequences, extending \\nbeyond a single domain. This synergy can revolutionize various industries, enhancing security \\nmeasures and industrial automation capabilities. \\nThe central focus of this paper is the crucial integration of metadata with AI object detection \\nsystems, representing a defining aspect of the exploration. Metadata, containing contextual'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='information about detected objects, becomes pivotal for unlocking the full potential of object \\ndetection. It includes critical details like object class, detection location, time, and inter -object \\nrelationships. This symbiosis between AI and metadata refines object identifi cation and \\ntracking and finds applications in sectors like autonomous vehicles, surveillance, and \\naugmented reality.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 123 editor@iaeme.com \\nThe implications of the collaboration between AI and metadata extend into heightened \\nsecurity protocols, increased efficiency in industrial automation, and a deeper understanding of \\nthe visual environment. Ethical and privacy concerns related to AI-driven object detection and \\nmetadata are also addressed in this paper. This introduction sets the stage for a comprehensive'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='exploration of AI in object detection and its synergistic interplay with metadata, offering \\ninsights into the transformative potential of t hese technologies. It provides a glimpse into a \\nfuture where intelligent object detection systems redefine our perception and interaction with \\nthe visual world. \\nObject detection in computer vision involves locating and identifying items in images or \\nvideo frames, going beyond mere identification to accurately define their locations using'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='bounding boxes. This technical advancement is fundamental to applications lik e robots, \\naugmented reality, driverless cars, and surveillance systems, shaping how machines perceive \\nand interact with their visual surroundings. \\nObject detection commonly involves the following fundamental elements: \\nIdentifying the position and dimensions of objects within an image. This is often visualized by \\noutlining objects with bounding boxes, specifying their spatial coordinates (x, y) and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='dimensions (width and height). \\nAssigning a label or category to each detected object, indicating its type. For instance, in a \\nscene with diverse objects, object detection can discern whether there are cars, pedestrians, \\nanimals, or other entities and assign corresponding labels. \\nDeep learning models, such as convolutional neural networks (CNNs), are frequently \\nemployed for object detection. These models are designed to handle both localization and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='classification tasks concurrently. Through training on extensive datasets with label ed images, \\nthey can recognize objects and accurately determine their positions. \\nEffectively training object detection models necessitates large datasets with annotated \\nimages. These datasets typically include images where objects of interest are labeled with \\nbounding boxes and associated class labels. During training, models learn from these examples'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='to precisely predict object positions and classes in new, unseen images. \\nWhat makes object detection crucial? \\nObject detection holds significant importance in computer vision for various reasons. The key \\nrationales for its significance include: \\n1. Visual Scene Content Interpretation: \\nObject detection aids in interpreting the content of visual scenes. It enables computers to \\nidentify and locate objects, providing context for more sophisticated processing. Object'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='detection is crucial in industrial settings for monitoring product movement, automating \\nprocesses, and ensuring quality control. Additionally, it promotes greater independence for \\nindividuals with visual impairments by recognizing and characterizing object s and their \\nlocations. \\n2. Obstacle Avoidance and Navigation: \\nObject detection is essential for identifying and avoiding obstacles, navigating surroundings,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='and making decisions based on the presence of objects. This is particularly crucial in \\nautonomous vehicles, drones, and robotics applications. For security and surveillance systems, \\nobject detection enables real-time identification and monitoring of items and people, enhancing \\nthreat detection and response. In retail, it automates checkout procedures, tracks products on \\nshelves, and manages inventory for improved productivity and consumer satisfaction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 124 editor@iaeme.com \\n3. Medical Imaging and Diagnosis: \\nIn medical imaging, object detection aids in locating and identifying specific structures, \\ncontributing to diagnosis and therapy. It ensures precision in identifying objects within images, \\nfacilitating better healthcare outcomes. \\n4. Interactive Experiences and Entertainment:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='4. Interactive Experiences and Entertainment: \\nObject detection follows user movements and gestures in interactive applications and games, \\ncreating immersive experiences. It automatically tags and categorizes films and images, \\nsimplifying the search and organization of extensive media collections. \\n5. Automation and Speeding Up Processes: \\nObject detection automates tasks across various industries, reducing the need for manual'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='involvement and accelerating procedures. Augmented reality (AR) applications recognize real-\\nworld objects through object detection, enhancing user experiences by overl aying digital data \\nor graphics. Combining object detection with natural language processing (NLP) bridges \\ninformation gaps between text and images, facilitating content comprehension. \\n6. Environmental Monitoring and Wildlife Research:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Researchers and conservationists benefit from object detection by obtaining crucial data for \\ntracking environmental changes, monitoring wildlife, and studying animal behavior. It plays a \\npivotal role in understanding and preserving ecosystems. \\nObject detection is a versatile and indispensable technology with applications ranging from \\nindustrial automation to healthcare, security, entertainment, and environmental conservation. \\nObject Detection and the Impact of Deep Learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"Object Detection and the Impact of Deep Learning \\nThe realms of object detection and deep learning are intricately linked, with deep learning \\ntechniques profoundly shaping the landscape of object detection. The advent of deep learning \\nhas brought about a paradigm shift, enhancing the accuracy and efficien cy of object detection \\nsystems to handle intricate tasks with remarkable precision. Let's delve into the intricate \\nconnection between object detection and deep learning:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"CNNs, in particular, are deep learning models that include specialized convolutional layers \\nthat are skilled at extracting spatial information from images. These layers are excellent at \\nrecognizing objects because they can recognize linkages and local patt erns among pixels. \\nModels pre -trained on large datasets (like ImageNet) can be refined for particular item \\nrecognition tasks using relatively small, labeled datasets thanks to deep learning's support for\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='transfer learning. As a result, less training data is needed for functions involving bespoke object \\ndetection. \\nConvolutional neural networks (CNNs) are deep learning models that show proficiency in \\nautomatically extracting hierarchical features from images. Acquired traits are essential for \\ndifferentiating objects with different sizes, shapes, and orientations. Deep neural networks can \\ncapture high-level features that represent the components and structures of an item and low -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='level features like edges and textures. Learning The benefit of end -to-end training for object \\nlocalization and classification is provided by deep learning models. This suggests that an \\nindividual model can recognize things and anticipate where they will be found in an image \\ninstead of conventional computer vision techniques that call for distinct, labor-intensive phases \\nfor object recognition and feature extraction.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='for object recognition and feature extraction. \\nA wide range of object sizes, types, and degrees of complexity may be handled by deep \\nlearning models with versatility. They may adapt to real -world events and are not limited to \\nsimple items or shapes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 125 editor@iaeme.com \\nDeep learning models are particularly good at handling partially veiled or occluded objects. \\nTheir capacity to pick up on intricate object appearances makes it easier to identify objects even \\nwhen they are partially obscured. Ongoing research and advanceme nts in deep learning \\ncontribute to the continual enhancement of the accuracy and speed of object detection systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='This ongoing progress renders them suitable for an expanding range of applications. \\nLarge, labeled datasets are essential for the effectiveness of deep learning -based item \\ndetection. Deep learning models may learn from various examples, increasing their accuracy \\nand robustness. This is made possible by the availability of large image datasets that have been \\ntagged with object locations and class labels. Researchers have introduced deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='architectures specifically designed for object detection. Examples include SSD (Single Shot \\nMultiBox Detector), YOLO (You Only Look Once), and Fast er R -CNN. These designs \\neffectively combine localization and classification to optimize tasks related to object \\nidentification. \\nDeep learning has made real-time object detection possible, allowing for high-speed object \\ntracking and recognition in applications like augmented reality, video surveillance, and \\nautonomous cars. \\nChatGPT'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='autonomous cars. \\nChatGPT \\nThe realms of object detection and deep learning are intricately connected, with deep learning \\ntechniques significantly influencing and enhancing the landscape of object detection. The \\nintroduction of deep learning has led to a paradigm shift, considerably improving the precision \\nand efficiency of object detection systems for handling complex tasks. \\n1. Role of Convolutional Neural Networks (CNNs):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='1. Role of Convolutional Neural Networks (CNNs): \\nCNNs, a deep learning model, feature specialized convolutional layers adept at extracting \\nspatial information from images. These layers excel at recognizing objects by identifying \\nlinkages and local patterns among pixels. Pretrained models, such as those o n ImageNet, can \\nbe fine-tuned for specific object recognition tasks using smaller labeled datasets, showcasing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='the transfer learning capabilities of deep learning and reducing the need for extensive training \\ndata. \\n2. Hierarchical Feature Extraction: \\nCNNs automatically extract hierarchical features from images, which is crucial for \\ndistinguishing objects with varying sizes, shapes, and orientations. These models capture high-\\nlevel features representing components and low-level features like edges and textures. End-to-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='end training facilitates object localization and classification within a single model, eliminating \\nthe need for separate, labor-intensive phases in conventional computer vision techniques. \\n3. Versatility and Adaptability: \\nDeep learning models exhibit versatility in handling various object sizes, types, and \\ncomplexities. They can adapt to real -world scenarios and are not limited to simple objects or'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='shapes. Notably, deep learning models excel at handling partially veiled or occluded objects, \\nthanks to their ability to recognize intricate object appearances even when partially obscured. \\nOngoing research contributes to continuous improvements in the accuracy and speed of object \\ndetection systems, expanding their applicability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 126 editor@iaeme.com \\n4. Importance of Large, Labeled Datasets: \\nLarge, labeled datasets are essential for the effectiveness of deep learning -based object \\ndetection. These datasets enable models to learn from diverse examples, enhancing their \\naccuracy and robustness. Researchers have introduced specialized deep learning  architectures'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='for object detection, such as SSD, YOLO, and Faster R -CNN, which efficiently combine \\nlocalization and classification to optimize object identification tasks. \\n5. Real-Time Object Detection: \\nDeep learning has enabled real -time object detection, facilitating high -speed object tracking \\nand recognition in applications like augmented reality, video surveillance, and autonomous \\nvehicles. \\nThe marriage of object detection and deep learning, mainly through CNNs and other'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='specialized architectures, has ushered in a transformative era, offering enhanced accuracy, \\nadaptability, and real-time capabilities across diverse applications. Ongoing adva ncements in \\ndeep learning continue to push the boundaries of what is achievable in object detection. \\n \\nHow Object Detection Operates \\nObject detection, a critical computer vision task, involves using algorithms, particularly deep'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='learning models, to identify and locate objects within images or video frames. The operational \\nprocess of object detection can be broken down as follows: \\n1. Data Gathering and Preparation: \\nCollect a dataset comprising images or video frames displaying objects of interest. \\nLabel each image with bounding boxes indicating the object\\'s location and class labels \\nspecifying the object type (e.g., \"car,\" \"person,\" \"cat\").'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 127 editor@iaeme.com \\n2. Selection of Deep Learning Model: \\nFor object detection, utilize deep learning models, especially convolutional neural networks \\n(CNNs). \\nVarious models like SSD, RetinaNet, YOLO, and Faster R -CNN address specific \\napplication, speed, and accuracy concerns. \\n3. Training the Model: \\nTrain the selected deep learning model using the prepared dataset.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Adjust internal parameters to minimize the difference between model predictions and \\nground truth (labeled bounding boxes and class labels). \\nOptimize a loss function measuring the discrepancy between predicted and actual object \\npositions and classifications through an iterative training process. \\n4. Inference: \\nAfter training, use the model to identify objects in new, unlabeled images or video frames. \\nDuring inference, analyze the image either as a whole or by segmenting it into smaller'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"regions based on the model architecture. \\n5. Prediction Outputs: \\nGenerate predictions for each region, including: \\nBounding boxes (coordinates) around local objects. \\nClass designations corresponding to the identified objects. \\nConfidence scores indicate the model's confidence in each prediction. \\n6. Post-Processing: \\nAddress repetitive boxes or predictions with low confidence scores using post -processing \\ntechniques.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='techniques. \\nNon-maximum suppression (NMS) is one such method to eliminate redundant boxes, \\nretaining the most accurate and confident predictions. \\n7. Final Results: \\nDraw bounding boxes around objects in the image to represent the final object detection results, \\nincluding position and class information. \\n8. Applications: \\nApply object detection findings in various domains, such as augmented reality, safety systems, \\nautonomous vehicle decision-making, and object tracking.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 128 editor@iaeme.com \\nMost Popular Object Detection Algorithms \\n \\nConvolutional neural networks, such as Mask R -CNN, R -CNN (Region -Based \\nConvolutional Neural Networks), YOLO (You Only Look Once), MobileNet, and SqueezeDet, \\nare frequently used methods for object detection. \\nR-CNN: \\nThe Region -based Convolutional Neural Network (R -CNN) is a computer vision model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='designed for object recognition and image detection. Here\\'s a breakdown of its functionality: \\n1. Region Proposal: \\nR-CNN begins by identifying potential objects in an image using a \" selective search method,\" \\nsuggesting areas likely to contain objects. \\n2. Convolutional Neural Network (CNN) Analysis: \\nR-CNN utilizes a CNN for each suggested region to analyze and understand the content, \\ncreating a descriptive map of the area. \\n3. Support Vector Machine (SVM):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='3. Support Vector Machine (SVM): \\nR-CNN applies a support vector machine to identify the objects present in the regions. It \\ncategorizes and recognizes objects using labels. \\n4. Bounding Box Refinement: \\nThe model refines the bounding boxes surrounding the recognized objects, adjusting them to \\nmatch the shapes of the objects better. \\n5. Result Selection: \\nR-CNN selects the most accurate and confident outcomes, eliminating less trustworthy ones to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='prevent displaying excessively similar findings. \\n6. Drawbacks and Enhancements: \\nR-CNN, while a breakthrough, was slow and complex for object identification. Later models \\nlike Fast R -CNN and Faster R -CNN addressed these issues, introducing improvements for \\nincreased accuracy and speed in object detection.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 129 editor@iaeme.com \\nMask R-CNN: \\n• An extension of Faster R -CNN, Mask R -CNN, introduced in 2017, handles instance \\nsegmentation, providing pixel -level masks for each unique instance of an object in \\naddition to object detection. \\n• Mask R-CNN utilizes a CNN to extract features, refines bounding boxes for precise \\nobject fitting, and creates binary masks to identify pixel-level object instances.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='• The model groups identified items into various types or categories and employed non -\\nmaximum suppression to remove duplicate or overlapping results after object detection, \\nmask creation, and classification. \\n• Applications for Mask R -CNN are found in domains requiring instance -level \\nsegmentation and pixel -level precision, such as robotics, computer vision tasks, and \\nmedical imaging. \\n \\nDeep learning-based object identification models are crucial in recognizing items in images'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='or video frames and delineating them with bounding boxes. After object detection, metadata \\nmodels come into play, focusing on obtaining additional data or characteristics associated with \\nthe detected objects. The extracted metadata is linked to each bounding box, providing precise \\nattribute descriptions for each recognized object. Here are some common metadata attributes: \\n• Object Class: Identifying the type of object (e.g., \"car,\" \"person,\" \"dog\").'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"• Color: Specifying the object's color or color patterns. \\n• Size: Determining the object's actual dimensions. \\n• Orientation: Identifying the object's orientation or position. \\n• Velocity: Determining the motion or speed of the object. \\n• Timestamp: Documenting the moment the object was detected. \\n• Context: Recognizing how an object fits into its surroundings or connects to other \\nobjects.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 130 editor@iaeme.com \\nCombining spatial information from the bounding box with associated metadata enables \\npinpointing specific characteristics of individual objects. For instance, metadata might describe \\na detected car as \"red,\" \"sedan,\" and \"parked.\" \\nApplications for combined object detection and metadata information span various fields:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='• Traffic Monitoring: Identifying vehicle types, colors, and speeds for traffic studies. \\n• Security and Surveillance: Sorting individuals and items based on potential threats. \\n• Autonomous Vehicles: Recognizing characteristics and motion of nearby automobiles \\nand people. \\n• Retail Inventory: Monitoring goods on store shelves, including locations and \\nattributes. \\n• Augmented Reality: Overlaying digital data based on identified items and their \\ncharacteristics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='characteristics. \\n• Medical Imaging: Analyzing and recording characteristics of anatomical structures or \\nanomalies. \\nCombining object detection and metadata models provides a comprehensive and contextual \\nunderstanding of objects within an image or video frame. This information enhances the utility \\nand functionality of computer vision systems, enabling improved decision -making, analysis, \\nand automation across diverse fields. \\nYOLO (You Only Look Once):'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='YOLO (You Only Look Once): \\nYOLO (You Only Look Once) is a real -time object detection system widely popular in \\ncomputer vision and deep learning. Introduced by Joseph Redmon and Santosh Divvala in a \\nseries of papers, the original paper titled \"You Only Look Once: Unified, Real -Time Object \\nDetection\" was published in 2016. YOLO is designed for instantaneous object detection in \\nvideo and image streams, quickly locating and identifying multiple objects in a single neural \\nnetwork pass.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='network pass. \\nKey features and aspects of YOLO include: \\n1. Unified Approach: \\n• YOLO takes a unique approach by splitting the image into a grid and predicting \\nbounding boxes and class probabilities for every grid cell. Unlike region -based object \\nidentification techniques like R-CNN, YOLO analyzes the entire image in a single pass. \\n2. Single Pass Analysis: \\n• YOLO effectively analyzes the complete picture in one pass, providing predictions for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='every object simultaneously. This architecture significantly improves speed compared \\nto traditional techniques that require multiple network passes. \\n3. Bounding Box Prediction: \\n• YOLO predicts bounding boxes that define the locations of objects for each grid cell. \\nConfidence scores indicate the accuracy of predictions. Class probability predictions are \\nalso used to identify the object type in each bounding box.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 131 editor@iaeme.com \\n4. Non-Maximum Suppression: \\n• YOLO uses non -maximum suppression to eliminate redundant bounding boxes, \\nretaining the most reliable and accurate ones. This post-prediction step is performed for \\nevery grid cell. \\n5. Iterative Improvements: \\n• YOLO has undergone several revisions, including YOLOv1, YOLOv2 (YOLO9000), \\nYOLOv3, and YOLOv4. Each iteration has surpassed the previous ones in accuracy and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"speed, solidifying YOLO's reputation as a flexible option for various object detection \\napplications. \\n6. Applications: \\n• YOLO's strength lies in its high-precision real-time object identification capability, \\nmaking it ideal for applications such as robots, autonomous cars, surveillance, and \\nmore. \\n7. Ongoing Relevance: \\n• YOLO is still frequently used in computer vision and has inspired the development of \\nother real-time object detection methods.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"other real-time object detection methods. \\n• YOLO's innovative approach to real -time object detection, combined with its iterative \\nimprovements, has made it a widely adopted and versatile solution for various \\napplications in the field of computer vision. \\nMobileNet \\nMobileNet is a family of simplified convolutional neural network architectures designed for \\nefficient and low-latency deep learning inference on mobile and embedded devices. Developed\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"by Google's Research Brain Team, MobileNet prioritizes high performance within a small \\nmemory and computational framework. Notable features include using depthwise separable \\nconvolutions and dividing the convolution process into depthwise and pointwise stages to \\nreduce parameters and calculations without compromising precision . MobileNet is widely \\nutilized in settings with limited computational resources, such as edge computing, IoT devices,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='and mobile phones. It has undergone iterative improvements with versions like MobileNetV1, \\nV2, and V3, each introducing enhancements to the architecture, accuracy, and speed. \\nIn the realm of real -time object detection for autonomous driving, SqueezeDet is a \\nspecialized model designed for identifying objects on roads. Introduced in a 2017 paper titled \\n\"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Object Detection for Autonomous Driving,\" SqueezeDet is built on the lightweight SqueezeNet \\narchitecture, well-suited for embedded and real-time applications. It processes whole images in \\na single pass using a fully convolutional neural network (FCN) architecture, generating \\nbounding boxes and class predictions for recognized objects. SqueezeDet incorporates \"squeeze \\nand excitation\" blocks to dynamically vary channel -wise scaling factors, improving object'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='feature representation. The model predicts bounding boxes and assigns class labels to each box, \\nachieving a balance between processing efficiency and accuracy for real -time object detection \\nin autonomous vehicles. \\nMobileNet and SqueezeDet  showcase the importance of efficient and lightweight neural \\nnetwork architectures in enabling deep learning applications on devices with limited \\ncomputational capabilities.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content=\"Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 132 editor@iaeme.com \\nMobileNet's versatility makes it a popular choice for various applications, while \\nSqueezeDet excels in the specific context of real-time object detection for autonomous driving. \\nCONCLUSION \\nWhen integrated with analytics and metadata, object detection proves invaluable across diverse\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='industries, especially in manufacturing and quality control. This powerful combination excels \\nin identifying flaws on assembly lines, optimizing processes, and e nabling preventive \\nmaintenance. For inventory tracking, metadata enhances supply chain efficiency, while in \\nproduct monitoring, it predicts quality and facilitates customization. Integration is equally \\nbeneficial in tracking machinery functionality, enabli ng predictive maintenance, and ensuring'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='compliance in sectors with stringent regulations. This amalgamation revolutionizes \\nmanufacturing and quality control, fostering data -driven decision-making, improving product \\nquality, reducing waste, and enhancing overall productivity. \\nREFERENCES \\n[1] Deci, “Deci Introduces YOLO-NAS - A Next-Generation, Object Detection Foundation Model \\nGenerated by Deci’s Neural Architecture Search Technology,”  Deci, May 03, 2023. \\nhttps://deci.ai/blog/yolo-nas-foundation-model-object-detection'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='[2] S. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, and B. Lee, “A survey of modern \\ndeep learning based object detection models,”  Digital Signal Processing, vol. 126, p. 103514, \\nJun. 2022, doi: https://doi.org/10.1016/j.dsp.2022.103514. \\n[3] “SqueezeDet: Deep Learning for Object Detection,”  Mez Gebre. \\nhttps://mez.sh/2017/04/21/squeezedet-deep-learning-for-object-detection/ (accessed Dec. 22, \\n2023).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='2023). \\n[4] R. Alake, “How Does AI Detect Objects? (Technical),”  Medium, Jan. 14, 2020. \\nhttps://towardsdatascience.com/how-does-ai-detect-objects-technical-d8d63fc12881 \\n[5] D. S. Shenwai, “Top Object Detection Algorithms and Libraries in Artificial Intelligence \\n(AI),” MarkTechPost, Jul. 18, 2023. https://www.marktechpost.com/2023/07/18/top -object-\\ndetection-algorithms-and-libraries-in-artificial-intelligence-ai/ (accessed Dec. 22, 2023).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='[6] Rajath Karangara, “Unique Methods for Highly Populous Countries to Leverage Post-Pandemic \\nEconomy to Ramp Up Digital Payments,” SSRG international journal of computer science and \\nengineering, vol. 10, no. 7, pp. 21–26, Jul. 2023, doi: https://doi.org/10.14445/23488387/ijcse-\\nv10i7p103.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 133 editor@iaeme.com \\nAuthor Details \\nNarayana Challa, Director of ERP Strategy, IEEE Senior Member, Texas, USA  \\nNarayana Challa serves as the Director of ERP Strategy in the manufacturing industry.  \\nExpert in Digital Transformation leveraging Enterprise Resource Planning to unlock \\noperational efficiencies in supply chain elements such as manufacturing and inventory'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='management. He has led multiple teams across various technologies throughout his career, \\ndemonstrating enthusiasm for researching new technologies and processes within the \\ninformation technology department. With a diverse skill set, he has executed numerous projects \\nin roles such as architect, data engineering, data ingestion, ETL developer,  administrator, and \\nenterprise architect. His expertise extends to cloud platforms, notably Amazon Web Services \\nand Azure.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='and Azure. \\n \\n \\n \\n \\nCitation: Narayana Challa, Artificial Intelligence for Object Detection and Its Metadata, International Journal of \\nArtificial Intelligence & Machine Learning (IJAIML), 2(1), 2023, pp. 121-133 \\n \\nDOI: https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\nArticle Link:  \\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJAIML/VOLUME_2_ISSUE_1/IJAIML_02_01_012.pdf \\n \\nAbstract:  \\nhttps://iaeme.com/Home/article_id/IJAIML_02_01_012'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-01-09T10:36:51+05:30', 'title': 'Artificial Intelligence for Object Detection and Its Metadata', 'author': 'Narayana Challa', 'subject': 'International Journal of Artificial Intelligence & Machine Learning (IJAIML)', 'keywords': 'Artificial Intelligence (AI), Machine Learning (ML), Internet of Things (IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data Science careers', 'moddate': '2024-01-09T10:36:51+05:30', 'rgid': 'PB:376885449_AS:11431281221989761@1707016071416', 'source': '..\\\\data\\\\pdf\\\\ObjectDetection.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'ObjectDetection.pdf', 'file_type': 'pdf'}, page_content='Copyright: © 2023 Authors. This is an open -access article distributed under the terms of the Creative Commons \\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the \\noriginal author and source are credited. \\n \\nThis work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0) . \\n \\n \\n✉ editor@iaeme.com \\n \\n \\nView publication stats')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "851ae6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Attention capabilities for AI systems \\n \\nHelgi Páll Helgason1, Kristinn R. Thórisson1,2 1Center for Analysis & Design of Intelligent Agents / School of Computer Science, Venus 2nd floor, Reykjavik \\nUniversity,Menntavegur 1, 101 Reykjavik, Iceland \\n2Icelandic Institute for Intelligent Machines, 2. h. Uranus, Menntavegur 1, 101 Reykjavik, Iceland \\nhelgih09@ru.is, thorisson@ru.is  \\nKeywords: Artificial intelligence, attention, resou rce management',\n",
       " 'Abstract: Much of present AI research is based on t he assumption of computational systems with infinit e resources, \\nan assumption that is either explicitly stated or i mplicit in the work as researchers ignore the fact that most \\nreal-world tasks must be finished within certain ti me limits, and it is the role of intelligence to ef fectively \\ndeal with such limitations. Expecting AI systems to  g i v e  e q u a l  t r e a t m e n t  t o  e v e r y  p i e c e  o f  d a t a  t h e y',\n",
       " 'encounter is not appropriate in most real-world cas es; available resources are likely to be insufficie nt for \\nkeeping up with available data in even moderately c omplex environments. Even if sufficient resources a re \\navailable, they might possibly be put to better use  t ha n bl i n dl y  a ppl y i ng  t he m  t o ev e ry  pos s i bl e  pi e c e of  \\ndata. Finding inspiration for more intelligent reso urce management schemes is not hard, we need to loo k no',\n",
       " 'further than ourselves. This paper explores what hu man attention has to offer in terms of ideas and co ncepts \\nfor implementing intelligent resource management an d how the resulting principles can be extended to \\nlevels beyond human attention. We also discuss some  ideas for the principles behind attention mechanis ms \\nfor artificial (general) intelligences. \\n1 INTRODUCTION \\nThe field of AI has a long history of targeting \\nisolated, well-defined problems to demonstrate',\n",
       " 'isolated, well-defined problems to demonstrate \\nintelligent capabilities. While useful, many of the se \\nproblems (and especially their task environments, a s \\nperceived by the system) are extremely simple \\ncompared to the problem of learning how to solve \\nnovel tasks and adapting to changes in real-world \\nenvironments - a problem which must be addressed \\nand solved in order for AI systems to approach \\nhuman-level intelligence. Given the nature of this',\n",
       " 'prior work, it is not surprising that limited focus  has \\nbeen given to real-time processing and resource \\nmanagement. However, the design of any AI system \\nexpected to learn and perform a range of tasks in \\neveryday environments needs to face these realities: \\n \\n\\uf001 The real world is highly dynamic and complex \\nand can provide an abundance of information \\nat any given moment. \\n\\uf001 Resources of any intelligent system are not \\nonly limited, but insufficient in light of the',\n",
       " 'only limited, but insufficient in light of the \\nmassive amount of information available from \\nthe environment. \\n \\n\\uf001 A range of time constraints, many of which \\nare dictated by the environment, must be \\nsatisfied in order to ensure safe and successful \\noperation of the system. \\n \\nMuch of existing work in the field of AI is also \\nbased on greatly simplified operating assumptions -  \\na case in point being the practically impossible (b ut \\nsurprisingly common) assumption of infinite',\n",
       " 'surprisingly common) assumption of infinite \\nresources, often in terms of storage but particular ly \\nin terms of processing: A system based on this \\nassumption will fail to perform and potentially cra sh \\nin real world operation when fed with information a t \\na greater rate than it is capable of processing. To  \\nfind inspiration for implementing intelligent \\nresource management we need not look far, nature \\nhas provided us with a prime example in human',\n",
       " 'has provided us with a prime example in human \\nattention; a cognitive function that enables us to \\nfocus our limited resources selectively on \\ninformation that is most important to us at any giv en \\nmoment as we perform various tasks while',\n",
       " 'remaining reactive to unexpected but important \\nevents in the environment. Consider that while \\nreading this chapter you have effectively ignored \\nmore than 99.9% of the numerous things that your \\nmind could have spent time and resources on doing. \\nPerhaps not surprisingly, it turns out that this is  \\nexactly the kind of resource management that is \\nrequired to enable AI systems to approach human-\\nlevel intelligence in real-world environments. Thus ,',\n",
       " 'it makes perfect sense to investigate how AI system s \\ncan be endowed with this cognitive function for the  \\npurpose of improving their operation and making \\nthem applicable to more open-ended and complex \\ntasks and environments. The goal need not be to \\nreplicate any biological function in detail, but ra ther \\nto extract useful concepts and methods from the \\nbiological side while leaving undesirable limitatio ns \\nbehind in order to facilitate the creation of AI',\n",
       " 'behind in order to facilitate the creation of AI \\nsystems that can successfully operate in real-world  \\nenvironments in realtime using limited resources.  \\n W h i l e  a t t e n t i o n  h a s  b e e n  l a r g e l y  i g n o r e d  i n  \\nthe field to-date, there are notable exceptions. Th ese \\ninclude cognitive architectures such as NARS \\n(Wang, 1995), LIDA (Baars, 2009) and Clarion \\n(Sun, 2006). However, the attentional functionality  \\nimplemented in these systems is incomplete in',\n",
       " 'implemented in these systems is incomplete in \\nvarious ways, such as focusing solely on data-\\nfiltering (ignoring control issues, e.g. how \\nprioritization affects processing of selected data)  and \\nexternal environmental information (ignoring \\ninternal system states). The ASMO framework \\n(Novianto, 2009) is somewhat unique as it assumes a \\ntight coupling between attention and self-awareness  \\nand includes focus on internal states. However, non e',\n",
       " 'of this work addresses realtime processing, which i s \\none of the major reasons we desire attentional \\nfunctionality, in a vigorous fashion. Attention has  \\nalso been studied in relation to AI within the limi ted \\nscope of working memory (c.f. Phillips 2005 and \\nSkubic 2004). While attention and working memory \\nare closely related, this is a restrictive context to \\nstudy attention within as working memory can in \\nmost cases be modelled as a cognitive function',\n",
       " 'most cases be modelled as a cognitive function \\nrather than an architectural component. \\nThis paper starts with a brief overview of \\nhuman attention and subsequently attempts to \\nextract principles that may be useful for AI system s. \\nThis is followed by a discussion of how these \\nprinciples might be extended to levels beyond \\nhuman attention for meta-reasoning and \\nintrospection. We then present a high-level design of \\nan attention mechanism intended for AI \\narchitectures.  \\n2 HUMAN ATTENTION',\n",
       " 'architectures.  \\n2 HUMAN ATTENTION \\nResearch of human attention has a long history \\ndating back to the beginnings of psychology. Back \\nin 1890, the American psychologist William James \\nwrote the following (James 1890): \\n \\n“Everyone knows what attention is. It is the taking \\npossession by the mind, in clear and vivid form, of  \\none out of what seem several simultaneously \\npossible objects or trains of thought. Focalization , \\nconcentration, of consciousness are of its essence.  It',\n",
       " 'implies withdrawal from some things in order to \\ndeal effectively with others, and is a condition wh ich \\nhas a real opposite in the confused, dazed, \\nscatterbrained state which in French is called \\ndistraction, and Zerstreutheit in German.” \\n \\n- William James \\n \\nThis elegant description indicates that the \\nimportance of attention for the human mind was \\nidentified as early as the 18 th century. The beginning \\nof modern attention research is commonly tied to',\n",
       " 'of modern attention research is commonly tied to \\nColin Cherry’s work on what has been called the \\n“cocktail party effect” (Cherry 1953), which \\naddresses how we are able to focus on particular \\nsensory data in the presence of distracting \\ninformation and noise, such as following and \\nparticipating in a conversation at a cocktail party  in \\nthe presence of many other conversations and \\nbackground noise, and still be able to catch when \\nsomeone calls our name in the background. The',\n",
       " 'someone calls our name in the background. The \\nability to be in a focused state of attention while  \\nremaining reactive to unexpected events, seems to \\ncall for a selective filtering mechanism of some so rt \\nwhile at the same time requiring deliberate steerin g \\nof cognitive resources. The cocktail party scenario  is \\na good illustration of the dual nature of attention : \\nWe will refer to the deliberate, goal-driven side a s \\ntop-down attention and the reactive, stimulus-driven',\n",
       " 'side as bottom-up attention. \\nA number of models for attention were \\nsubsequently proposed, some of which were \\nconsidered early selection models  a s  s e l e c t i o n  o f  \\nsensory information is assumed to occur early in th e \\nsensory pipeline based on primitive physical featur es \\nof the information. This implies that the \\ndetermination of what is important and should be \\nselected is based on shallow, primitive processing \\nwith very limited or non-existent analysis of',\n",
       " 'with very limited or non-existent analysis of \\nmeaning. The Broadbent filter model (Broadbent \\n1958) is the most prominent of these. A number of \\nlate selection models  h a v e  a l s o be e n  pr o pos e d,  t h a t',\n",
       " 'assume further analysis of incoming sensory \\ninformation must be performed in order to determine  \\nits relevance and carry out efficient selection. Th e \\nDeutsch-Norman (Norman 1969) model is based on \\nthe assumption that sensory information is not \\nactually filtered, but processed to the point of \\nactivating representations stored in memory. \\nSelection then occurs at the level of representatio ns, \\nwhere the most active ones are selected for further',\n",
       " 'processing. The model also assumes an attentional \\nbottleneck at this point, where only one \\nrepresentation can be selected for processing at a \\ntime. These two classes of attention models are \\nreferred to as the early vs. late selection  models, and \\nhave resulted in some debate. Shortcomings of many \\nearly selection models are obvious, as they fail to  \\naccount for parts of the cocktail party effect, \\nespecially phenomena such as noticing your own',\n",
       " 'especially phenomena such as noticing your own \\nname being called from across the room while \\nengaged in conversation. This contradicts the model , \\nas the physical characteristics of the data (our na me \\nbeing called) would not be sufficient to attract ou r \\nattention and pass through the filter; some analysi s \\nof meaning must be involved. \\nSome more recent theories and models of \\nattention focus on the interaction between top-down  \\nand bottom-up attention. In (Knudsen 2007), an',\n",
       " 'and bottom-up attention. In (Knudsen 2007), an \\nattention framework is presented based on four \\nfundamental processes: working memory, top-down \\nsensitivity control, competitive selection and \\nbottom-up filtering for salient stimuli. The first three \\nprocesses work in a recurrent loop to implement top -\\ndown control attention. Working memory is \\nintimately linked to attention as its contents are \\ndetermined by attention. This framework seems to \\ncapture most of the essential components of',\n",
       " 'capture most of the essential components of \\nattention and is a promising candidate for inspirat ion \\nwith regards to attention for AI. \\n \\n3 ATTENTION AND AI \\nLet us now consider how the previous chapter can \\ninspire implementation of attentional capabilities for \\nAI systems. As suggested in the introduction, we \\nspecifically target general AI systems designed to \\noperate in complex environments under real-time \\nconstraints with limited resources. These systems are',\n",
       " 'expected to perform various tasks while being \\nreactive to events in the environment, a requiremen t \\nthat  maps  neatly to the  top-down  and  bottom-up  a \\nworkings  of  attention  mentioned  earlier. Both of \\n \\nFigure 1: The Knudsen attention framework (from \\nKnudsen 2007). Data flows up from the environment, \\npasses through salience filters (which detect infre quent or \\nimportant stimuli) and activates neural representat ions, \\nwhich encode various types of knowledge. The activa tion',\n",
       " 'of neural representations is also influenced by wor king \\nmemory via the sensitivity control of top-down atte ntion \\nthat adjusts activation thresholds of individual \\nrepresentations. Representations compete for access to \\nworking memory with only the most active ones being  \\nadmitted. Gaze is controlled by working memory and the \\nselection process. \\nthese are necessary for a complete system; those th at \\nimplement only top-down down attention will',\n",
       " 'implement only top-down down attention will \\ncontinue to work on tasks without being able to rea ct \\nto unexpected or novel events in the environment – \\nevents that may be relevant to the current task or \\nnecessary triggers for generation of new ones. \\nConversely, systems implementing only bottom-up \\nattention cannot perform tasks beyond those that ar e \\nsimple and reactive; tasks consisting of multiple \\nsteps are not possible. However, when these two',\n",
       " 'steps are not possible. However, when these two \\ntypes of attention are properly combined, the resul t \\nis a flexible system capable of performing complex \\ntasks while being faced with interruptions and \\nunexpected events. Part of the role of attention \\ntherefore, is to manage the balance between these \\ntwo at every point in time. \\nThe early vs. late selection debate mentioned in \\nthe previous chapter is also relevant here. It is \\npossible to implement attention mechanisms for AI',\n",
       " 'systems that perform selection early in the sensory',\n",
       " 'pipeline based on primitive features of the data. T his \\napproach is adopted in some of the best known \\nexisting cognitive architectures, such as SOAR \\n(Laird 2008), where attention is viewed as a \\nperceptual process rather than a cognitive one. Ear ly \\nselection unavoidably means that some data is \\n(partly or fully) ignored without being processed f or \\nmeaning; ignoring data that is not understood by th e \\nsystem introduces considerable risk as its relevanc e',\n",
       " 'for the system is not known. This may be acceptable  \\nfor narrow AI systems designed for specific tasks i n \\nspecific environments as it may be possible to crea te \\nshortcuts to understand the nature of incoming \\ninformation in such cases. However, for general AI \\n(AGI) systems designed for tasks and environments \\nnot specified at implementation time, this is highl y \\nproblematic. Early stages of the sensory pipeline c an \\ncontribute to attention in useful ways, such as',\n",
       " 'contribute to attention in useful ways, such as \\nperforming biasing as opposed to absolute selection . \\nFor example, such biasing might be based on \\nnovelty or unexpectedness of the data as these \\nproperties may give rough clues to the importance o f \\nthe information without requiring the information t o \\nbe processed for meaning. Furthermore, this is a \\nreasonable way to implement bottom-up attention, as \\nsuggested by the Knudsen model in Figure 1. As',\n",
       " 'suggested by the Knudsen model in Figure 1. As \\nshallow processing at early stages of the sensory \\npipeline seems unlikely to provide a reliable \\nmeasure of the importance of information, the late \\nselection paradigm seems more promising than early \\nselection in terms of AI and attention. \\nTop-down attention may be viewed as a goal-\\ndriven process as it is intimately related to curre nt \\ngoals of the system. For goals to direct top-down',\n",
       " 'attention, their level of specification is critical . In a \\nsystem where goals are fully specified in terms of \\noperation, the goal definition will be extremely \\nuseful in adjusting attention to elements that are \\nrelevant to the goal. A top-down attention \\nmechanism based on pattern matching could \\ngenerate partially specified patterns from goal \\nspecifications and attempt to find matches in senso ry \\ninformation. Predictions and expectations may also',\n",
       " 'be expected to be necessary control input for top-\\ndown attention in systems that explicitly implement  \\npredictive capabilities – and there is good reason to \\nbelieve that this is necessary in order to approach  \\nhuman-level intelligence. In terms of top-down \\nattention, predictions may be treated in virtually the \\nsame fashion as goals (with level of specification \\nbeing equally important as for goals). \\n \\n4 AI ATTENTION: BEYOND THE \\nHUMAN LEVEL \\nAI systems have an interesting advantage over',\n",
       " 'AI systems have an interesting advantage over \\nhuman minds; they are based on software rather than  \\nhardware (“wetware”). While neurons of our brains \\ncan adaptively wire up to encode skills, knowledge \\nand experiences the core mechanisms of these \\nprocesses are fixed. For example, humans cannot \\neasily acquire dramatically better ways of learning  \\nor remembering. This limitation does not apply to \\nsoftware AI systems; their potential for flexibilit y',\n",
       " 'and reconfiguration are only limited by their \\narchitectural design. The same can be said for thei r \\nlevel of introspection; our introspective capabilit ies \\nare greatly limited - we only have a very vague \\nsense of what is going on in our minds. On the othe r \\nhand, there are much weaker limitations on self-\\nobservation in software AI systems, which again are  \\nlimited only by architectural design. \\nA case for flexible architectures capable of \\nautonomous self-reconfiguration is made in',\n",
       " 'autonomous self-reconfiguration is made in \\n(Thórisson 2009). There are limitations on the \\ncomplexity of manually built software systems and it \\nis not unreasonable to assume that more complex \\nsoftware systems than exist today are needed in \\norder to approach human-like AI. If our chances of \\nmanually building such systems are low, having the \\nsystems build themselves (in a sense) from \\nexperience is not an unreasonable line of research. \\nIn order to perform deep levels of introspection',\n",
       " 'In order to perform deep levels of introspection \\nin complex AI systems, attention is equally useful as \\nfor information originating outside the system; the  \\nsum of activity within such a system can be \\nconsidered to be a vast stream of information and \\nsystem resources remain limited. Determining which \\nparts of this stream are worth processing in order to \\nachieve meta-cognitive goals may be considered as \\nthe role of attention, in much the same way as',\n",
       " 'the role of attention, in much the same way as \\nattention operates on environmental information. \\nThe main purpose of introspection is to provide \\ninformation to direct self-reconfiguration of the \\nsystem. For example, an observation that system \\nprocess P fails repeatedly in certain contexts can be \\nused by the system to shut down process P a n d  \\nactivate a different process (which may exist or ne ed \\nto be created/learned, generating a new meta-',\n",
       " 'to be created/learned, generating a new meta-\\ncognitive goal) when such contexts occur in the \\nfuture.',\n",
       " '5 AN ATTENTION MECHANISM \\nFOR AI SYSTEMS \\nThis section presents a design of one possible \\nattention mechanism for AI systems which addresses \\nthe concepts related to attention discussed \\npreviously. The implementation and evaluation of \\nthis mechanism is upcoming future work. The \\napproach taken adopts the theoretical and \\nmethodological framework presented in Thórisson \\n(2009). \\nAs attention is a ubiquitous cognitive process \\nthat cannot be easily separated from the rest of th e',\n",
       " 'cognitive architecture, some architectural \\nrequirements are unavoidable when tackling the \\ndesign of an AI attention mechanism. The attention \\nmechanism proposed here rests on the requirements \\nthat the underlying cognitive architecture has the \\nfollowing properties: \\n \\n\\uf001 Data-driven. All processing occurs in \\nreaction to data. Processes are activated only \\nwhen paired with compatible input data \\n(fitting the input data specification of the \\nprocess). Absence of fixed control loops allow',\n",
       " 'process). Absence of fixed control loops allow \\nfor greater flexibility and operation on \\nmultiple time scales. \\n\\uf001 Fine-grained. Processing and data elements \\nof the architecture are numerous and small. \\nComplex tasks require collaboration of many \\nsuch elements. Reasoning about small, simple \\ncomponents and their effects on the system is \\nmore practical than attempting to do so for \\nlarger components. \\n\\uf001 Predictive capabilities.  G e n e r a t e  p r e d i c t i o n s',\n",
       " 'with regards to expected events. Expectations \\nare part of the control data of the attention \\nmechanism. \\n\\uf001 Unified sensory pipeline. Data from the \\nenvironment and from within the system are \\ntreated equally. Enables systems to sense their \\nown operation and potentially allows \\ncognitive functions to be applied equally to \\ntask performance in the environment as well \\nas meta-cognitive processing (e.g. self-\\nreconfiguration). \\n \\nThe proposed attention mechanism implements',\n",
       " 'The proposed attention mechanism implements \\nboth top-down and bottom-up attention. Top-down \\nattention is based on goals and predictions, which \\nserve as the basis for generation of so called \\nattentional templates (AT), which are patterns that \\ntarget data to various levels of specification. An AT \\ncan target general data (such as all data from a si ngle \\nmodality, e.g. auditory) or more specific data such as \\nanything directly related to an object or location in',\n",
       " 'the environment and everything in between. As the \\narchitecture implements a unified sensory pipeline , \\nsensory data and internal data are targeted in an \\nidentical fashion by attention. When a data object \\nmatches an active AT, it becomes a candidate to \\nserve as input to a process for which it is compati ble \\nas input. Data objects that do not match any active  \\nAT are not caught by top-down attention and cannot \\ntrigger processing (unless caught by bottom-up',\n",
       " 'trigger processing (unless caught by bottom-up \\nattention). Each AT is created with an associated \\npriority value, which is used when a match occurs \\nwith data, where the matching data item is assigned  \\nthe same value. This value initially comes from the  \\ngoal or prediction used to generate the AT. The \\nassignment of priority values to data upon a match \\nwith an AT is called biasing. Available resources o f \\nthe system are allocated to data items in order of',\n",
       " 'their priority; data items with high priority value s \\n(greatest bias) will have better chances of receivi ng \\nprocessing than those with lower values. \\nBottom-up attention is implemented by primitive \\ndata selection principles that attempt to quantify the \\nnovelty and unexpectedness of input data based on \\ncontent, temporal factors and operational experience. \\nThe novelty of data is based on how similar it is t o \\ndata the system has previously seen, with higher',\n",
       " 'data the system has previously seen, with higher \\nnovelty values being assigned to data that is \\ndifferent from previously seen data. Time also play s \\na role as data that has not been seen recently (but  is \\nnot completely new to the system) will receive \\nhigher novelty values than those that have occurred  \\nrecently. For example, if the environment has been \\nsilent for a while and sound is suddenly heard, \\nauditory data is considered novel and would be',\n",
       " 'auditory data is considered novel and would be \\ncaught by bottom-up attention. If the sound persist s \\nfor some period of time, auditory data will cease t o \\nbe novel and require top-down attention in order to  \\nbe processed. In this way, the bottom-up part of th e \\nattention mechanism implements habituation. \\nFinally, a special mapping process is responsible \\nfor ensuring processes capable of consuming data \\ncaught by attention will be in active states. As th e',\n",
       " 'system is expected to contain numerous processes \\nand data objects at any given time, attempting to \\nmatch every data object to every process to \\ndetermine if an operational match exists is not \\npractically feasible. The data-to-process mapping \\ncomponent can be viewed as an optimization that \\nreduces the number of data/process matched \\nrequired.',\n",
       " 'Figure 2: Overview of the proposed attention mechanism. \\n6 CONCLUSIONS  \\nAs has been shown, mapping models and \\nconcepts from attention in cognitive psychology to \\nAI systems can be useful and straightforward. \\nSurprisingly limited work has been performed on \\nattention in the field of AI given that it is a field with \\nthe ultimate goal of creating human-like intelligen ce \\nand that attention is clearly a critical cognitive \\nprocess for humans. The fact that the human mind',\n",
       " 'process for humans. The fact that the human mind \\nimplements this kind of sophisticated resource \\nmanagement while being orders of magnitude more \\ncomputationally powerful than existing computer \\nhardware today also hints at the importance of \\nattention for AI. \\nFurthermore, attention is likely to be equally \\ncritical for introspective systems such as those th at \\ncan manage their own growth and adapt to \\nexperience at the architecture level. The internals  of',\n",
       " 'the system can be viewed dynamic and complex \\nenvironment in the same way as the task \\nenvironment. With a general and flexible attention \\nmechanism, it may be possible to apply the same \\nattention mechanism for both environments \\nsimultaneously; giving rise to AI systems that \\nperform tasks and i m p r o v e  t h e i r  o w n  p e r f o r m a n c e  \\nwhile being subject to real-time constraints and \\nresource limitations.   \\nACKNOWLEDGEMENTS \\nThis work was supported by the European Project',\n",
       " 'This work was supported by the European Project \\nHUMANOBS – Humanoids that Learn Socio-\\nCommunicative Skills Through Observation (grant \\nnumber 231453). \\nREFERENCES \\nBaars, B. J., Franklin, S. 2009. Consciousness is \\ncomputational: The LIDA model of Global Workspace \\nTheory. International Journal of Machine \\nConsciousness, 2009, 1(1): p. 23-32. \\n \\nBroadbent, D. E. 1958. Perception and Communication.  \\nLondon: Pergamon. \\n \\nCherry, E. C. 1953. Some experiments on the recogniti on',\n",
       " 'of speech, with one and two ears. Journal of the \\nAcoustical Society of America. Pages 975-979. \\n \\nKnudsen, E. I. 2007. Fundamental components of \\nattention. Annu Rev Neurosci, volume 30. Pages 57-78. \\n \\nJames, W. 1890. The Principles of Psychology. New York: \\nHenry Holt, Vol.1, pages 403-404. \\n \\nNorman, D. A. 1969. Memory while shadowing. \\nQuarterly Journal of Experimental Psychology,  v o l .  \\n21, pages 85-93. \\n \\nNovianto, R., Williams, M.-A. 2009. The Role of',\n",
       " 'Novianto, R., Williams, M.-A. 2009. The Role of \\nAttention in Robot Self-Awareness, The 18 th \\nInternational Symposium on Robot and Human \\nInteractive Communication. Pages 1047-1053. \\n \\nLaird, J. E. 2008. Extending the SOAR cognitive \\narchitecture. In Proceedings of the artificial general \\nintelligence conference. Memphis. TN: IOS Press. \\n \\nPhillips, J. L. 2005. A biologically inspired worki ng \\nmemory framework for robots. Proc. 27 th A nn.  C onf .',\n",
       " 'Congitive Science Society. Pages 1750-1755. \\n \\nSkubic, M., Noelle, D., Wilkes, M., Kawamura, K., \\nKeller, J.M. 2004. A biologically inspired adaptive  \\nworking memory for robots. AAAI Fall Symp., \\nWorkshop on the Intersection of Cognitive Science and \\nRobotics. Washington D.C. 2004. \\n \\nSun, R. 2006. The CLARION cognitive architecture: \\nExtending cognitive modelling to social simulation.  \\nIn: Ron Sun (ed.), Cognition and Multi-Agent \\nInteraction. Cambridge University Press, New York.',\n",
       " 'Thórisson, K. R. 2009. From Constructionist to \\nConstructivist A.I. Keynote, Technical Report, FS-90-\\n01, AAAI press, Menlo Park, California. \\n \\nWang, P. 1995. Non-Axiomatic Reasoning System: \\nExploring the Essence of Intelligence. Ph.D. diss., Dept. \\nof Computer Science, Indiana Univ., CITY, Indiana.',\n",
       " 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/390598346\\nVector Embeddings Unveiled: A Comprehensive Exploration of Their Creation,\\nTypes, Applications, Challenges, and Future Directions in Machine Learning\\nResearch · April 2025\\nDOI: 10.13140/RG.2.2.15544.05129\\nCITATIONS\\n0\\nREADS\\n381\\n1 author:\\nPaul Pajo\\nDe La Salle-College of Saint Benilde\\n109 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE',\n",
       " '109 PUBLICATIONS\\xa0\\xa0\\xa010 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Paul Pajo on 09 April 2025.\\nThe user has requested enhancement of the downloaded file.',\n",
       " 'Vector Embeddings Unveiled: A Comprehensive Exploration of\\nTheir Creation, Types, Applications, Challenges, and Future\\nDirections in Machine Learning\\nby Paul Pajo∗\\nApril 9, 2025\\nAbstract\\nVector embeddings, numerical representations of complex data such as text, images,\\nand audio, have become foundational in machine learning by encoding semantic relation-\\nships in high-dimensional spaces. This paper provides a thorough examination of their',\n",
       " 'creation via neural networks (e.g., Word2Vec, BERT, CLIP), categorization into word, sen-\\ntence, document, image, audio, and multimodal types, and diverse applications including\\nsemantic search, recommendation systems, and generative AI. We analyze persistent chal-\\nlenges—high dimensionality, interpretability, and scalability—and recent advancements like\\ncontextual embeddings, vector databases, and multimodal integration, supported by empiri-',\n",
       " 'cal evidence and theoretical insights. Our findings highlight embeddings’ transformative role\\nin AI, with static models like Word2Vec offering efficiency and contextual models like BERT\\nenhancing semantic precision, though at increased computational cost. We conclude that\\nvector embeddings bridge human-like understanding and machine processing, with future\\nresearch poised to address efficiency, bias mitigation, and cross-modal generalization. This',\n",
       " 'work synthesizes current knowledge and charts a path for advancing embedding technologies.\\n1 Introduction\\nVector embeddings represent a cornerstone of modern artificial intelligence (AI), enabling ma-\\nchines to process and understand unstructured data—such as text, images, or sounds—by con-\\nverting them into numerical vectors. These vectors, situated in a high-dimensional space, posi-\\ntion similar items closer together, reflecting their semantic or relational proximity. For example,',\n",
       " 'in natural language processing (NLP), the words ”king” and ”queen” might occupy nearby coor-\\ndinates, capturing their shared royal context [13]. This intuitive yet powerful concept underpins\\napplications ranging from search engines to generative models like DALL-E.\\nFor those new to the field, consider embeddings as a translation mechanism: just as a\\ndictionary translates words between languages, embeddings translate diverse data into a format',\n",
       " 'machines can analyze. This translation preserves meaning, allowing AI to perform tasks like\\nfinding similar documents or recommending movies. However, creating and utilizing these\\nembeddings involves complex processes, diverse methodologies, and significant challenges.\\nThis paper aims to demystify vector embeddings by exploring their creation, types, ap-\\nplications, challenges, and advancements. We provide a structured analysis for both novices',\n",
       " 'and experts, bolstered by citations to seminal works and recent studies. We also propose fu-\\nture research directions to address unresolved issues, ensuring a comprehensive resource for\\nunderstanding this pivotal technology.\\n∗thanks Grok(xAI) frompaulamerigo.pajojr@benilde.edu.ph\\n1',\n",
       " '2 Creation of Vector Embeddings\\nVector embeddings are generated through a multi-step process involving neural networks trained\\non large datasets:\\n1. Data Collection : A representative dataset is amassed, such as a text corpus (e.g.,\\nWikipedia) for NLP or an image set (e.g., ImageNet) for vision tasks [3].\\n2. Preprocessing: Data is cleaned—text tokenized into words, images resized—to ensure\\nuniformity [9].\\n3. Model Training: Neural networks map data to vectors. Word2Vec predicts word con-',\n",
       " 'texts [13], BERT uses transformers for contextual understanding [4], and CNNs extract\\nimage features [10].\\n4. Embedding Generation: Trained models produce vectors for new inputs, preserving\\nlearned relationships.\\nModels like Word2Vec (trained on 100 billion words [14]) and CLIP (400 million image-\\ncaption pairs [18]) exemplify this process, balancing efficiency and semantic depth.\\n3 Types of Vector Embeddings',\n",
       " '3 Types of Vector Embeddings\\nEmbeddings vary by data type and granularity: - Word Embeddings: Represent words (e.g.,\\nWord2Vec, 300 dimensions [13]; GloVe [15]). - Sentence Embeddings : Capture sentence\\nmeaning (e.g., Sentence-BERT [20]). - Document Embeddings: Encode entire texts (e.g.,\\nDoc2Vec [11]). - Image Embeddings : Represent visual content (e.g., CNNs [6]). - Au-\\ndio Embeddings: Model sound features (e.g., VGGish [7]). - Multimodal Embeddings :\\nCombine data types (e.g., CLIP [18]).',\n",
       " 'Combine data types (e.g., CLIP [18]).\\nEach type addresses specific needs, from fine-grained word analysis to cross-modal tasks.\\n4 Applications\\nEmbeddings enable diverse applications: - Semantic Search: Match queries to content by\\nmeaning [17]. - Recommendation Systems: Suggest items via vector proximity [21]. - NLP\\nTasks: Enhance translation, sentiment analysis [9]. - Generative AI: Power text-to-image\\nmodels [19]. - Multimodal Tasks: Enable image captioning [18].',\n",
       " 'These applications demonstrate embeddings’ versatility across industries.\\n5 Challenges\\nKey challenges include: - High Dimensionality: Increases computational cost [1]. - Inter-\\npretability: Dimensions lack clear meaning [12]. - Contextual Limitations: Static embed-\\ndings miss nuance [16]. - Scalability: Large datasets strain storage [22].\\n6 Advancements\\nRecent innovations address these issues: - Contextual Embeddings: BERT improves pol-',\n",
       " 'ysemy handling [4]. - Vector Databases: Pinecone, Weaviate enhance scalability [17, 22].\\n- Multimodal Models : CLIP integrates data types [18]. - Compression: Quantization\\nreduces resource use [5].\\n2',\n",
       " '7 Analysis\\nStatic embeddings (e.g., Word2Vec) offer efficiency for resource-constrained systems, with train-\\ning times in hours, while contextual models (e.g., BERT) demand days but excel in accuracy\\n[4, 13]. Multimodal embeddings like CLIP suggest a convergence of data modalities, though\\ntheir 512-dimensional vectors require optimization [18]. Vector databases mitigate scalability,\\nsupporting real-time applications [8].\\n8 Conclusion',\n",
       " '8 Conclusion\\nVector embeddings are a linchpin of AI, translating complex data into actionable represen-\\ntations. Their evolution from static to contextual and multimodal forms reflects a trade-off\\nbetween efficiency and richness, with infrastructure like vector databases ensuring practical\\ndeployment. Future research should focus on: - Efficiency: Optimizing contextual models\\nfor edge devices. - Bias Mitigation : Addressing training data biases [2]. - Cross-Modal',\n",
       " 'Generalization: Enhancing multimodal robustness.\\nEmbeddings will continue shaping AI’s ability to mirror human understanding.\\nReferences\\n[1] Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural prob-\\nabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.\\n[2] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.\\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings.',\n",
       " 'Advances in Neural Information Processing Systems, 29, 2016.\\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\\nscale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 248–255, 2009.\\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\\narXiv:1810.04805, 2018.',\n",
       " 'arXiv:1810.04805, 2018.\\n[5] Robert M Gray. Quantization and Data Compression. Springer, 2011.\\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\\nimage recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[7] Shawn Hershey, Sourish Chaudhuri, Daniel P W Ellis, Jort F Gemmeke, Aren Jansen,\\nR Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, et al. Cnn architectures for',\n",
       " 'large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pages 131–135, 2017.\\n[8] Jeff Johnson, Matthijs Douze, and Herv´ e J´ egou. Billion-scale similarity search with gpus.\\nIEEE Transactions on Big Data, 7(3):535–547, 2019.\\n[9] Daniel Jurafsky and James H Martin. Speech and Language Processing. Pearson, 2009.\\n[10] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep',\n",
       " 'convolutional neural networks. Advances in Neural Information Processing Systems, 25,\\n2012.\\n3',\n",
       " '[11] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents.\\nInternational Conference on Machine Learning, pages 1188–1196, 2014.\\n[12] Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018.\\n[13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed',\n",
       " 'representations of words and phrases and their compositionality. Advances in Neural In-\\nformation Processing Systems, 26, 2013.\\n[15] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for\\nword representation. Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 1532–1543, 2014.\\n[16] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton',\n",
       " 'Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\\narXiv:1802.05365, 2018.\\n[17] Pinecone. What are vector embeddings. https://www.pinecone.io/learn/\\nvector-embeddings/, 2023.\\n[18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning trans-\\nferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020,\\n2021.',\n",
       " '2021.\\n[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint\\narXiv:2102.12092, 2021.\\n[20] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese\\nbert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[21] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B Kantor, editors. Recommender\\nSystems Handbook. Springer, 2011.',\n",
       " 'Systems Handbook. Springer, 2011.\\n[22] Weaviate. Vector embeddings explained. https://weaviate.io/blog/\\nvector-embeddings-explained, 2023.\\n4\\nView publication stats',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The',\n",
       " 'data transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-',\n",
       " 'ing initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for',\n",
       " 'advancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-',\n",
       " 'tor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding',\n",
       " '2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In',\n",
       " '2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-',\n",
       " 'strict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-',\n",
       " 'namic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully',\n",
       " 'designed loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-',\n",
       " 'tion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance',\n",
       " 'of up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-',\n",
       " 'Instruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-',\n",
       " 'formation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative',\n",
       " 'sampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both',\n",
       " 'CMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods',\n",
       " 'signiﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation',\n",
       " '(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-',\n",
       " 'lied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-',\n",
       " 'nAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[',\n",
       " 'work like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving',\n",
       " 'performance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-',\n",
       " 'modal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-',\n",
       " 'embedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering',\n",
       " '41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,',\n",
       " 'retrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-',\n",
       " 'Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented',\n",
       " 'a dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model',\n",
       " 'to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-',\n",
       " 'cessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-',\n",
       " 'ing data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions',\n",
       " 'for each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-',\n",
       " 'chitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following',\n",
       " 'modiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement',\n",
       " 'with data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of',\n",
       " 'title-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed',\n",
       " 'by extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume',\n",
       " 'and diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,',\n",
       " 'encompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample',\n",
       " 'pairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,',\n",
       " '4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities',\n",
       " 'in reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-',\n",
       " 'ios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to',\n",
       " 'to enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a',\n",
       " 'negative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent',\n",
       " 'loss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[',\n",
       " 'this issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous',\n",
       " 'penalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni',\n",
       " 'j,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)',\n",
       " 'have signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative',\n",
       " 'examples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must',\n",
       " 'accurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.',\n",
       " 'For each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible',\n",
       " 'improvements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings',\n",
       " '4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.',\n",
       " 'illustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-',\n",
       " 'guity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-',\n",
       " 'level partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt',\n",
       " 'the data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-',\n",
       " 'oriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.',\n",
       " 'cantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,',\n",
       " 'MSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then',\n",
       " 'scaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned',\n",
       " 'datasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update',\n",
       " 'harder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view',\n",
       " '5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],',\n",
       " 'PAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-',\n",
       " 'proximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10',\n",
       " 'to top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-',\n",
       " 'tions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use',\n",
       " 'all training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we',\n",
       " 'employ bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR',\n",
       " '9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.',\n",
       " 'are provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[',\n",
       " 'ative models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results',\n",
       " 'and piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding',\n",
       " 'secured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1',\n",
       " 'Seed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65',\n",
       " 'QZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99',\n",
       " 'xiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion',\n",
       " '7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data',\n",
       " 'synthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB',\n",
       " 'benchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences',\n",
       " 'References\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.',\n",
       " 'preprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-',\n",
       " 'trastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.',\n",
       " '[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders',\n",
       " 'are generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM',\n",
       " 'SIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are',\n",
       " 'secretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).',\n",
       " '(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,',\n",
       " 'Junaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt',\n",
       " 'Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in',\n",
       " 'information retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-',\n",
       " 'mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.',\n",
       " 'arXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,',\n",
       " 'Amanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-',\n",
       " 'han Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.',\n",
       " '”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-',\n",
       " 'colo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.',\n",
       " 'Computational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.',\n",
       " 'In Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).',\n",
       " 'arXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models',\n",
       " 'with eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min',\n",
       " 'Zhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-',\n",
       " 'trieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis',\n",
       " 'platform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,',\n",
       " 'Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First',\n",
       " 'Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.',\n",
       " '1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).',\n",
       " '[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and',\n",
       " 'Michael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference',\n",
       " 'on Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions',\n",
       " 'of the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.',\n",
       " 'arXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.',\n",
       " 'Linguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.',\n",
       " '[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation',\n",
       " 'Item Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.',\n",
       " 'original sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while',\n",
       " 'containing exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)',\n",
       " '2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses',\n",
       " 'Yelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.',\n",
       " 'can apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and',\n",
       " 'poetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-',\n",
       " 'land boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.',\n",
       " 'William Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-',\n",
       " 'century. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they',\n",
       " 'cessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-',\n",
       " 'help build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-',\n",
       " 'the lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English',\n",
       " '’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential',\n",
       " 'King Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential',\n",
       " 'King Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27',\n",
       " 'See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/376885449\\nArtiﬁcial Intelligence for Object Detection and its Metadata\\nArticle · December 2023\\nDOI: 10.17605/OSF.IO/FG3SQ\\nCITATIONS\\n5\\nREADS\\n2,055\\n1 author:\\nNarayana Challa\\nJawaharlal Nehru Technological University, Hyderabad\\n26 PUBLICATIONS\\xa0\\xa0\\xa049 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Narayana Challa on 04 February 2024.',\n",
       " 'The user has requested enhancement of the downloaded file.',\n",
       " 'https://iaeme.com/Home/journal/IJAIML 121 editor@iaeme.com \\nInternational Journal of Artificial Intelligence & Machine Learning (IJAIML)  \\nVolume 2, Issue 01, Jan-Dec 2023, pp. 121-133. Article ID: IJAIML_02_01_012 \\nAvailable online at https://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1 \\nJournal ID: 9339-1263, https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\n© IAEME Publication \\nARTIFICIAL INTELLIGENCE FOR OBJECT \\nDETECTION AND ITS METADATA \\nNarayana Challa',\n",
       " \"DETECTION AND ITS METADATA \\nNarayana Challa \\nDirector of ERP Strategy at Cabinetworks Group, Texas, USA \\nABSTRACT \\nIn the ever-evolving field of computer vision, the infusion of artificial intelligence \\n(AI) has inaugurated a revolutionary era, providing unparalleled precision and \\nefficiency in identifying objects within images and videos. This exploration delves into \\nthe domain of AI -driven object detection, emphasizing metadata's pivotal role in\",\n",
       " 'enhancing the understanding and utility of recognized entities. The collaboration \\nbetween AI and metadata enhances the precision of object detection and opens up novel \\navenues for extracting and analyzing information. \\nThe significance of metadata is underscored as it contributes context and \\ncategorization to identified objects. This metadata encompasses crucial details such as \\nobject class, detection location, time of occurrence, and inter -object relationships,',\n",
       " 'furnishing invaluable insights for downstream applications like autonomous vehicles, \\nsurveillance, and augmented reality. The research paper showcases the seamless \\nintegration of metadata extraction and management with AI -powered object detection \\nsystems, thereby boosting the accuracy of object identification and tracking. \\nThis study sheds light on the intricate interplay between artificial intelligence and',\n",
       " 'computer vision, molding a landscape where precision and adaptability redefine the \\nboundaries of object detection capabilities. \\nKeywords: Artificial Intelligence (AI), Machine Learning (ML), Internet of Things \\n(IOT) Data Science, Data Analysis, Data Processing, Data Presentations, and Data \\nScience careers \\n \\nCite this Article: Narayana Challa, Artificial Intelligence for Object Detection and Its \\nMetadata, International Journal of Artificial Intelligence & Machine Learning',\n",
       " '(IJAIML), 2(1), 2023, pp. 121-133. \\nhttps://iaeme.com/Home/issue/IJAIML?Volume=2&Issue=1',\n",
       " 'Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 122 editor@iaeme.com \\nINTRODUCTION \\nArtificial Intelligence (AI) has significantly advanced in computer vision, particularly object \\ndetection. This paper explores how AI has transformed object detection, emphasizing the \\ncrucial role of metadata in enhancing its capabilities. \\nIn the digital era, the abundance of visual data, from surveillance to medical imaging and',\n",
       " 'self-driving cars, necessitates accurate object detection. AI, especially with deep learning \\nmodels like convolutional neural networks (CNNs), has become a robust solution, overcoming \\nchallenges such as occlusion and scale variations with increasing accuracy. \\nAI, especially utilizing CNNs, has revolutionized object detection in various sectors dealing \\nwith vast visual data. These models excel in acquiring hierarchical feature representations,',\n",
       " 'adapting to the dynamic nature of visual input, and continuously improving comprehension. \\nObject detection faces challenges like occlusion and background clutter. Through \\ncontinuous learning and sophisticated algorithms, AI techniques adeptly address these \\nchallenges. The adaptive nature of AI allows the development of nuanced models for accura te \\ndetection in complex environments. \\nWhile AI enhances object detection, metadata is crucial, providing additional information',\n",
       " 'such as object class, position, detection time, and associations. Integrating metadata with AI -\\ndriven systems transforms the comprehension and utility of identified objects. \\nMetadata goes beyond visual identification, adding context to detected objects. \\nUnderstanding object class, location, and temporal occurrence enhances visual data \\ninterpretation, contributing to richer information. In downstream applications like augmented',\n",
       " 'reality and surveillance, metadata becomes invaluable, aiding in informed decision-making. \\nThe study demonstrates the seamless integration of AI object identification systems with \\nmetadata extraction. This integration enhances object tracking and recognition accuracy, paving \\nthe way for sophisticated decision-making across various applications. \\n \\nConsequences of AI-Powered Object Identification and Integration with \\nMetadata',\n",
       " 'Metadata \\nIntegrating AI with metadata for object detection has far -reaching consequences, extending \\nbeyond a single domain. This synergy can revolutionize various industries, enhancing security \\nmeasures and industrial automation capabilities. \\nThe central focus of this paper is the crucial integration of metadata with AI object detection \\nsystems, representing a defining aspect of the exploration. Metadata, containing contextual',\n",
       " 'information about detected objects, becomes pivotal for unlocking the full potential of object \\ndetection. It includes critical details like object class, detection location, time, and inter -object \\nrelationships. This symbiosis between AI and metadata refines object identifi cation and \\ntracking and finds applications in sectors like autonomous vehicles, surveillance, and \\naugmented reality.',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 123 editor@iaeme.com \\nThe implications of the collaboration between AI and metadata extend into heightened \\nsecurity protocols, increased efficiency in industrial automation, and a deeper understanding of \\nthe visual environment. Ethical and privacy concerns related to AI-driven object detection and \\nmetadata are also addressed in this paper. This introduction sets the stage for a comprehensive',\n",
       " 'exploration of AI in object detection and its synergistic interplay with metadata, offering \\ninsights into the transformative potential of t hese technologies. It provides a glimpse into a \\nfuture where intelligent object detection systems redefine our perception and interaction with \\nthe visual world. \\nObject detection in computer vision involves locating and identifying items in images or \\nvideo frames, going beyond mere identification to accurately define their locations using',\n",
       " 'bounding boxes. This technical advancement is fundamental to applications lik e robots, \\naugmented reality, driverless cars, and surveillance systems, shaping how machines perceive \\nand interact with their visual surroundings. \\nObject detection commonly involves the following fundamental elements: \\nIdentifying the position and dimensions of objects within an image. This is often visualized by \\noutlining objects with bounding boxes, specifying their spatial coordinates (x, y) and',\n",
       " 'dimensions (width and height). \\nAssigning a label or category to each detected object, indicating its type. For instance, in a \\nscene with diverse objects, object detection can discern whether there are cars, pedestrians, \\nanimals, or other entities and assign corresponding labels. \\nDeep learning models, such as convolutional neural networks (CNNs), are frequently \\nemployed for object detection. These models are designed to handle both localization and',\n",
       " 'classification tasks concurrently. Through training on extensive datasets with label ed images, \\nthey can recognize objects and accurately determine their positions. \\nEffectively training object detection models necessitates large datasets with annotated \\nimages. These datasets typically include images where objects of interest are labeled with \\nbounding boxes and associated class labels. During training, models learn from these examples',\n",
       " 'to precisely predict object positions and classes in new, unseen images. \\nWhat makes object detection crucial? \\nObject detection holds significant importance in computer vision for various reasons. The key \\nrationales for its significance include: \\n1. Visual Scene Content Interpretation: \\nObject detection aids in interpreting the content of visual scenes. It enables computers to \\nidentify and locate objects, providing context for more sophisticated processing. Object',\n",
       " 'detection is crucial in industrial settings for monitoring product movement, automating \\nprocesses, and ensuring quality control. Additionally, it promotes greater independence for \\nindividuals with visual impairments by recognizing and characterizing object s and their \\nlocations. \\n2. Obstacle Avoidance and Navigation: \\nObject detection is essential for identifying and avoiding obstacles, navigating surroundings,',\n",
       " 'and making decisions based on the presence of objects. This is particularly crucial in \\nautonomous vehicles, drones, and robotics applications. For security and surveillance systems, \\nobject detection enables real-time identification and monitoring of items and people, enhancing \\nthreat detection and response. In retail, it automates checkout procedures, tracks products on \\nshelves, and manages inventory for improved productivity and consumer satisfaction.',\n",
       " 'Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 124 editor@iaeme.com \\n3. Medical Imaging and Diagnosis: \\nIn medical imaging, object detection aids in locating and identifying specific structures, \\ncontributing to diagnosis and therapy. It ensures precision in identifying objects within images, \\nfacilitating better healthcare outcomes. \\n4. Interactive Experiences and Entertainment:',\n",
       " '4. Interactive Experiences and Entertainment: \\nObject detection follows user movements and gestures in interactive applications and games, \\ncreating immersive experiences. It automatically tags and categorizes films and images, \\nsimplifying the search and organization of extensive media collections. \\n5. Automation and Speeding Up Processes: \\nObject detection automates tasks across various industries, reducing the need for manual',\n",
       " 'involvement and accelerating procedures. Augmented reality (AR) applications recognize real-\\nworld objects through object detection, enhancing user experiences by overl aying digital data \\nor graphics. Combining object detection with natural language processing (NLP) bridges \\ninformation gaps between text and images, facilitating content comprehension. \\n6. Environmental Monitoring and Wildlife Research:',\n",
       " 'Researchers and conservationists benefit from object detection by obtaining crucial data for \\ntracking environmental changes, monitoring wildlife, and studying animal behavior. It plays a \\npivotal role in understanding and preserving ecosystems. \\nObject detection is a versatile and indispensable technology with applications ranging from \\nindustrial automation to healthcare, security, entertainment, and environmental conservation. \\nObject Detection and the Impact of Deep Learning',\n",
       " \"Object Detection and the Impact of Deep Learning \\nThe realms of object detection and deep learning are intricately linked, with deep learning \\ntechniques profoundly shaping the landscape of object detection. The advent of deep learning \\nhas brought about a paradigm shift, enhancing the accuracy and efficien cy of object detection \\nsystems to handle intricate tasks with remarkable precision. Let's delve into the intricate \\nconnection between object detection and deep learning:\",\n",
       " \"CNNs, in particular, are deep learning models that include specialized convolutional layers \\nthat are skilled at extracting spatial information from images. These layers are excellent at \\nrecognizing objects because they can recognize linkages and local patt erns among pixels. \\nModels pre -trained on large datasets (like ImageNet) can be refined for particular item \\nrecognition tasks using relatively small, labeled datasets thanks to deep learning's support for\",\n",
       " 'transfer learning. As a result, less training data is needed for functions involving bespoke object \\ndetection. \\nConvolutional neural networks (CNNs) are deep learning models that show proficiency in \\nautomatically extracting hierarchical features from images. Acquired traits are essential for \\ndifferentiating objects with different sizes, shapes, and orientations. Deep neural networks can \\ncapture high-level features that represent the components and structures of an item and low -',\n",
       " 'level features like edges and textures. Learning The benefit of end -to-end training for object \\nlocalization and classification is provided by deep learning models. This suggests that an \\nindividual model can recognize things and anticipate where they will be found in an image \\ninstead of conventional computer vision techniques that call for distinct, labor-intensive phases \\nfor object recognition and feature extraction.',\n",
       " 'for object recognition and feature extraction. \\nA wide range of object sizes, types, and degrees of complexity may be handled by deep \\nlearning models with versatility. They may adapt to real -world events and are not limited to \\nsimple items or shapes.',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 125 editor@iaeme.com \\nDeep learning models are particularly good at handling partially veiled or occluded objects. \\nTheir capacity to pick up on intricate object appearances makes it easier to identify objects even \\nwhen they are partially obscured. Ongoing research and advanceme nts in deep learning \\ncontribute to the continual enhancement of the accuracy and speed of object detection systems.',\n",
       " 'This ongoing progress renders them suitable for an expanding range of applications. \\nLarge, labeled datasets are essential for the effectiveness of deep learning -based item \\ndetection. Deep learning models may learn from various examples, increasing their accuracy \\nand robustness. This is made possible by the availability of large image datasets that have been \\ntagged with object locations and class labels. Researchers have introduced deep learning',\n",
       " 'architectures specifically designed for object detection. Examples include SSD (Single Shot \\nMultiBox Detector), YOLO (You Only Look Once), and Fast er R -CNN. These designs \\neffectively combine localization and classification to optimize tasks related to object \\nidentification. \\nDeep learning has made real-time object detection possible, allowing for high-speed object \\ntracking and recognition in applications like augmented reality, video surveillance, and \\nautonomous cars. \\nChatGPT',\n",
       " 'autonomous cars. \\nChatGPT \\nThe realms of object detection and deep learning are intricately connected, with deep learning \\ntechniques significantly influencing and enhancing the landscape of object detection. The \\nintroduction of deep learning has led to a paradigm shift, considerably improving the precision \\nand efficiency of object detection systems for handling complex tasks. \\n1. Role of Convolutional Neural Networks (CNNs):',\n",
       " '1. Role of Convolutional Neural Networks (CNNs): \\nCNNs, a deep learning model, feature specialized convolutional layers adept at extracting \\nspatial information from images. These layers excel at recognizing objects by identifying \\nlinkages and local patterns among pixels. Pretrained models, such as those o n ImageNet, can \\nbe fine-tuned for specific object recognition tasks using smaller labeled datasets, showcasing',\n",
       " 'the transfer learning capabilities of deep learning and reducing the need for extensive training \\ndata. \\n2. Hierarchical Feature Extraction: \\nCNNs automatically extract hierarchical features from images, which is crucial for \\ndistinguishing objects with varying sizes, shapes, and orientations. These models capture high-\\nlevel features representing components and low-level features like edges and textures. End-to-',\n",
       " 'end training facilitates object localization and classification within a single model, eliminating \\nthe need for separate, labor-intensive phases in conventional computer vision techniques. \\n3. Versatility and Adaptability: \\nDeep learning models exhibit versatility in handling various object sizes, types, and \\ncomplexities. They can adapt to real -world scenarios and are not limited to simple objects or',\n",
       " 'shapes. Notably, deep learning models excel at handling partially veiled or occluded objects, \\nthanks to their ability to recognize intricate object appearances even when partially obscured. \\nOngoing research contributes to continuous improvements in the accuracy and speed of object \\ndetection systems, expanding their applicability.',\n",
       " 'Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 126 editor@iaeme.com \\n4. Importance of Large, Labeled Datasets: \\nLarge, labeled datasets are essential for the effectiveness of deep learning -based object \\ndetection. These datasets enable models to learn from diverse examples, enhancing their \\naccuracy and robustness. Researchers have introduced specialized deep learning  architectures',\n",
       " 'for object detection, such as SSD, YOLO, and Faster R -CNN, which efficiently combine \\nlocalization and classification to optimize object identification tasks. \\n5. Real-Time Object Detection: \\nDeep learning has enabled real -time object detection, facilitating high -speed object tracking \\nand recognition in applications like augmented reality, video surveillance, and autonomous \\nvehicles. \\nThe marriage of object detection and deep learning, mainly through CNNs and other',\n",
       " 'specialized architectures, has ushered in a transformative era, offering enhanced accuracy, \\nadaptability, and real-time capabilities across diverse applications. Ongoing adva ncements in \\ndeep learning continue to push the boundaries of what is achievable in object detection. \\n \\nHow Object Detection Operates \\nObject detection, a critical computer vision task, involves using algorithms, particularly deep',\n",
       " 'learning models, to identify and locate objects within images or video frames. The operational \\nprocess of object detection can be broken down as follows: \\n1. Data Gathering and Preparation: \\nCollect a dataset comprising images or video frames displaying objects of interest. \\nLabel each image with bounding boxes indicating the object\\'s location and class labels \\nspecifying the object type (e.g., \"car,\" \"person,\" \"cat\").',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 127 editor@iaeme.com \\n2. Selection of Deep Learning Model: \\nFor object detection, utilize deep learning models, especially convolutional neural networks \\n(CNNs). \\nVarious models like SSD, RetinaNet, YOLO, and Faster R -CNN address specific \\napplication, speed, and accuracy concerns. \\n3. Training the Model: \\nTrain the selected deep learning model using the prepared dataset.',\n",
       " 'Adjust internal parameters to minimize the difference between model predictions and \\nground truth (labeled bounding boxes and class labels). \\nOptimize a loss function measuring the discrepancy between predicted and actual object \\npositions and classifications through an iterative training process. \\n4. Inference: \\nAfter training, use the model to identify objects in new, unlabeled images or video frames. \\nDuring inference, analyze the image either as a whole or by segmenting it into smaller',\n",
       " \"regions based on the model architecture. \\n5. Prediction Outputs: \\nGenerate predictions for each region, including: \\nBounding boxes (coordinates) around local objects. \\nClass designations corresponding to the identified objects. \\nConfidence scores indicate the model's confidence in each prediction. \\n6. Post-Processing: \\nAddress repetitive boxes or predictions with low confidence scores using post -processing \\ntechniques.\",\n",
       " 'techniques. \\nNon-maximum suppression (NMS) is one such method to eliminate redundant boxes, \\nretaining the most accurate and confident predictions. \\n7. Final Results: \\nDraw bounding boxes around objects in the image to represent the final object detection results, \\nincluding position and class information. \\n8. Applications: \\nApply object detection findings in various domains, such as augmented reality, safety systems, \\nautonomous vehicle decision-making, and object tracking.',\n",
       " 'Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 128 editor@iaeme.com \\nMost Popular Object Detection Algorithms \\n \\nConvolutional neural networks, such as Mask R -CNN, R -CNN (Region -Based \\nConvolutional Neural Networks), YOLO (You Only Look Once), MobileNet, and SqueezeDet, \\nare frequently used methods for object detection. \\nR-CNN: \\nThe Region -based Convolutional Neural Network (R -CNN) is a computer vision model',\n",
       " 'designed for object recognition and image detection. Here\\'s a breakdown of its functionality: \\n1. Region Proposal: \\nR-CNN begins by identifying potential objects in an image using a \" selective search method,\" \\nsuggesting areas likely to contain objects. \\n2. Convolutional Neural Network (CNN) Analysis: \\nR-CNN utilizes a CNN for each suggested region to analyze and understand the content, \\ncreating a descriptive map of the area. \\n3. Support Vector Machine (SVM):',\n",
       " '3. Support Vector Machine (SVM): \\nR-CNN applies a support vector machine to identify the objects present in the regions. It \\ncategorizes and recognizes objects using labels. \\n4. Bounding Box Refinement: \\nThe model refines the bounding boxes surrounding the recognized objects, adjusting them to \\nmatch the shapes of the objects better. \\n5. Result Selection: \\nR-CNN selects the most accurate and confident outcomes, eliminating less trustworthy ones to',\n",
       " 'prevent displaying excessively similar findings. \\n6. Drawbacks and Enhancements: \\nR-CNN, while a breakthrough, was slow and complex for object identification. Later models \\nlike Fast R -CNN and Faster R -CNN addressed these issues, introducing improvements for \\nincreased accuracy and speed in object detection.',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 129 editor@iaeme.com \\nMask R-CNN: \\n• An extension of Faster R -CNN, Mask R -CNN, introduced in 2017, handles instance \\nsegmentation, providing pixel -level masks for each unique instance of an object in \\naddition to object detection. \\n• Mask R-CNN utilizes a CNN to extract features, refines bounding boxes for precise \\nobject fitting, and creates binary masks to identify pixel-level object instances.',\n",
       " '• The model groups identified items into various types or categories and employed non -\\nmaximum suppression to remove duplicate or overlapping results after object detection, \\nmask creation, and classification. \\n• Applications for Mask R -CNN are found in domains requiring instance -level \\nsegmentation and pixel -level precision, such as robotics, computer vision tasks, and \\nmedical imaging. \\n \\nDeep learning-based object identification models are crucial in recognizing items in images',\n",
       " 'or video frames and delineating them with bounding boxes. After object detection, metadata \\nmodels come into play, focusing on obtaining additional data or characteristics associated with \\nthe detected objects. The extracted metadata is linked to each bounding box, providing precise \\nattribute descriptions for each recognized object. Here are some common metadata attributes: \\n• Object Class: Identifying the type of object (e.g., \"car,\" \"person,\" \"dog\").',\n",
       " \"• Color: Specifying the object's color or color patterns. \\n• Size: Determining the object's actual dimensions. \\n• Orientation: Identifying the object's orientation or position. \\n• Velocity: Determining the motion or speed of the object. \\n• Timestamp: Documenting the moment the object was detected. \\n• Context: Recognizing how an object fits into its surroundings or connects to other \\nobjects.\",\n",
       " 'Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 130 editor@iaeme.com \\nCombining spatial information from the bounding box with associated metadata enables \\npinpointing specific characteristics of individual objects. For instance, metadata might describe \\na detected car as \"red,\" \"sedan,\" and \"parked.\" \\nApplications for combined object detection and metadata information span various fields:',\n",
       " '• Traffic Monitoring: Identifying vehicle types, colors, and speeds for traffic studies. \\n• Security and Surveillance: Sorting individuals and items based on potential threats. \\n• Autonomous Vehicles: Recognizing characteristics and motion of nearby automobiles \\nand people. \\n• Retail Inventory: Monitoring goods on store shelves, including locations and \\nattributes. \\n• Augmented Reality: Overlaying digital data based on identified items and their \\ncharacteristics.',\n",
       " 'characteristics. \\n• Medical Imaging: Analyzing and recording characteristics of anatomical structures or \\nanomalies. \\nCombining object detection and metadata models provides a comprehensive and contextual \\nunderstanding of objects within an image or video frame. This information enhances the utility \\nand functionality of computer vision systems, enabling improved decision -making, analysis, \\nand automation across diverse fields. \\nYOLO (You Only Look Once):',\n",
       " 'YOLO (You Only Look Once): \\nYOLO (You Only Look Once) is a real -time object detection system widely popular in \\ncomputer vision and deep learning. Introduced by Joseph Redmon and Santosh Divvala in a \\nseries of papers, the original paper titled \"You Only Look Once: Unified, Real -Time Object \\nDetection\" was published in 2016. YOLO is designed for instantaneous object detection in \\nvideo and image streams, quickly locating and identifying multiple objects in a single neural \\nnetwork pass.',\n",
       " 'network pass. \\nKey features and aspects of YOLO include: \\n1. Unified Approach: \\n• YOLO takes a unique approach by splitting the image into a grid and predicting \\nbounding boxes and class probabilities for every grid cell. Unlike region -based object \\nidentification techniques like R-CNN, YOLO analyzes the entire image in a single pass. \\n2. Single Pass Analysis: \\n• YOLO effectively analyzes the complete picture in one pass, providing predictions for',\n",
       " 'every object simultaneously. This architecture significantly improves speed compared \\nto traditional techniques that require multiple network passes. \\n3. Bounding Box Prediction: \\n• YOLO predicts bounding boxes that define the locations of objects for each grid cell. \\nConfidence scores indicate the accuracy of predictions. Class probability predictions are \\nalso used to identify the object type in each bounding box.',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 131 editor@iaeme.com \\n4. Non-Maximum Suppression: \\n• YOLO uses non -maximum suppression to eliminate redundant bounding boxes, \\nretaining the most reliable and accurate ones. This post-prediction step is performed for \\nevery grid cell. \\n5. Iterative Improvements: \\n• YOLO has undergone several revisions, including YOLOv1, YOLOv2 (YOLO9000), \\nYOLOv3, and YOLOv4. Each iteration has surpassed the previous ones in accuracy and',\n",
       " \"speed, solidifying YOLO's reputation as a flexible option for various object detection \\napplications. \\n6. Applications: \\n• YOLO's strength lies in its high-precision real-time object identification capability, \\nmaking it ideal for applications such as robots, autonomous cars, surveillance, and \\nmore. \\n7. Ongoing Relevance: \\n• YOLO is still frequently used in computer vision and has inspired the development of \\nother real-time object detection methods.\",\n",
       " \"other real-time object detection methods. \\n• YOLO's innovative approach to real -time object detection, combined with its iterative \\nimprovements, has made it a widely adopted and versatile solution for various \\napplications in the field of computer vision. \\nMobileNet \\nMobileNet is a family of simplified convolutional neural network architectures designed for \\nefficient and low-latency deep learning inference on mobile and embedded devices. Developed\",\n",
       " \"by Google's Research Brain Team, MobileNet prioritizes high performance within a small \\nmemory and computational framework. Notable features include using depthwise separable \\nconvolutions and dividing the convolution process into depthwise and pointwise stages to \\nreduce parameters and calculations without compromising precision . MobileNet is widely \\nutilized in settings with limited computational resources, such as edge computing, IoT devices,\",\n",
       " 'and mobile phones. It has undergone iterative improvements with versions like MobileNetV1, \\nV2, and V3, each introducing enhancements to the architecture, accuracy, and speed. \\nIn the realm of real -time object detection for autonomous driving, SqueezeDet is a \\nspecialized model designed for identifying objects on roads. Introduced in a 2017 paper titled \\n\"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time',\n",
       " 'Object Detection for Autonomous Driving,\" SqueezeDet is built on the lightweight SqueezeNet \\narchitecture, well-suited for embedded and real-time applications. It processes whole images in \\na single pass using a fully convolutional neural network (FCN) architecture, generating \\nbounding boxes and class predictions for recognized objects. SqueezeDet incorporates \"squeeze \\nand excitation\" blocks to dynamically vary channel -wise scaling factors, improving object',\n",
       " 'feature representation. The model predicts bounding boxes and assigns class labels to each box, \\nachieving a balance between processing efficiency and accuracy for real -time object detection \\nin autonomous vehicles. \\nMobileNet and SqueezeDet  showcase the importance of efficient and lightweight neural \\nnetwork architectures in enabling deep learning applications on devices with limited \\ncomputational capabilities.',\n",
       " \"Artificial Intelligence for Object Detection and Its Metadata \\nhttps://iaeme.com/Home/journal/IJAIML 132 editor@iaeme.com \\nMobileNet's versatility makes it a popular choice for various applications, while \\nSqueezeDet excels in the specific context of real-time object detection for autonomous driving. \\nCONCLUSION \\nWhen integrated with analytics and metadata, object detection proves invaluable across diverse\",\n",
       " 'industries, especially in manufacturing and quality control. This powerful combination excels \\nin identifying flaws on assembly lines, optimizing processes, and e nabling preventive \\nmaintenance. For inventory tracking, metadata enhances supply chain efficiency, while in \\nproduct monitoring, it predicts quality and facilitates customization. Integration is equally \\nbeneficial in tracking machinery functionality, enabli ng predictive maintenance, and ensuring',\n",
       " 'compliance in sectors with stringent regulations. This amalgamation revolutionizes \\nmanufacturing and quality control, fostering data -driven decision-making, improving product \\nquality, reducing waste, and enhancing overall productivity. \\nREFERENCES \\n[1] Deci, “Deci Introduces YOLO-NAS - A Next-Generation, Object Detection Foundation Model \\nGenerated by Deci’s Neural Architecture Search Technology,”  Deci, May 03, 2023. \\nhttps://deci.ai/blog/yolo-nas-foundation-model-object-detection',\n",
       " '[2] S. S. A. Zaidi, M. S. Ansari, A. Aslam, N. Kanwal, M. Asghar, and B. Lee, “A survey of modern \\ndeep learning based object detection models,”  Digital Signal Processing, vol. 126, p. 103514, \\nJun. 2022, doi: https://doi.org/10.1016/j.dsp.2022.103514. \\n[3] “SqueezeDet: Deep Learning for Object Detection,”  Mez Gebre. \\nhttps://mez.sh/2017/04/21/squeezedet-deep-learning-for-object-detection/ (accessed Dec. 22, \\n2023).',\n",
       " '2023). \\n[4] R. Alake, “How Does AI Detect Objects? (Technical),”  Medium, Jan. 14, 2020. \\nhttps://towardsdatascience.com/how-does-ai-detect-objects-technical-d8d63fc12881 \\n[5] D. S. Shenwai, “Top Object Detection Algorithms and Libraries in Artificial Intelligence \\n(AI),” MarkTechPost, Jul. 18, 2023. https://www.marktechpost.com/2023/07/18/top -object-\\ndetection-algorithms-and-libraries-in-artificial-intelligence-ai/ (accessed Dec. 22, 2023).',\n",
       " '[6] Rajath Karangara, “Unique Methods for Highly Populous Countries to Leverage Post-Pandemic \\nEconomy to Ramp Up Digital Payments,” SSRG international journal of computer science and \\nengineering, vol. 10, no. 7, pp. 21–26, Jul. 2023, doi: https://doi.org/10.14445/23488387/ijcse-\\nv10i7p103.',\n",
       " 'Narayana Challa \\nhttps://iaeme.com/Home/journal/IJAIML 133 editor@iaeme.com \\nAuthor Details \\nNarayana Challa, Director of ERP Strategy, IEEE Senior Member, Texas, USA  \\nNarayana Challa serves as the Director of ERP Strategy in the manufacturing industry.  \\nExpert in Digital Transformation leveraging Enterprise Resource Planning to unlock \\noperational efficiencies in supply chain elements such as manufacturing and inventory',\n",
       " 'management. He has led multiple teams across various technologies throughout his career, \\ndemonstrating enthusiasm for researching new technologies and processes within the \\ninformation technology department. With a diverse skill set, he has executed numerous projects \\nin roles such as architect, data engineering, data ingestion, ETL developer,  administrator, and \\nenterprise architect. His expertise extends to cloud platforms, notably Amazon Web Services \\nand Azure.',\n",
       " 'and Azure. \\n \\n \\n \\n \\nCitation: Narayana Challa, Artificial Intelligence for Object Detection and Its Metadata, International Journal of \\nArtificial Intelligence & Machine Learning (IJAIML), 2(1), 2023, pp. 121-133 \\n \\nDOI: https://doi.org/10.17605/OSF.IO/FG3SQ \\n \\nArticle Link:  \\nhttps://iaeme.com/MasterAdmin/Journal_uploads/IJAIML/VOLUME_2_ISSUE_1/IJAIML_02_01_012.pdf \\n \\nAbstract:  \\nhttps://iaeme.com/Home/article_id/IJAIML_02_01_012',\n",
       " 'Copyright: © 2023 Authors. This is an open -access article distributed under the terms of the Creative Commons \\nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the \\noriginal author and source are credited. \\n \\nThis work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0) . \\n \\n \\n✉ editor@iaeme.com \\n \\n \\nView publication stats']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Convert Text to Embeddings\n",
    "\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b1ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 327 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11/11 [00:11<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (327, 384)\n",
      "Adding 327 documents to vector store...\n",
      "Successfully added 327 documents to vector store\n",
      "Total documents in collection: 495\n"
     ]
    }
   ],
   "source": [
    "##Generate Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##Store int he vector database\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0ee1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever Pipeline From VectorStore\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "\n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No Documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "        \n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "277833b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1da19080d90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50ba8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention is all you need'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_467e271a_8',\n",
       "  'content': 'has provided us with a prime example in human \\nattention; a cognitive function that enables us to \\nfocus our limited resources selectively on \\ninformation that is most important to us at any giv en \\nmoment as we perform various tasks while',\n",
       "  'metadata': {'moddate': \"D:20130115124904Z00'00'\",\n",
       "   'title': 'Helgason-2012-CR',\n",
       "   'producer': 'Mac OS X 10.8.2 Quartz PDFContext',\n",
       "   'creator': 'PDFCreator Version 1.2.3',\n",
       "   'source_file': 'Attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '1',\n",
       "   'page': 0,\n",
       "   'author': 'Helgi',\n",
       "   'creationdate': \"D:20130115124904Z00'00'\",\n",
       "   'doc_index': 8,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Attention.pdf',\n",
       "   'total_pages': 6,\n",
       "   'content_length': 239},\n",
       "  'similarity_score': 0.11798709630966187,\n",
       "  'distance': 0.8820129036903381,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_bc71b03b_8',\n",
       "  'content': 'has provided us with a prime example in human \\nattention; a cognitive function that enables us to \\nfocus our limited resources selectively on \\ninformation that is most important to us at any giv en \\nmoment as we perform various tasks while',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'doc_index': 8,\n",
       "   'moddate': \"D:20130115124904Z00'00'\",\n",
       "   'creationdate': \"D:20130115124904Z00'00'\",\n",
       "   'page': 0,\n",
       "   'creator': 'PDFCreator Version 1.2.3',\n",
       "   'title': 'Helgason-2012-CR',\n",
       "   'total_pages': 6,\n",
       "   'page_label': '1',\n",
       "   'content_length': 239,\n",
       "   'author': 'Helgi',\n",
       "   'source': '..\\\\data\\\\pdf\\\\Attention.pdf',\n",
       "   'producer': 'Mac OS X 10.8.2 Quartz PDFContext',\n",
       "   'source_file': 'Attention.pdf'},\n",
       "  'similarity_score': 0.11798709630966187,\n",
       "  'distance': 0.8820129036903381,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_5e45381d_15',\n",
       "  'content': 'architectures.  \\n2 HUMAN ATTENTION \\nResearch of human attention has a long history \\ndating back to the beginnings of psychology. Back \\nin 1890, the American psychologist William James \\nwrote the following (James 1890): \\n \\n“Everyone knows what attention is. It is the taking \\npossession by the mind, in clear and vivid form, of  \\none out of what seem several simultaneously \\npossible objects or trains of thought. Focalization , \\nconcentration, of consciousness are of its essence.  It',\n",
       "  'metadata': {'doc_index': 15,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Attention.pdf',\n",
       "   'creationdate': \"D:20130115124904Z00'00'\",\n",
       "   'creator': 'PDFCreator Version 1.2.3',\n",
       "   'page': 1,\n",
       "   'total_pages': 6,\n",
       "   'source_file': 'Attention.pdf',\n",
       "   'author': 'Helgi',\n",
       "   'title': 'Helgason-2012-CR',\n",
       "   'content_length': 484,\n",
       "   'moddate': \"D:20130115124904Z00'00'\",\n",
       "   'producer': 'Mac OS X 10.8.2 Quartz PDFContext',\n",
       "   'page_label': '2',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.11597287654876709,\n",
       "  'distance': 0.8840271234512329,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_d7b8df9b_15',\n",
       "  'content': 'architectures.  \\n2 HUMAN ATTENTION \\nResearch of human attention has a long history \\ndating back to the beginnings of psychology. Back \\nin 1890, the American psychologist William James \\nwrote the following (James 1890): \\n \\n“Everyone knows what attention is. It is the taking \\npossession by the mind, in clear and vivid form, of  \\none out of what seem several simultaneously \\npossible objects or trains of thought. Focalization , \\nconcentration, of consciousness are of its essence.  It',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\Attention.pdf',\n",
       "   'doc_index': 15,\n",
       "   'title': 'Helgason-2012-CR',\n",
       "   'producer': 'Mac OS X 10.8.2 Quartz PDFContext',\n",
       "   'moddate': \"D:20130115124904Z00'00'\",\n",
       "   'creationdate': \"D:20130115124904Z00'00'\",\n",
       "   'page_label': '2',\n",
       "   'creator': 'PDFCreator Version 1.2.3',\n",
       "   'content_length': 484,\n",
       "   'file_type': 'pdf',\n",
       "   'page': 1,\n",
       "   'total_pages': 6,\n",
       "   'source_file': 'Attention.pdf',\n",
       "   'author': 'Helgi'},\n",
       "  'similarity_score': 0.11597287654876709,\n",
       "  'distance': 0.8840271234512329,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_b76527c7_40',\n",
       "  'content': 'In order to perform deep levels of introspection \\nin complex AI systems, attention is equally useful as \\nfor information originating outside the system; the  \\nsum of activity within such a system can be \\nconsidered to be a vast stream of information and \\nsystem resources remain limited. Determining which \\nparts of this stream are worth processing in order to \\nachieve meta-cognitive goals may be considered as \\nthe role of attention, in much the same way as',\n",
       "  'metadata': {'author': 'Helgi',\n",
       "   'title': 'Helgason-2012-CR',\n",
       "   'page': 3,\n",
       "   'creationdate': \"D:20130115124904Z00'00'\",\n",
       "   'source': '..\\\\data\\\\pdf\\\\Attention.pdf',\n",
       "   'total_pages': 6,\n",
       "   'moddate': \"D:20130115124904Z00'00'\",\n",
       "   'content_length': 459,\n",
       "   'source_file': 'Attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'PDFCreator Version 1.2.3',\n",
       "   'doc_index': 40,\n",
       "   'page_label': '4',\n",
       "   'producer': 'Mac OS X 10.8.2 Quartz PDFContext'},\n",
       "  'similarity_score': 0.06443607807159424,\n",
       "  'distance': 0.9355639219284058,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3ed44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Training Optimization'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_954cf6ec_172',\n",
       "  'content': 'all training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we',\n",
       "  'metadata': {'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'content_length': 441,\n",
       "   'keywords': '',\n",
       "   'total_pages': 27,\n",
       "   'page_label': '14',\n",
       "   'page': 13,\n",
       "   'source_file': 'Embeddings2.pdf',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 172,\n",
       "   'source': '..\\\\data\\\\pdf\\\\Embeddings2.pdf',\n",
       "   'producer': 'pikepdf 8.15.1'},\n",
       "  'similarity_score': 0.11010527610778809,\n",
       "  'distance': 0.8898947238922119,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Training Optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGs (3.10.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
